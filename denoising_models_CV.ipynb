{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb45f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "#from resotrmer import Restormer_Denoise\n",
    "from models_DnCNN import DnCNN_Denoiser\n",
    "from denoise_classical import GaussianBlur, MedianBlur\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import random\n",
    "#from glob import glob\n",
    "\n",
    "\n",
    "\n",
    "#Turn all the randomisation off to ensure the results of every execution is the same \n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "#torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a4b32",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9735a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "FOLDER_PATH = \"Images/100\"\n",
    "\n",
    "#restomer = Restormer_Denoise(\"blind\")\n",
    "#dncnn = DnCNN_Denoiser()\n",
    "\n",
    "denoise_methods = {\n",
    "    \"None\": lambda x:x,\n",
    "    #\"Restormer\": restomer.denoise_image,\n",
    "    \"Gaussian_Blur\": GaussianBlur,\n",
    "    \"Median_Blur\": MedianBlur,\n",
    "    #\"DnCNN\": dncnn.denoise_image\n",
    "}\n",
    "\n",
    "classification_models = [\"CNN\", \"KNN\", \"SVM\", \"Random Forest\"]\n",
    "\n",
    "models_denoising_accuracies = {model: {denoise_method: -1 for denoise_method in denoise_methods.keys()}  # -1 represent not yet calculated\n",
    "                               for model in classification_models}\n",
    "models_denoising_f1_scores = {model: {denoise_method: -1 for denoise_method in denoise_methods.keys()}  # -1 represent not yet calculated\n",
    "                               for model in classification_models}\n",
    "models_denoising_classification_times = {model: {denoise_method: -1 for denoise_method in denoise_methods.keys()}  # -1 represent not yet calculated\n",
    "                               for model in classification_models}\n",
    "models_denoising_confusion_metrics = {model: {denoise_method: -1 for denoise_method in denoise_methods.keys()}  # -1 represent not yet calculated\n",
    "                               for model in classification_models}\n",
    "\n",
    "def build_transform(denoise_method: str) -> transforms.Compose:\n",
    "    denoise_fn = denoise_methods.get(denoise_method, lambda x:x)\n",
    "\n",
    "    return transforms.Compose([\n",
    "        transforms.Lambda(denoise_fn),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509a275",
   "metadata": {},
   "source": [
    "### Preprocessing: Merge labels(**Only** execute the following cell if you haven't merge the folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d1cda05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m         cate_count += \u001b[32m1\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mmerge_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mB_Cells\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCD4+_T_Cells\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDCIS_1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDCIS_2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mInvasive_Tumor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mProlif_Invasive_Tumor\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mmerge_folder\u001b[39m\u001b[34m(list1, list2, list3)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m folder_dests:\n\u001b[32m     19\u001b[39m     dest_path = source_path + i\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mos\u001b[49m.makedirs(dest_path, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdest_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is created.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# start copying file process\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# each list argument contains some subfolders, this function merge all images in subfolders into one big folder\n",
    "# list1: immune cells class\n",
    "# list2: non-invasive tumor\n",
    "# list3: invasive tumor\n",
    "def merge_folder(list1, list2, list3):\n",
    "    # define folder paths\n",
    "    source_path = './Images/100/'\n",
    "\n",
    "    immune_cells = list1\n",
    "\n",
    "    non_invasive_tumor = list2\n",
    "\n",
    "    invasive_tumor_cell = list3\n",
    "\n",
    "    folder_dests = [\"Immune_Cells\", \"Non_Invasive_Tumor\", \"Invasive_Tumor_Set\"]\n",
    "\n",
    "    # make empty folder\n",
    "    for i in folder_dests:\n",
    "        dest_path = source_path + i\n",
    "        os.makedirs(dest_path, exist_ok=True)\n",
    "        print(f\"{dest_path} is created.\")\n",
    "\n",
    "    # start copying file process\n",
    "    source_categories = [immune_cells, non_invasive_tumor, invasive_tumor_cell]\n",
    "    extensions = '*.png'\n",
    "\n",
    "    count = 1\n",
    "    cate_count = 0\n",
    "    for source_dirs in source_categories:\n",
    "        for src in source_dirs:\n",
    "            pattern = os.path.join(source_path+src, extensions)\n",
    "            print(pattern)\n",
    "            for img_path in glob.glob(pattern):\n",
    "                print(img_path)\n",
    "                filename = os.path.basename(img_path)\n",
    "                dest_path = source_path + folder_dests[cate_count]\n",
    "                file_dest_path = os.path.join(dest_path, filename)\n",
    "                # copying images from sub-folder to big folder, (overwirte if same images exist)\n",
    "                shutil.copy2(img_path, file_dest_path)\n",
    "                print(f'{count}: Copied {img_path} -> {file_dest_path}')\n",
    "                count += 1\n",
    "        cate_count += 1\n",
    "    return 0\n",
    "\n",
    "merge_folder(['B_Cells','CD4+_T_Cells'], ['DCIS_1', 'DCIS_2'], ['Invasive_Tumor', 'Prolif_Invasive_Tumor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df20f2",
   "metadata": {},
   "source": [
    "### Define Image Dataset structure and image transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3b14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataSet(Dataset):\n",
    "    def __init__(self, image_names, transform):\n",
    "        self.file_names = []\n",
    "        self.labels = []\n",
    "        for numeric_label, names in enumerate(image_names):\n",
    "            self.labels.extend([numeric_label]*len(names))\n",
    "            self.file_names.extend(names)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.file_names[index]\n",
    "        img = Image.open(img_name).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        label = self.labels[index]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "\n",
    "# labels = [\"B_Cells\", \"CD4+_T_Cells\", \"DCIS_1\", \"DCIS_2\", \"Invasive_Tumor\", \"Prolif_Invasive_Tumor\"]\n",
    "labels = [\"Immune_Cells\", \"Non_Invasive_Tumor\", \"Invasive_Tumor_Set\"]\n",
    "le = LabelEncoder()\n",
    "numeric_labels = le.fit_transform(labels)\n",
    "image_names = []\n",
    "for _ in numeric_labels:\n",
    "    image_names.append([])\n",
    "\n",
    "for (dir_path, dir_names, file_names) in os.walk(FOLDER_PATH):\n",
    "    parent_folder = os.path.basename(dir_path)\n",
    "    if parent_folder in labels: # Read the subset of dataset to reduce training time \n",
    "        for file in file_names:\n",
    "            image = cv2.imread(os.path.join(dir_path, file))\n",
    "            if image.shape[0] < 100 and image.shape[1] < 100: #skip the small image, it doesn't give much info\n",
    "                continue\n",
    "            numeric_label = le.transform([parent_folder])[0]\n",
    "            image_names[numeric_label].append(os.path.join(dir_path, file))\n",
    "\n",
    "\n",
    "#denoising_datasets = {key : ImageDataSet(image_names, build_transform(key)) for key in denoise_methods.keys()}\n",
    "\n",
    "# toy dataset\n",
    "toy_set_names = []\n",
    "for names in image_names:\n",
    "    # randomly select 10%\n",
    "    subset_size = 0.1\n",
    "    k = max(1, int(subset_size * len(names)))\n",
    "    toy_set_names.append(random.sample(names, k))\n",
    "denoising_datasets = {key : ImageDataSet(toy_set_names, build_transform(key)) for key in denoise_methods.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e5748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13440\n",
      "38149\n",
      "24606\n"
     ]
    }
   ],
   "source": [
    "# dataset total size for each category\n",
    "for names in image_names:\n",
    "    n = max(1, int(len(names)))\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0161227",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e58d79a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #check if the computer has GPU\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(image_names))\n",
    "model = model.to(device)\n",
    "\n",
    "denoising_cnn_models = {key : deepcopy(model).to(device) for key in denoise_methods.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4646c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #check if the computer has GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    # You can proceed to verify GPU properties\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "    # PyTorch might not be utilizing the AMD GPU or other GPU\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0) # 0 is the default device\n",
    "    print(f\"GPU name: {device_name}\")\n",
    "    device_properties = torch.cuda.get_device_properties(0)\n",
    "    print(device_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64124e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters setting\n",
    "num_epochs = 100\n",
    "patience = 10 #for early stopping\n",
    "batch_size = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9806ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset: ImageDataSet):\n",
    "    train_idx, temp_idx = train_test_split(list(range(len(dataset))), test_size=0.3, random_state=0)\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=0)\n",
    "\n",
    "    print(len(train_idx))\n",
    "    print(len(temp_idx))\n",
    "    print(len(val_idx))\n",
    "\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    test_subset = Subset(dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1197a",
   "metadata": {},
   "source": [
    "### CNN Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7afac9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training None model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 1.284135341644287\n",
      "Batch loss: 0.6855536103248596\n",
      "Batch loss: 0.6943813562393188\n",
      "Batch loss: 0.6939305663108826\n",
      "Batch loss: 0.5582002997398376\n",
      "Batch loss: 0.5074259042739868\n",
      "Batch loss: 1.019144892692566\n",
      "Batch loss: 0.5984938144683838\n",
      "Batch loss: 0.6779911518096924\n",
      "Batch loss: 0.42383915185928345\n",
      "Batch loss: 0.5571345090866089\n",
      "Batch loss: 0.4899900555610657\n",
      "Batch loss: 0.44944360852241516\n",
      "Batch loss: 0.7300111651420593\n",
      "Batch loss: 0.4895065724849701\n",
      "Batch loss: 0.44679397344589233\n",
      "Batch loss: 0.30481213331222534\n",
      "Batch loss: 0.36908018589019775\n",
      "Batch loss: 0.3026810586452484\n",
      "Batch loss: 0.47486257553100586\n",
      "Batch loss: 0.46612098813056946\n",
      "Batch loss: 0.3421182930469513\n",
      "Batch loss: 0.2919856607913971\n",
      "Batch loss: 0.3899948298931122\n",
      "Batch loss: 0.31888288259506226\n",
      "Batch loss: 0.2262706309556961\n",
      "Batch loss: 0.43046078085899353\n",
      "Batch loss: 0.3185242712497711\n",
      "Batch loss: 0.27556487917900085\n",
      "Batch loss: 0.28116878867149353\n",
      "Batch loss: 0.3946831524372101\n",
      "Batch loss: 0.3596433997154236\n",
      "Batch loss: 0.30697742104530334\n",
      "Batch loss: 0.2138834297657013\n",
      "Batch loss: 0.2653656303882599\n",
      "Batch loss: 0.37167784571647644\n",
      "Batch loss: 0.3657342791557312\n",
      "Batch loss: 0.22475770115852356\n",
      "Batch loss: 0.3282013237476349\n",
      "Batch loss: 0.17656707763671875\n",
      "Batch loss: 0.35889995098114014\n",
      "Batch loss: 0.31202232837677\n",
      "Batch loss: 0.33073049783706665\n",
      "Batch loss: 0.22026006877422333\n",
      "Batch loss: 0.34671348333358765\n",
      "Batch loss: 0.2685658633708954\n",
      "Batch loss: 0.2978596091270447\n",
      "Batch loss: 0.23970013856887817\n",
      "Batch loss: 0.22728395462036133\n",
      "Batch loss: 0.2619004249572754\n",
      "Batch loss: 0.31779199838638306\n",
      "Batch loss: 0.3184252679347992\n",
      "Batch loss: 0.24103282392024994\n",
      "Batch loss: 0.25600534677505493\n",
      "Batch loss: 0.2697303593158722\n",
      "Batch loss: 0.3357631266117096\n",
      "Batch loss: 0.2573128640651703\n",
      "Batch loss: 0.30365365743637085\n",
      "Batch loss: 0.21749499440193176\n",
      "Batch loss: 0.25851863622665405\n",
      "Batch loss: 0.24294966459274292\n",
      "Batch loss: 0.218541219830513\n",
      "Batch loss: 0.2334001213312149\n",
      "Batch loss: 0.25496289134025574\n",
      "Batch loss: 0.2263706773519516\n",
      "Batch loss: 0.2775755524635315\n",
      "Batch loss: 0.2892439365386963\n",
      "Batch loss: 0.3444380462169647\n",
      "Batch loss: 0.29922357201576233\n",
      "Batch loss: 0.23610235750675201\n",
      "Batch loss: 0.24165154993534088\n",
      "Batch loss: 0.36596837639808655\n",
      "Batch loss: 0.21333803236484528\n",
      "Batch loss: 0.25345632433891296\n",
      "Batch loss: 0.23215417563915253\n",
      "Batch loss: 0.22619953751564026\n",
      "Batch loss: 0.2541173994541168\n",
      "Batch loss: 0.27222225069999695\n",
      "Batch loss: 0.2179974615573883\n",
      "Batch loss: 0.3448190689086914\n",
      "Batch loss: 0.20794105529785156\n",
      "Batch loss: 0.16754809021949768\n",
      "Batch loss: 0.2631290853023529\n",
      "Batch loss: 0.2956763803958893\n",
      "Batch loss: 0.2102295309305191\n",
      "Batch loss: 0.18667002022266388\n",
      "Batch loss: 0.18779294192790985\n",
      "Batch loss: 0.2747895121574402\n",
      "Batch loss: 0.31490543484687805\n",
      "Batch loss: 0.20903943479061127\n",
      "Batch loss: 0.34568750858306885\n",
      "Batch loss: 0.16473877429962158\n",
      "Batch loss: 0.284984827041626\n",
      "Batch loss: 0.21026533842086792\n",
      "Batch loss: 0.2859337627887726\n",
      "Batch loss: 0.1886000633239746\n",
      "Batch loss: 0.21420279145240784\n",
      "Batch loss: 0.3310239315032959\n",
      "Batch loss: 0.24728254973888397\n",
      "Batch loss: 0.3183674216270447\n",
      "Batch loss: 0.2728697657585144\n",
      "Batch loss: 0.19051361083984375\n",
      "Batch loss: 0.2160268872976303\n",
      "Batch loss: 0.23837050795555115\n",
      "Batch loss: 0.2564363181591034\n",
      "Batch loss: 0.2967189848423004\n",
      "Batch loss: 0.354158878326416\n",
      "Batch loss: 0.26127490401268005\n",
      "Batch loss: 0.20702283084392548\n",
      "Batch loss: 0.3269251883029938\n",
      "Batch loss: 0.2659303545951843\n",
      "Batch loss: 0.1521693617105484\n",
      "Batch loss: 0.2360437959432602\n",
      "Batch loss: 0.18857447803020477\n",
      "Batch loss: 0.242898628115654\n",
      "Batch loss: 0.2859852612018585\n",
      "Batch loss: 0.19351927936077118\n",
      "Batch loss: 0.1414939910173416\n",
      "Batch loss: 0.17698059976100922\n",
      "Batch loss: 0.1780179888010025\n",
      "Batch loss: 0.18070471286773682\n",
      "Batch loss: 0.224458709359169\n",
      "Batch loss: 0.25527864694595337\n",
      "Batch loss: 0.11836910247802734\n",
      "Batch loss: 0.2876344919204712\n",
      "Batch loss: 0.34398123621940613\n",
      "Batch loss: 0.24099384248256683\n",
      "Batch loss: 0.2053772211074829\n",
      "Batch loss: 0.18776284158229828\n",
      "Batch loss: 0.2392168939113617\n",
      "Batch loss: 0.15946955978870392\n",
      "Batch loss: 0.35375744104385376\n",
      "Batch loss: 0.19108904898166656\n",
      "Batch loss: 0.2690000832080841\n",
      "Batch loss: 0.32557976245880127\n",
      "Batch loss: 0.21847482025623322\n",
      "Batch loss: 0.27319201827049255\n",
      "Batch loss: 0.24623417854309082\n",
      "Batch loss: 0.2799704670906067\n",
      "Batch loss: 0.15920449793338776\n",
      "Batch loss: 0.1457161158323288\n",
      "Batch loss: 0.1910017728805542\n",
      "Batch loss: 0.1999577432870865\n",
      "Batch loss: 0.2598513960838318\n",
      "Batch loss: 0.19550009071826935\n",
      "Batch loss: 0.22256000339984894\n",
      "Batch loss: 0.2500626742839813\n",
      "Batch loss: 0.18927671015262604\n",
      "Batch loss: 0.21322675049304962\n",
      "Batch loss: 0.16044174134731293\n",
      "Batch loss: 0.165441632270813\n",
      "Batch loss: 0.22600597143173218\n",
      "Batch loss: 0.3255527913570404\n",
      "Batch loss: 0.3085639774799347\n",
      "Batch loss: 0.2540383040904999\n",
      "Batch loss: 0.30899563431739807\n",
      "Batch loss: 0.24320295453071594\n",
      "Batch loss: 0.20759010314941406\n",
      "Batch loss: 0.15908733010292053\n",
      "Batch loss: 0.19111721217632294\n",
      "Batch loss: 0.1751563996076584\n",
      "Batch loss: 0.19663219153881073\n",
      "Batch loss: 0.201975479722023\n",
      "Batch loss: 0.2199966311454773\n",
      "Batch loss: 0.23082402348518372\n",
      "Batch loss: 0.24053344130516052\n",
      "Batch loss: 0.1981753408908844\n",
      "Batch loss: 0.19669736921787262\n",
      "Batch loss: 0.18854252994060516\n",
      "Batch loss: 0.16977393627166748\n",
      "Batch loss: 0.26609838008880615\n",
      "Batch loss: 0.27703046798706055\n",
      "Batch loss: 0.13206426799297333\n",
      "Batch loss: 0.18572624027729034\n",
      "Batch loss: 0.17313240468502045\n",
      "Batch loss: 0.25349268317222595\n",
      "Batch loss: 0.27147039771080017\n",
      "Batch loss: 0.140032097697258\n",
      "Batch loss: 0.18962271511554718\n",
      "Batch loss: 0.17447970807552338\n",
      "Batch loss: 0.21314606070518494\n",
      "Batch loss: 0.1556769460439682\n",
      "Batch loss: 0.08260678499937057\n",
      "Batch loss: 0.30761581659317017\n",
      "Batch loss: 0.2806851863861084\n",
      "Batch loss: 0.16153405606746674\n",
      "Batch loss: 0.242937833070755\n",
      "Batch loss: 0.24856452643871307\n",
      "Batch loss: 0.16087931394577026\n",
      "Batch loss: 0.17204879224300385\n",
      "Batch loss: 0.20262333750724792\n",
      "Batch loss: 0.19437403976917267\n",
      "Batch loss: 0.22978359460830688\n",
      "Batch loss: 0.19853638112545013\n",
      "Batch loss: 0.14401622116565704\n",
      "Batch loss: 0.1529226154088974\n",
      "Batch loss: 0.1124805361032486\n",
      "Batch loss: 0.3093157708644867\n",
      "Batch loss: 0.209074467420578\n",
      "Batch loss: 0.15412651002407074\n",
      "Batch loss: 0.2389160841703415\n",
      "Batch loss: 0.2693682312965393\n",
      "Batch loss: 0.16086485981941223\n",
      "Batch loss: 0.14254209399223328\n",
      "Batch loss: 0.23101761937141418\n",
      "Batch loss: 0.3117206394672394\n",
      "Batch loss: 0.2730484902858734\n",
      "Batch loss: 0.23864907026290894\n",
      "Batch loss: 0.1399497389793396\n",
      "Batch loss: 0.22254185378551483\n",
      "Batch loss: 0.16637621819972992\n",
      "Batch loss: 0.2308712899684906\n",
      "Batch loss: 0.19673924148082733\n",
      "Batch loss: 0.2754780650138855\n",
      "Batch loss: 0.19864870607852936\n",
      "Batch loss: 0.16359028220176697\n",
      "Batch loss: 0.26219847798347473\n",
      "Batch loss: 0.17979057133197784\n",
      "Batch loss: 0.13066770136356354\n",
      "Batch loss: 0.24754634499549866\n",
      "Batch loss: 0.13418236374855042\n",
      "Batch loss: 0.1660975068807602\n",
      "Batch loss: 0.17759087681770325\n",
      "Batch loss: 0.3445054292678833\n",
      "Batch loss: 0.15892770886421204\n",
      "Batch loss: 0.3356207609176636\n",
      "Batch loss: 0.14501672983169556\n",
      "Batch loss: 0.2433554232120514\n",
      "Batch loss: 0.3061502277851105\n",
      "Batch loss: 0.14093400537967682\n",
      "Batch loss: 0.35640451312065125\n",
      "Batch loss: 0.294307678937912\n",
      "Batch loss: 0.18422885239124298\n",
      "Batch loss: 0.13683289289474487\n",
      "Batch loss: 0.17724572122097015\n",
      "Batch loss: 0.2601695954799652\n",
      "Batch loss: 0.1865280717611313\n",
      "Batch loss: 0.20560526847839355\n",
      "Batch loss: 0.15035893023014069\n",
      "Batch loss: 0.18015344440937042\n",
      "Batch loss: 0.27049896121025085\n",
      "Batch loss: 0.29058441519737244\n",
      "Batch loss: 0.17126582562923431\n",
      "Batch loss: 0.1868513822555542\n",
      "Batch loss: 0.15282002091407776\n",
      "Batch loss: 0.1616598665714264\n",
      "Batch loss: 0.17980073392391205\n",
      "Batch loss: 0.19850830733776093\n",
      "Batch loss: 0.16295236349105835\n",
      "Batch loss: 0.16963942348957062\n",
      "Batch loss: 0.21267060935497284\n",
      "Batch loss: 0.25951826572418213\n",
      "Batch loss: 0.30894795060157776\n",
      "Batch loss: 0.3026144504547119\n",
      "Batch loss: 0.16609352827072144\n",
      "Batch loss: 0.34914636611938477\n",
      "Batch loss: 0.22923465073108673\n",
      "Batch loss: 0.21460473537445068\n",
      "Batch loss: 0.3234497010707855\n",
      "Batch loss: 0.18952980637550354\n",
      "Batch loss: 0.19563953578472137\n",
      "Batch loss: 0.22027085721492767\n",
      "Batch loss: 0.2740621864795685\n",
      "Batch loss: 0.26483261585235596\n",
      "Batch loss: 0.26107847690582275\n",
      "Batch loss: 0.13064835965633392\n",
      "Batch loss: 0.22753122448921204\n",
      "Batch loss: 0.2113865315914154\n",
      "Batch loss: 0.16945576667785645\n",
      "Batch loss: 0.23581592738628387\n",
      "Batch loss: 0.24112561345100403\n",
      "Batch loss: 0.21186259388923645\n",
      "Batch loss: 0.14207617938518524\n",
      "Batch loss: 0.23363164067268372\n",
      "Batch loss: 0.23543231189250946\n",
      "Batch loss: 0.17082877457141876\n",
      "Batch loss: 0.17101334035396576\n",
      "Batch loss: 0.24569940567016602\n",
      "Batch loss: 0.1745900809764862\n",
      "Batch loss: 0.2222142517566681\n",
      "Batch loss: 0.20836350321769714\n",
      "Batch loss: 0.1394548863172531\n",
      "Batch loss: 0.24425408244132996\n",
      "Batch loss: 0.42810001969337463\n",
      "Batch loss: 0.22211891412734985\n",
      "Batch loss: 0.2557728886604309\n",
      "Batch loss: 0.205434650182724\n",
      "Batch loss: 0.1388537734746933\n",
      "Batch loss: 0.17730101943016052\n",
      "Batch loss: 0.13218453526496887\n",
      "Batch loss: 0.1701461225748062\n",
      "Batch loss: 0.19304434955120087\n",
      "Batch loss: 0.15398965775966644\n",
      "Batch loss: 0.17967228591442108\n",
      "Batch loss: 0.20341096818447113\n",
      "Batch loss: 0.3078872561454773\n",
      "Batch loss: 0.2307378202676773\n",
      "Batch loss: 0.14729297161102295\n",
      "Batch loss: 0.23236849904060364\n",
      "Batch loss: 0.13235537707805634\n",
      "Batch loss: 0.22490650415420532\n",
      "Batch loss: 0.1731097549200058\n",
      "Batch loss: 0.200109601020813\n",
      "Batch loss: 0.16120974719524384\n",
      "Batch loss: 0.2698594629764557\n",
      "Batch loss: 0.19420960545539856\n",
      "Batch loss: 0.10486756265163422\n",
      "Batch loss: 0.1723427027463913\n",
      "Batch loss: 0.15770211815834045\n",
      "Batch loss: 0.2455362230539322\n",
      "Batch loss: 0.2244398295879364\n",
      "Batch loss: 0.18567807972431183\n",
      "Batch loss: 0.17552834749221802\n",
      "Batch loss: 0.2686072885990143\n",
      "Batch loss: 0.27737686038017273\n",
      "Batch loss: 0.3044199049472809\n",
      "Batch loss: 0.2052556872367859\n",
      "Batch loss: 0.1608550101518631\n",
      "Batch loss: 0.2602391242980957\n",
      "Batch loss: 0.18971706926822662\n",
      "Batch loss: 0.20106753706932068\n",
      "Batch loss: 0.12888088822364807\n",
      "Batch loss: 0.16372628509998322\n",
      "Batch loss: 0.24079038202762604\n",
      "Batch loss: 0.21213525533676147\n",
      "Batch loss: 0.16859328746795654\n",
      "Batch loss: 0.2907443344593048\n",
      "Batch loss: 0.2695709466934204\n",
      "Batch loss: 0.21206046640872955\n",
      "Batch loss: 0.25293803215026855\n",
      "Batch loss: 0.1316794753074646\n",
      "Batch loss: 0.29306676983833313\n",
      "Batch loss: 0.14812052249908447\n",
      "Batch loss: 0.2222977876663208\n",
      "Batch loss: 0.190090149641037\n",
      "Batch loss: 0.1544470489025116\n",
      "Batch loss: 0.10218390822410583\n",
      "Batch loss: 0.16775034368038177\n",
      "Batch loss: 0.18957871198654175\n",
      "Batch loss: 0.13681066036224365\n",
      "Batch loss: 0.15507261455059052\n",
      "Batch loss: 0.139175146818161\n",
      "Batch loss: 0.1549595594406128\n",
      "Batch loss: 0.22262804210186005\n",
      "Batch loss: 0.14339566230773926\n",
      "Batch loss: 0.19408462941646576\n",
      "Batch loss: 0.2514311373233795\n",
      "Batch loss: 0.3689449429512024\n",
      "Batch loss: 0.15341593325138092\n",
      "Batch loss: 0.1308501958847046\n",
      "Batch loss: 0.171652153134346\n",
      "Batch loss: 0.21361365914344788\n",
      "Batch loss: 0.09556525200605392\n",
      "Batch loss: 0.1650511473417282\n",
      "Batch loss: 0.17353719472885132\n",
      "Batch loss: 0.15836843848228455\n",
      "Batch loss: 0.12320326268672943\n",
      "Batch loss: 0.25671908259391785\n",
      "Batch loss: 0.24108707904815674\n",
      "Batch loss: 0.2403050810098648\n",
      "Batch loss: 0.2038360983133316\n",
      "Batch loss: 0.19040928781032562\n",
      "Batch loss: 0.19418177008628845\n",
      "Batch loss: 0.3619292676448822\n",
      "Batch loss: 0.2275320589542389\n",
      "Batch loss: 0.18244288861751556\n",
      "Batch loss: 0.24820974469184875\n",
      "Batch loss: 0.16019679605960846\n",
      "Batch loss: 0.1892937868833542\n",
      "Batch loss: 0.3085012137889862\n",
      "Batch loss: 0.1739744246006012\n",
      "Batch loss: 0.22453922033309937\n",
      "Batch loss: 0.19819441437721252\n",
      "Batch loss: 0.13188262283802032\n",
      "Batch loss: 0.21205782890319824\n",
      "Batch loss: 0.13510596752166748\n",
      "Batch loss: 0.24965153634548187\n",
      "Batch loss: 0.13583776354789734\n",
      "Batch loss: 0.23456023633480072\n",
      "Batch loss: 0.28932443261146545\n",
      "Batch loss: 0.3445271849632263\n",
      "Batch loss: 0.1829109787940979\n",
      "Batch loss: 0.16115131974220276\n",
      "Batch loss: 0.2365364283323288\n",
      "Batch loss: 0.14236953854560852\n",
      "Batch loss: 0.24183684587478638\n",
      "Batch loss: 0.28097859025001526\n",
      "Batch loss: 0.15937447547912598\n",
      "Batch loss: 0.18589313328266144\n",
      "Batch loss: 0.18278056383132935\n",
      "Batch loss: 0.1692034751176834\n",
      "Batch loss: 0.20751459896564484\n",
      "Batch loss: 0.13344524800777435\n",
      "Batch loss: 0.16193443536758423\n",
      "Batch loss: 0.27868416905403137\n",
      "Batch loss: 0.228362038731575\n",
      "Batch loss: 0.06086275354027748\n",
      "Batch loss: 0.1899685561656952\n",
      "Batch loss: 0.11060607433319092\n",
      "Batch loss: 0.17492784559726715\n",
      "Batch loss: 0.16334056854248047\n",
      "Batch loss: 0.14868274331092834\n",
      "Batch loss: 0.2973576784133911\n",
      "Batch loss: 0.14784319698810577\n",
      "Batch loss: 0.0984327495098114\n",
      "Batch loss: 0.32911020517349243\n",
      "Batch loss: 0.21033278107643127\n",
      "Batch loss: 0.17101959884166718\n",
      "Batch loss: 0.17520515620708466\n",
      "Batch loss: 0.25832346081733704\n",
      "Batch loss: 0.19884486496448517\n",
      "Batch loss: 0.19642333686351776\n",
      "Batch loss: 0.13094563782215118\n",
      "Batch loss: 0.16082081198692322\n",
      "Batch loss: 0.22007611393928528\n",
      "Batch loss: 0.15138264000415802\n",
      "Batch loss: 0.13410265743732452\n",
      "Epoch [1/100], Loss: 0.2404\n",
      "Validation Loss: 0.7273, Accuracy: 73.91%\n",
      "Batch loss: 0.23750437796115875\n",
      "Batch loss: 0.20127558708190918\n",
      "Batch loss: 0.21472802758216858\n",
      "Batch loss: 0.2006070762872696\n",
      "Batch loss: 0.2040393352508545\n",
      "Batch loss: 0.19252878427505493\n",
      "Batch loss: 0.49995896220207214\n",
      "Batch loss: 0.21322837471961975\n",
      "Batch loss: 0.25958579778671265\n",
      "Batch loss: 0.12594059109687805\n",
      "Batch loss: 0.2308376282453537\n",
      "Batch loss: 0.22420552372932434\n",
      "Batch loss: 0.16976378858089447\n",
      "Batch loss: 0.3298853039741516\n",
      "Batch loss: 0.2580394446849823\n",
      "Batch loss: 0.22834716737270355\n",
      "Batch loss: 0.1595691293478012\n",
      "Batch loss: 0.2518462836742401\n",
      "Batch loss: 0.11256550997495651\n",
      "Batch loss: 0.20992033183574677\n",
      "Batch loss: 0.20639100670814514\n",
      "Batch loss: 0.17552851140499115\n",
      "Batch loss: 0.17796874046325684\n",
      "Batch loss: 0.17925873398780823\n",
      "Batch loss: 0.16240635514259338\n",
      "Batch loss: 0.1522090584039688\n",
      "Batch loss: 0.21276922523975372\n",
      "Batch loss: 0.20551203191280365\n",
      "Batch loss: 0.11869803071022034\n",
      "Batch loss: 0.12773163616657257\n",
      "Batch loss: 0.21357032656669617\n",
      "Batch loss: 0.19127638638019562\n",
      "Batch loss: 0.17109999060630798\n",
      "Batch loss: 0.13926705718040466\n",
      "Batch loss: 0.12485652416944504\n",
      "Batch loss: 0.3122861683368683\n",
      "Batch loss: 0.1738426387310028\n",
      "Batch loss: 0.16388943791389465\n",
      "Batch loss: 0.22485215961933136\n",
      "Batch loss: 0.12579193711280823\n",
      "Batch loss: 0.17277434468269348\n",
      "Batch loss: 0.15073032677173615\n",
      "Batch loss: 0.22749684751033783\n",
      "Batch loss: 0.19039130210876465\n",
      "Batch loss: 0.21175232529640198\n",
      "Batch loss: 0.1839103102684021\n",
      "Batch loss: 0.2091277539730072\n",
      "Batch loss: 0.12663903832435608\n",
      "Batch loss: 0.11577736586332321\n",
      "Batch loss: 0.1842176616191864\n",
      "Batch loss: 0.16721804440021515\n",
      "Batch loss: 0.16114819049835205\n",
      "Batch loss: 0.11755655705928802\n",
      "Batch loss: 0.17598406970500946\n",
      "Batch loss: 0.19786766171455383\n",
      "Batch loss: 0.22619150578975677\n",
      "Batch loss: 0.1278216540813446\n",
      "Batch loss: 0.2265097051858902\n",
      "Batch loss: 0.17758837342262268\n",
      "Batch loss: 0.15143537521362305\n",
      "Batch loss: 0.17027969658374786\n",
      "Batch loss: 0.15970981121063232\n",
      "Batch loss: 0.19037514925003052\n",
      "Batch loss: 0.2035144418478012\n",
      "Batch loss: 0.10608255863189697\n",
      "Batch loss: 0.22211487591266632\n",
      "Batch loss: 0.17535188794136047\n",
      "Batch loss: 0.2511308491230011\n",
      "Batch loss: 0.21006374061107635\n",
      "Batch loss: 0.19819270074367523\n",
      "Batch loss: 0.20792154967784882\n",
      "Batch loss: 0.2073073536157608\n",
      "Batch loss: 0.1053110733628273\n",
      "Batch loss: 0.13335412740707397\n",
      "Batch loss: 0.12395795434713364\n",
      "Batch loss: 0.1496153026819229\n",
      "Batch loss: 0.16247031092643738\n",
      "Batch loss: 0.19578014314174652\n",
      "Batch loss: 0.13666898012161255\n",
      "Batch loss: 0.25615012645721436\n",
      "Batch loss: 0.11604415625333786\n",
      "Batch loss: 0.10541333258152008\n",
      "Batch loss: 0.18779787421226501\n",
      "Batch loss: 0.2779439687728882\n",
      "Batch loss: 0.16772563755512238\n",
      "Batch loss: 0.1316506713628769\n",
      "Batch loss: 0.10739107429981232\n",
      "Batch loss: 0.14676888287067413\n",
      "Batch loss: 0.17090846598148346\n",
      "Batch loss: 0.1282334327697754\n",
      "Batch loss: 0.24500244855880737\n",
      "Batch loss: 0.13324753940105438\n",
      "Batch loss: 0.27320730686187744\n",
      "Batch loss: 0.1434323489665985\n",
      "Batch loss: 0.13575048744678497\n",
      "Batch loss: 0.17590411007404327\n",
      "Batch loss: 0.10031060129404068\n",
      "Batch loss: 0.23466160893440247\n",
      "Batch loss: 0.20983023941516876\n",
      "Batch loss: 0.1956803798675537\n",
      "Batch loss: 0.14113351702690125\n",
      "Batch loss: 0.10908105224370956\n",
      "Batch loss: 0.23621471226215363\n",
      "Batch loss: 0.18492627143859863\n",
      "Batch loss: 0.1455477774143219\n",
      "Batch loss: 0.18004007637500763\n",
      "Batch loss: 0.2197209745645523\n",
      "Batch loss: 0.20131301879882812\n",
      "Batch loss: 0.13267900049686432\n",
      "Batch loss: 0.24592486023902893\n",
      "Batch loss: 0.2635841965675354\n",
      "Batch loss: 0.11726374924182892\n",
      "Batch loss: 0.1414583921432495\n",
      "Batch loss: 0.1137104481458664\n",
      "Batch loss: 0.14611747860908508\n",
      "Batch loss: 0.2145964354276657\n",
      "Batch loss: 0.15864714980125427\n",
      "Batch loss: 0.09855227172374725\n",
      "Batch loss: 0.13774250447750092\n",
      "Batch loss: 0.15005648136138916\n",
      "Batch loss: 0.13549087941646576\n",
      "Batch loss: 0.15405479073524475\n",
      "Batch loss: 0.23992083966732025\n",
      "Batch loss: 0.16141898930072784\n",
      "Batch loss: 0.22074271738529205\n",
      "Batch loss: 0.2346460074186325\n",
      "Batch loss: 0.19058968126773834\n",
      "Batch loss: 0.21371686458587646\n",
      "Batch loss: 0.13746307790279388\n",
      "Batch loss: 0.19258898496627808\n",
      "Batch loss: 0.157327800989151\n",
      "Batch loss: 0.22941313683986664\n",
      "Batch loss: 0.15902087092399597\n",
      "Batch loss: 0.29244980216026306\n",
      "Batch loss: 0.262601375579834\n",
      "Batch loss: 0.10411925613880157\n",
      "Batch loss: 0.21986600756645203\n",
      "Batch loss: 0.22282938659191132\n",
      "Batch loss: 0.20063799619674683\n",
      "Batch loss: 0.1052430123090744\n",
      "Batch loss: 0.19436471164226532\n",
      "Batch loss: 0.13754461705684662\n",
      "Batch loss: 0.1963934302330017\n",
      "Batch loss: 0.22507278621196747\n",
      "Batch loss: 0.201102614402771\n",
      "Batch loss: 0.24697692692279816\n",
      "Batch loss: 0.23951804637908936\n",
      "Batch loss: 0.13584724068641663\n",
      "Batch loss: 0.1726623773574829\n",
      "Batch loss: 0.1079062893986702\n",
      "Batch loss: 0.17769333720207214\n",
      "Batch loss: 0.2037220448255539\n",
      "Batch loss: 0.20958168804645538\n",
      "Batch loss: 0.2833831310272217\n",
      "Batch loss: 0.22437798976898193\n",
      "Batch loss: 0.20007941126823425\n",
      "Batch loss: 0.24175530672073364\n",
      "Batch loss: 0.15910524129867554\n",
      "Batch loss: 0.14899441599845886\n",
      "Batch loss: 0.16592930257320404\n",
      "Batch loss: 0.08704385161399841\n",
      "Batch loss: 0.17535477876663208\n",
      "Batch loss: 0.1622810810804367\n",
      "Batch loss: 0.17459075152873993\n",
      "Batch loss: 0.19379878044128418\n",
      "Batch loss: 0.1815013289451599\n",
      "Batch loss: 0.1616811752319336\n",
      "Batch loss: 0.1555885225534439\n",
      "Batch loss: 0.13506565988063812\n",
      "Batch loss: 0.13709090650081635\n",
      "Batch loss: 0.1990479677915573\n",
      "Batch loss: 0.25595924258232117\n",
      "Batch loss: 0.11643468588590622\n",
      "Batch loss: 0.14057092368602753\n",
      "Batch loss: 0.13430476188659668\n",
      "Batch loss: 0.21162107586860657\n",
      "Batch loss: 0.23744122684001923\n",
      "Batch loss: 0.09178707003593445\n",
      "Batch loss: 0.174608051776886\n",
      "Batch loss: 0.13521212339401245\n",
      "Batch loss: 0.18180334568023682\n",
      "Batch loss: 0.10182415693998337\n",
      "Batch loss: 0.07824086397886276\n",
      "Batch loss: 0.24887287616729736\n",
      "Batch loss: 0.1943725049495697\n",
      "Batch loss: 0.1293429136276245\n",
      "Batch loss: 0.18139344453811646\n",
      "Batch loss: 0.177302747964859\n",
      "Batch loss: 0.17747674882411957\n",
      "Batch loss: 0.13892289996147156\n",
      "Batch loss: 0.19887493550777435\n",
      "Batch loss: 0.13914087414741516\n",
      "Batch loss: 0.18572303652763367\n",
      "Batch loss: 0.164270281791687\n",
      "Batch loss: 0.13936461508274078\n",
      "Batch loss: 0.1371670365333557\n",
      "Batch loss: 0.11067315936088562\n",
      "Batch loss: 0.19114553928375244\n",
      "Batch loss: 0.15556123852729797\n",
      "Batch loss: 0.12932026386260986\n",
      "Batch loss: 0.18379491567611694\n",
      "Batch loss: 0.1723836213350296\n",
      "Batch loss: 0.16537897288799286\n",
      "Batch loss: 0.0852525606751442\n",
      "Batch loss: 0.2265235334634781\n",
      "Batch loss: 0.23445561528205872\n",
      "Batch loss: 0.26146888732910156\n",
      "Batch loss: 0.15793046355247498\n",
      "Batch loss: 0.09142559766769409\n",
      "Batch loss: 0.11922741681337357\n",
      "Batch loss: 0.09911338984966278\n",
      "Batch loss: 0.16139882802963257\n",
      "Batch loss: 0.1842363476753235\n",
      "Batch loss: 0.19989585876464844\n",
      "Batch loss: 0.15657731890678406\n",
      "Batch loss: 0.10882390290498734\n",
      "Batch loss: 0.1339215785264969\n",
      "Batch loss: 0.12760351598262787\n",
      "Batch loss: 0.07969987392425537\n",
      "Batch loss: 0.14336277544498444\n",
      "Batch loss: 0.11084619909524918\n",
      "Batch loss: 0.16330532729625702\n",
      "Batch loss: 0.12078144401311874\n",
      "Batch loss: 0.3316330313682556\n",
      "Batch loss: 0.09688320010900497\n",
      "Batch loss: 0.2472342699766159\n",
      "Batch loss: 0.20454175770282745\n",
      "Batch loss: 0.1949453502893448\n",
      "Batch loss: 0.2855251729488373\n",
      "Batch loss: 0.12369140237569809\n",
      "Batch loss: 0.20578105747699738\n",
      "Batch loss: 0.2727319300174713\n",
      "Batch loss: 0.15025387704372406\n",
      "Batch loss: 0.1125432625412941\n",
      "Batch loss: 0.12815403938293457\n",
      "Batch loss: 0.26938295364379883\n",
      "Batch loss: 0.1781143844127655\n",
      "Batch loss: 0.2042204737663269\n",
      "Batch loss: 0.15622608363628387\n",
      "Batch loss: 0.17663946747779846\n",
      "Batch loss: 0.23104360699653625\n",
      "Batch loss: 0.23033584654331207\n",
      "Batch loss: 0.13609939813613892\n",
      "Batch loss: 0.15389449894428253\n",
      "Batch loss: 0.16234128177165985\n",
      "Batch loss: 0.12484417855739594\n",
      "Batch loss: 0.13283386826515198\n",
      "Batch loss: 0.23314765095710754\n",
      "Batch loss: 0.13810275495052338\n",
      "Batch loss: 0.15218129754066467\n",
      "Batch loss: 0.19572091102600098\n",
      "Batch loss: 0.27755439281463623\n",
      "Batch loss: 0.3216892182826996\n",
      "Batch loss: 0.27037301659584045\n",
      "Batch loss: 0.17996536195278168\n",
      "Batch loss: 0.28351572155952454\n",
      "Batch loss: 0.21677839756011963\n",
      "Batch loss: 0.18416321277618408\n",
      "Batch loss: 0.24719597399234772\n",
      "Batch loss: 0.14909695088863373\n",
      "Batch loss: 0.17583636939525604\n",
      "Batch loss: 0.1934841126203537\n",
      "Batch loss: 0.150978222489357\n",
      "Batch loss: 0.1831919401884079\n",
      "Batch loss: 0.18991981446743011\n",
      "Batch loss: 0.09706351161003113\n",
      "Batch loss: 0.1914321780204773\n",
      "Batch loss: 0.16597183048725128\n",
      "Batch loss: 0.14418484270572662\n",
      "Batch loss: 0.1704399585723877\n",
      "Batch loss: 0.1785857379436493\n",
      "Batch loss: 0.2328193336725235\n",
      "Batch loss: 0.11465451121330261\n",
      "Batch loss: 0.19172613322734833\n",
      "Batch loss: 0.19571326673030853\n",
      "Batch loss: 0.11394358426332474\n",
      "Batch loss: 0.1217709481716156\n",
      "Batch loss: 0.23069973289966583\n",
      "Batch loss: 0.14628857374191284\n",
      "Batch loss: 0.21431265771389008\n",
      "Batch loss: 0.16698575019836426\n",
      "Batch loss: 0.11395782977342606\n",
      "Batch loss: 0.25259649753570557\n",
      "Batch loss: 0.39007002115249634\n",
      "Batch loss: 0.17024777829647064\n",
      "Batch loss: 0.21860043704509735\n",
      "Batch loss: 0.1287916600704193\n",
      "Batch loss: 0.09800605475902557\n",
      "Batch loss: 0.1201038658618927\n",
      "Batch loss: 0.11818084865808487\n",
      "Batch loss: 0.11336261034011841\n",
      "Batch loss: 0.14840590953826904\n",
      "Batch loss: 0.12759645283222198\n",
      "Batch loss: 0.11716733872890472\n",
      "Batch loss: 0.19048599898815155\n",
      "Batch loss: 0.22702354192733765\n",
      "Batch loss: 0.18253332376480103\n",
      "Batch loss: 0.10962605476379395\n",
      "Batch loss: 0.1698288470506668\n",
      "Batch loss: 0.11460582166910172\n",
      "Batch loss: 0.17679812014102936\n",
      "Batch loss: 0.14575622975826263\n",
      "Batch loss: 0.17137354612350464\n",
      "Batch loss: 0.15774305164813995\n",
      "Batch loss: 0.251477986574173\n",
      "Batch loss: 0.16204339265823364\n",
      "Batch loss: 0.07524058222770691\n",
      "Batch loss: 0.13421215116977692\n",
      "Batch loss: 0.1470891535282135\n",
      "Batch loss: 0.17208436131477356\n",
      "Batch loss: 0.17338979244232178\n",
      "Batch loss: 0.1158999651670456\n",
      "Batch loss: 0.12796585261821747\n",
      "Batch loss: 0.18571263551712036\n",
      "Batch loss: 0.22368477284908295\n",
      "Batch loss: 0.28504371643066406\n",
      "Batch loss: 0.14596837759017944\n",
      "Batch loss: 0.13759580254554749\n",
      "Batch loss: 0.20485574007034302\n",
      "Batch loss: 0.17425107955932617\n",
      "Batch loss: 0.1621951013803482\n",
      "Batch loss: 0.11837238818407059\n",
      "Batch loss: 0.12706148624420166\n",
      "Batch loss: 0.20711445808410645\n",
      "Batch loss: 0.17996791005134583\n",
      "Batch loss: 0.18678319454193115\n",
      "Batch loss: 0.1726343035697937\n",
      "Batch loss: 0.27753135561943054\n",
      "Batch loss: 0.1619730293750763\n",
      "Batch loss: 0.27376604080200195\n",
      "Batch loss: 0.1202545091509819\n",
      "Batch loss: 0.2562543451786041\n",
      "Batch loss: 0.110199935734272\n",
      "Batch loss: 0.13358433544635773\n",
      "Batch loss: 0.17815226316452026\n",
      "Batch loss: 0.15846829116344452\n",
      "Batch loss: 0.06856585294008255\n",
      "Batch loss: 0.12747952342033386\n",
      "Batch loss: 0.15853455662727356\n",
      "Batch loss: 0.07615174353122711\n",
      "Batch loss: 0.12122577428817749\n",
      "Batch loss: 0.08834923058748245\n",
      "Batch loss: 0.14573422074317932\n",
      "Batch loss: 0.2186817228794098\n",
      "Batch loss: 0.15227925777435303\n",
      "Batch loss: 0.15859900414943695\n",
      "Batch loss: 0.18884481489658356\n",
      "Batch loss: 0.2800033390522003\n",
      "Batch loss: 0.15296703577041626\n",
      "Batch loss: 0.0915699377655983\n",
      "Batch loss: 0.2071531116962433\n",
      "Batch loss: 0.16202180087566376\n",
      "Batch loss: 0.06191602349281311\n",
      "Batch loss: 0.10436685383319855\n",
      "Batch loss: 0.1668197214603424\n",
      "Batch loss: 0.1547071486711502\n",
      "Batch loss: 0.08229917287826538\n",
      "Batch loss: 0.2571519613265991\n",
      "Batch loss: 0.2315213829278946\n",
      "Batch loss: 0.18660806119441986\n",
      "Batch loss: 0.13814714550971985\n",
      "Batch loss: 0.1750727891921997\n",
      "Batch loss: 0.1291990578174591\n",
      "Batch loss: 0.20807278156280518\n",
      "Batch loss: 0.19703224301338196\n",
      "Batch loss: 0.15527470409870148\n",
      "Batch loss: 0.1896064430475235\n",
      "Batch loss: 0.15300464630126953\n",
      "Batch loss: 0.13232076168060303\n",
      "Batch loss: 0.25572314858436584\n",
      "Batch loss: 0.11368554830551147\n",
      "Batch loss: 0.2133418619632721\n",
      "Batch loss: 0.16309411823749542\n",
      "Batch loss: 0.11508830636739731\n",
      "Batch loss: 0.20678134262561798\n",
      "Batch loss: 0.1168971136212349\n",
      "Batch loss: 0.1781279295682907\n",
      "Batch loss: 0.11050373315811157\n",
      "Batch loss: 0.26312950253486633\n",
      "Batch loss: 0.24126961827278137\n",
      "Batch loss: 0.30113735795021057\n",
      "Batch loss: 0.14695033431053162\n",
      "Batch loss: 0.14162445068359375\n",
      "Batch loss: 0.17213675379753113\n",
      "Batch loss: 0.10643970966339111\n",
      "Batch loss: 0.1927337944507599\n",
      "Batch loss: 0.22057226300239563\n",
      "Batch loss: 0.13342295587062836\n",
      "Batch loss: 0.18931324779987335\n",
      "Batch loss: 0.1721189171075821\n",
      "Batch loss: 0.13887368142604828\n",
      "Batch loss: 0.16144363582134247\n",
      "Batch loss: 0.10474611818790436\n",
      "Batch loss: 0.14860057830810547\n",
      "Batch loss: 0.22075338661670685\n",
      "Batch loss: 0.2016427367925644\n",
      "Batch loss: 0.049918003380298615\n",
      "Batch loss: 0.12042955309152603\n",
      "Batch loss: 0.11230549216270447\n",
      "Batch loss: 0.09724043309688568\n",
      "Batch loss: 0.15353356301784515\n",
      "Batch loss: 0.10718949884176254\n",
      "Batch loss: 0.3088334798812866\n",
      "Batch loss: 0.1316714733839035\n",
      "Batch loss: 0.08406638354063034\n",
      "Batch loss: 0.27579742670059204\n",
      "Batch loss: 0.15876348316669464\n",
      "Batch loss: 0.13492433726787567\n",
      "Batch loss: 0.12475457787513733\n",
      "Batch loss: 0.2537353038787842\n",
      "Batch loss: 0.1513470560312271\n",
      "Batch loss: 0.14015915989875793\n",
      "Batch loss: 0.11100979149341583\n",
      "Batch loss: 0.10843025147914886\n",
      "Batch loss: 0.15618357062339783\n",
      "Batch loss: 0.12880541384220123\n",
      "Batch loss: 0.07316245138645172\n",
      "Epoch [2/100], Loss: 0.1740\n",
      "Validation Loss: 0.5058, Accuracy: 82.21%\n",
      "Batch loss: 0.1947905272245407\n",
      "Batch loss: 0.16819539666175842\n",
      "Batch loss: 0.1528187245130539\n",
      "Batch loss: 0.18612216413021088\n",
      "Batch loss: 0.16655384004116058\n",
      "Batch loss: 0.1381911337375641\n",
      "Batch loss: 0.3183665871620178\n",
      "Batch loss: 0.19173245131969452\n",
      "Batch loss: 0.21032890677452087\n",
      "Batch loss: 0.08724843710660934\n",
      "Batch loss: 0.1733834594488144\n",
      "Batch loss: 0.22138544917106628\n",
      "Batch loss: 0.17624154686927795\n",
      "Batch loss: 0.24855433404445648\n",
      "Batch loss: 0.22262020409107208\n",
      "Batch loss: 0.15355870127677917\n",
      "Batch loss: 0.14122441411018372\n",
      "Batch loss: 0.15942049026489258\n",
      "Batch loss: 0.12370185554027557\n",
      "Batch loss: 0.17224477231502533\n",
      "Batch loss: 0.11812577396631241\n",
      "Batch loss: 0.15050987899303436\n",
      "Batch loss: 0.1617792546749115\n",
      "Batch loss: 0.1312408745288849\n",
      "Batch loss: 0.11113253235816956\n",
      "Batch loss: 0.11411610245704651\n",
      "Batch loss: 0.1873987317085266\n",
      "Batch loss: 0.17256365716457367\n",
      "Batch loss: 0.1066928505897522\n",
      "Batch loss: 0.11730444431304932\n",
      "Batch loss: 0.23120391368865967\n",
      "Batch loss: 0.12547124922275543\n",
      "Batch loss: 0.1501987874507904\n",
      "Batch loss: 0.14135806262493134\n",
      "Batch loss: 0.08754784613847733\n",
      "Batch loss: 0.2106967568397522\n",
      "Batch loss: 0.174075186252594\n",
      "Batch loss: 0.13196603953838348\n",
      "Batch loss: 0.18846997618675232\n",
      "Batch loss: 0.10278044641017914\n",
      "Batch loss: 0.1271454095840454\n",
      "Batch loss: 0.1253942847251892\n",
      "Batch loss: 0.16948361694812775\n",
      "Batch loss: 0.10010960698127747\n",
      "Batch loss: 0.17084136605262756\n",
      "Batch loss: 0.1566249430179596\n",
      "Batch loss: 0.17502661049365997\n",
      "Batch loss: 0.11510083824396133\n",
      "Batch loss: 0.1101108193397522\n",
      "Batch loss: 0.10801530629396439\n",
      "Batch loss: 0.17265425622463226\n",
      "Batch loss: 0.14513438940048218\n",
      "Batch loss: 0.11251449584960938\n",
      "Batch loss: 0.10620275884866714\n",
      "Batch loss: 0.1615779995918274\n",
      "Batch loss: 0.1784450262784958\n",
      "Batch loss: 0.10571085661649704\n",
      "Batch loss: 0.12999433279037476\n",
      "Batch loss: 0.1359458714723587\n",
      "Batch loss: 0.14241589605808258\n",
      "Batch loss: 0.1416526734828949\n",
      "Batch loss: 0.1876390129327774\n",
      "Batch loss: 0.20123769342899323\n",
      "Batch loss: 0.16650518774986267\n",
      "Batch loss: 0.07154325395822525\n",
      "Batch loss: 0.17765194177627563\n",
      "Batch loss: 0.16332033276557922\n",
      "Batch loss: 0.2370263934135437\n",
      "Batch loss: 0.12557458877563477\n",
      "Batch loss: 0.126067653298378\n",
      "Batch loss: 0.17006149888038635\n",
      "Batch loss: 0.15973322093486786\n",
      "Batch loss: 0.1576201170682907\n",
      "Batch loss: 0.16489455103874207\n",
      "Batch loss: 0.13494889438152313\n",
      "Batch loss: 0.16058453917503357\n",
      "Batch loss: 0.17169031500816345\n",
      "Batch loss: 0.1869140863418579\n",
      "Batch loss: 0.12360239028930664\n",
      "Batch loss: 0.12338320910930634\n",
      "Batch loss: 0.08627720922231674\n",
      "Batch loss: 0.12036918848752975\n",
      "Batch loss: 0.21096688508987427\n",
      "Batch loss: 0.2715149223804474\n",
      "Batch loss: 0.15721702575683594\n",
      "Batch loss: 0.11301809549331665\n",
      "Batch loss: 0.08289988338947296\n",
      "Batch loss: 0.14262904226779938\n",
      "Batch loss: 0.11606442183256149\n",
      "Batch loss: 0.13393394649028778\n",
      "Batch loss: 0.18312659859657288\n",
      "Batch loss: 0.10311542451381683\n",
      "Batch loss: 0.2288682758808136\n",
      "Batch loss: 0.14088697731494904\n",
      "Batch loss: 0.15686097741127014\n",
      "Batch loss: 0.10724275559186935\n",
      "Batch loss: 0.08485576510429382\n",
      "Batch loss: 0.1927536129951477\n",
      "Batch loss: 0.10468742251396179\n",
      "Batch loss: 0.19274203479290009\n",
      "Batch loss: 0.12357514351606369\n",
      "Batch loss: 0.08005136996507645\n",
      "Batch loss: 0.1705760359764099\n",
      "Batch loss: 0.1699376106262207\n",
      "Batch loss: 0.13476869463920593\n",
      "Batch loss: 0.14053121209144592\n",
      "Batch loss: 0.146023228764534\n",
      "Batch loss: 0.139242485165596\n",
      "Batch loss: 0.1247846707701683\n",
      "Batch loss: 0.20364980399608612\n",
      "Batch loss: 0.17027495801448822\n",
      "Batch loss: 0.10016513615846634\n",
      "Batch loss: 0.12672819197177887\n",
      "Batch loss: 0.10818461328744888\n",
      "Batch loss: 0.15261247754096985\n",
      "Batch loss: 0.20590201020240784\n",
      "Batch loss: 0.10408961772918701\n",
      "Batch loss: 0.07741954177618027\n",
      "Batch loss: 0.10651550441980362\n",
      "Batch loss: 0.17393361032009125\n",
      "Batch loss: 0.16407161951065063\n",
      "Batch loss: 0.1354106217622757\n",
      "Batch loss: 0.19871801137924194\n",
      "Batch loss: 0.13306017220020294\n",
      "Batch loss: 0.18312673270702362\n",
      "Batch loss: 0.20809818804264069\n",
      "Batch loss: 0.13861456513404846\n",
      "Batch loss: 0.181304931640625\n",
      "Batch loss: 0.11808386445045471\n",
      "Batch loss: 0.1796528548002243\n",
      "Batch loss: 0.13415543735027313\n",
      "Batch loss: 0.2648790180683136\n",
      "Batch loss: 0.1259104162454605\n",
      "Batch loss: 0.18816322088241577\n",
      "Batch loss: 0.1863347440958023\n",
      "Batch loss: 0.08989737927913666\n",
      "Batch loss: 0.09609420597553253\n",
      "Batch loss: 0.14775653183460236\n",
      "Batch loss: 0.18327097594738007\n",
      "Batch loss: 0.05999903753399849\n",
      "Batch loss: 0.15961627662181854\n",
      "Batch loss: 0.10311850905418396\n",
      "Batch loss: 0.2800268828868866\n",
      "Batch loss: 0.20834267139434814\n",
      "Batch loss: 0.12208189815282822\n",
      "Batch loss: 0.11544051766395569\n",
      "Batch loss: 0.1462990790605545\n",
      "Batch loss: 0.1514996439218521\n",
      "Batch loss: 0.13158860802650452\n",
      "Batch loss: 0.0701671615242958\n",
      "Batch loss: 0.14552828669548035\n",
      "Batch loss: 0.22143936157226562\n",
      "Batch loss: 0.2253953218460083\n",
      "Batch loss: 0.19403968751430511\n",
      "Batch loss: 0.1776014268398285\n",
      "Batch loss: 0.1473030149936676\n",
      "Batch loss: 0.16750888526439667\n",
      "Batch loss: 0.13362133502960205\n",
      "Batch loss: 0.13568393886089325\n",
      "Batch loss: 0.1718103140592575\n",
      "Batch loss: 0.07647762447595596\n",
      "Batch loss: 0.18005889654159546\n",
      "Batch loss: 0.1670677363872528\n",
      "Batch loss: 0.14310774207115173\n",
      "Batch loss: 0.1754695475101471\n",
      "Batch loss: 0.16754767298698425\n",
      "Batch loss: 0.13884122669696808\n",
      "Batch loss: 0.13240420818328857\n",
      "Batch loss: 0.11120086908340454\n",
      "Batch loss: 0.12374602258205414\n",
      "Batch loss: 0.17395254969596863\n",
      "Batch loss: 0.21964995563030243\n",
      "Batch loss: 0.11926081031560898\n",
      "Batch loss: 0.11418278515338898\n",
      "Batch loss: 0.10037705302238464\n",
      "Batch loss: 0.20048640668392181\n",
      "Batch loss: 0.2348659336566925\n",
      "Batch loss: 0.08788270503282547\n",
      "Batch loss: 0.1655726432800293\n",
      "Batch loss: 0.13572533428668976\n",
      "Batch loss: 0.20656126737594604\n",
      "Batch loss: 0.11175023764371872\n",
      "Batch loss: 0.07012773305177689\n",
      "Batch loss: 0.22925174236297607\n",
      "Batch loss: 0.1931447833776474\n",
      "Batch loss: 0.1203811839222908\n",
      "Batch loss: 0.1737995743751526\n",
      "Batch loss: 0.12426076084375381\n",
      "Batch loss: 0.15020257234573364\n",
      "Batch loss: 0.13619771599769592\n",
      "Batch loss: 0.14048612117767334\n",
      "Batch loss: 0.10959003120660782\n",
      "Batch loss: 0.1717444509267807\n",
      "Batch loss: 0.12645214796066284\n",
      "Batch loss: 0.09869380295276642\n",
      "Batch loss: 0.11236552149057388\n",
      "Batch loss: 0.07928914576768875\n",
      "Batch loss: 0.15431666374206543\n",
      "Batch loss: 0.14614352583885193\n",
      "Batch loss: 0.11372487246990204\n",
      "Batch loss: 0.26636335253715515\n",
      "Batch loss: 0.21725662052631378\n",
      "Batch loss: 0.11163945496082306\n",
      "Batch loss: 0.08997367322444916\n",
      "Batch loss: 0.19344176352024078\n",
      "Batch loss: 0.2188423126935959\n",
      "Batch loss: 0.1733069270849228\n",
      "Batch loss: 0.15691734850406647\n",
      "Batch loss: 0.09574316442012787\n",
      "Batch loss: 0.112101249396801\n",
      "Batch loss: 0.09711963683366776\n",
      "Batch loss: 0.15528473258018494\n",
      "Batch loss: 0.13279402256011963\n",
      "Batch loss: 0.18068772554397583\n",
      "Batch loss: 0.18555046617984772\n",
      "Batch loss: 0.14167849719524384\n",
      "Batch loss: 0.11581128090620041\n",
      "Batch loss: 0.08372358232736588\n",
      "Batch loss: 0.052161045372486115\n",
      "Batch loss: 0.08159743249416351\n",
      "Batch loss: 0.07528036087751389\n",
      "Batch loss: 0.1473197340965271\n",
      "Batch loss: 0.07476092875003815\n",
      "Batch loss: 0.2133656144142151\n",
      "Batch loss: 0.07402577251195908\n",
      "Batch loss: 0.226079523563385\n",
      "Batch loss: 0.09844420105218887\n",
      "Batch loss: 0.1367570459842682\n",
      "Batch loss: 0.23105984926223755\n",
      "Batch loss: 0.0977594181895256\n",
      "Batch loss: 0.16721735894680023\n",
      "Batch loss: 0.1213957816362381\n",
      "Batch loss: 0.10432960838079453\n",
      "Batch loss: 0.110605888068676\n",
      "Batch loss: 0.11248520016670227\n",
      "Batch loss: 0.20245306193828583\n",
      "Batch loss: 0.0980505496263504\n",
      "Batch loss: 0.14713536202907562\n",
      "Batch loss: 0.13700056076049805\n",
      "Batch loss: 0.1327088475227356\n",
      "Batch loss: 0.1776490956544876\n",
      "Batch loss: 0.19735638797283173\n",
      "Batch loss: 0.09705200791358948\n",
      "Batch loss: 0.14158669114112854\n",
      "Batch loss: 0.08575678616762161\n",
      "Batch loss: 0.12765789031982422\n",
      "Batch loss: 0.11868646740913391\n",
      "Batch loss: 0.20272378623485565\n",
      "Batch loss: 0.1250396966934204\n",
      "Batch loss: 0.09054199606180191\n",
      "Batch loss: 0.22573378682136536\n",
      "Batch loss: 0.22104905545711517\n",
      "Batch loss: 0.20586900413036346\n",
      "Batch loss: 0.22384078800678253\n",
      "Batch loss: 0.18300585448741913\n",
      "Batch loss: 0.2832508683204651\n",
      "Batch loss: 0.18986818194389343\n",
      "Batch loss: 0.20619083940982819\n",
      "Batch loss: 0.13669191300868988\n",
      "Batch loss: 0.15645654499530792\n",
      "Batch loss: 0.14811007678508759\n",
      "Batch loss: 0.1346200704574585\n",
      "Batch loss: 0.1576227992773056\n",
      "Batch loss: 0.16694428026676178\n",
      "Batch loss: 0.15378010272979736\n",
      "Batch loss: 0.09883752465248108\n",
      "Batch loss: 0.21504376828670502\n",
      "Batch loss: 0.12773223221302032\n",
      "Batch loss: 0.17438484728336334\n",
      "Batch loss: 0.16808423399925232\n",
      "Batch loss: 0.15448185801506042\n",
      "Batch loss: 0.12959618866443634\n",
      "Batch loss: 0.0878407433629036\n",
      "Batch loss: 0.17846788465976715\n",
      "Batch loss: 0.20046336948871613\n",
      "Batch loss: 0.08654707670211792\n",
      "Batch loss: 0.1298138052225113\n",
      "Batch loss: 0.18934151530265808\n",
      "Batch loss: 0.15646561980247498\n",
      "Batch loss: 0.18580158054828644\n",
      "Batch loss: 0.13162539899349213\n",
      "Batch loss: 0.11484161764383316\n",
      "Batch loss: 0.18259978294372559\n",
      "Batch loss: 0.3299311101436615\n",
      "Batch loss: 0.17362958192825317\n",
      "Batch loss: 0.2149934023618698\n",
      "Batch loss: 0.13274358212947845\n",
      "Batch loss: 0.06691110879182816\n",
      "Batch loss: 0.08058623224496841\n",
      "Batch loss: 0.11463728547096252\n",
      "Batch loss: 0.11612053960561752\n",
      "Batch loss: 0.1040773019194603\n",
      "Batch loss: 0.09929224848747253\n",
      "Batch loss: 0.1487269252538681\n",
      "Batch loss: 0.14143404364585876\n",
      "Batch loss: 0.1834215223789215\n",
      "Batch loss: 0.13456128537654877\n",
      "Batch loss: 0.10370267927646637\n",
      "Batch loss: 0.18935906887054443\n",
      "Batch loss: 0.08753006905317307\n",
      "Batch loss: 0.14545458555221558\n",
      "Batch loss: 0.12304539233446121\n",
      "Batch loss: 0.1709243804216385\n",
      "Batch loss: 0.11313842236995697\n",
      "Batch loss: 0.23072656989097595\n",
      "Batch loss: 0.1534520983695984\n",
      "Batch loss: 0.047404695302248\n",
      "Batch loss: 0.1026083305478096\n",
      "Batch loss: 0.09809250384569168\n",
      "Batch loss: 0.1895352303981781\n",
      "Batch loss: 0.14995364844799042\n",
      "Batch loss: 0.1209753155708313\n",
      "Batch loss: 0.1104242131114006\n",
      "Batch loss: 0.1390644609928131\n",
      "Batch loss: 0.15075212717056274\n",
      "Batch loss: 0.2234869748353958\n",
      "Batch loss: 0.13887199759483337\n",
      "Batch loss: 0.10820132493972778\n",
      "Batch loss: 0.17376281321048737\n",
      "Batch loss: 0.14767123758792877\n",
      "Batch loss: 0.14393122494220734\n",
      "Batch loss: 0.10975197702646255\n",
      "Batch loss: 0.1274053305387497\n",
      "Batch loss: 0.19397766888141632\n",
      "Batch loss: 0.21691542863845825\n",
      "Batch loss: 0.24366194009780884\n",
      "Batch loss: 0.16935904324054718\n",
      "Batch loss: 0.2467009425163269\n",
      "Batch loss: 0.15713047981262207\n",
      "Batch loss: 0.2269371747970581\n",
      "Batch loss: 0.10492447018623352\n",
      "Batch loss: 0.23715656995773315\n",
      "Batch loss: 0.08622845262289047\n",
      "Batch loss: 0.16051696240901947\n",
      "Batch loss: 0.17573611438274384\n",
      "Batch loss: 0.13651934266090393\n",
      "Batch loss: 0.06360546499490738\n",
      "Batch loss: 0.08587534725666046\n",
      "Batch loss: 0.16158029437065125\n",
      "Batch loss: 0.06531555950641632\n",
      "Batch loss: 0.14892266690731049\n",
      "Batch loss: 0.11352669447660446\n",
      "Batch loss: 0.1263616383075714\n",
      "Batch loss: 0.12715661525726318\n",
      "Batch loss: 0.10730693489313126\n",
      "Batch loss: 0.15872524678707123\n",
      "Batch loss: 0.16179077327251434\n",
      "Batch loss: 0.23640500009059906\n",
      "Batch loss: 0.11655200272798538\n",
      "Batch loss: 0.10573761910200119\n",
      "Batch loss: 0.1520525962114334\n",
      "Batch loss: 0.15154814720153809\n",
      "Batch loss: 0.05429083853960037\n",
      "Batch loss: 0.0891808494925499\n",
      "Batch loss: 0.13533999025821686\n",
      "Batch loss: 0.1206449568271637\n",
      "Batch loss: 0.0683906301856041\n",
      "Batch loss: 0.20541761815547943\n",
      "Batch loss: 0.18231463432312012\n",
      "Batch loss: 0.20372436940670013\n",
      "Batch loss: 0.1577865034341812\n",
      "Batch loss: 0.15595661103725433\n",
      "Batch loss: 0.10386919230222702\n",
      "Batch loss: 0.18387004733085632\n",
      "Batch loss: 0.16963742673397064\n",
      "Batch loss: 0.17779649794101715\n",
      "Batch loss: 0.1843220293521881\n",
      "Batch loss: 0.13649649918079376\n",
      "Batch loss: 0.097507044672966\n",
      "Batch loss: 0.23111066222190857\n",
      "Batch loss: 0.1173049584031105\n",
      "Batch loss: 0.13114285469055176\n",
      "Batch loss: 0.14431267976760864\n",
      "Batch loss: 0.1080520749092102\n",
      "Batch loss: 0.16811542212963104\n",
      "Batch loss: 0.1353808343410492\n",
      "Batch loss: 0.16150310635566711\n",
      "Batch loss: 0.09406714141368866\n",
      "Batch loss: 0.18838240206241608\n",
      "Batch loss: 0.23216542601585388\n",
      "Batch loss: 0.28318795561790466\n",
      "Batch loss: 0.13195736706256866\n",
      "Batch loss: 0.13494369387626648\n",
      "Batch loss: 0.16729062795639038\n",
      "Batch loss: 0.06830968707799911\n",
      "Batch loss: 0.16045360267162323\n",
      "Batch loss: 0.17862999439239502\n",
      "Batch loss: 0.1291159838438034\n",
      "Batch loss: 0.13825416564941406\n",
      "Batch loss: 0.14066271483898163\n",
      "Batch loss: 0.10427497327327728\n",
      "Batch loss: 0.12346240878105164\n",
      "Batch loss: 0.09279973059892654\n",
      "Batch loss: 0.09768830984830856\n",
      "Batch loss: 0.25032177567481995\n",
      "Batch loss: 0.2220182567834854\n",
      "Batch loss: 0.03827718272805214\n",
      "Batch loss: 0.10252100229263306\n",
      "Batch loss: 0.11530086398124695\n",
      "Batch loss: 0.10442782193422318\n",
      "Batch loss: 0.15571869909763336\n",
      "Batch loss: 0.10591530799865723\n",
      "Batch loss: 0.24509981274604797\n",
      "Batch loss: 0.09669990092515945\n",
      "Batch loss: 0.07539796084165573\n",
      "Batch loss: 0.26134195923805237\n",
      "Batch loss: 0.19465632736682892\n",
      "Batch loss: 0.10956308245658875\n",
      "Batch loss: 0.09692779183387756\n",
      "Batch loss: 0.17841589450836182\n",
      "Batch loss: 0.13940317928791046\n",
      "Batch loss: 0.10113153606653214\n",
      "Batch loss: 0.08404005318880081\n",
      "Batch loss: 0.12307323515415192\n",
      "Batch loss: 0.1184074729681015\n",
      "Batch loss: 0.1204688549041748\n",
      "Batch loss: 0.07790600508451462\n",
      "Epoch [3/100], Loss: 0.1484\n",
      "Validation Loss: 0.3704, Accuracy: 89.96%\n",
      "Batch loss: 0.19434478878974915\n",
      "Batch loss: 0.14202386140823364\n",
      "Batch loss: 0.16315969824790955\n",
      "Batch loss: 0.16127489507198334\n",
      "Batch loss: 0.1774362027645111\n",
      "Batch loss: 0.12272977828979492\n",
      "Batch loss: 0.39517274498939514\n",
      "Batch loss: 0.1937500238418579\n",
      "Batch loss: 0.22144486010074615\n",
      "Batch loss: 0.10091904550790787\n",
      "Batch loss: 0.15333403646945953\n",
      "Batch loss: 0.17207680642604828\n",
      "Batch loss: 0.15094533562660217\n",
      "Batch loss: 0.21144360303878784\n",
      "Batch loss: 0.1789136379957199\n",
      "Batch loss: 0.10961101204156876\n",
      "Batch loss: 0.1135312169790268\n",
      "Batch loss: 0.1685650646686554\n",
      "Batch loss: 0.09199509024620056\n",
      "Batch loss: 0.17466607689857483\n",
      "Batch loss: 0.12287035584449768\n",
      "Batch loss: 0.12489605695009232\n",
      "Batch loss: 0.1541762351989746\n",
      "Batch loss: 0.13214856386184692\n",
      "Batch loss: 0.10679110139608383\n",
      "Batch loss: 0.1068950816988945\n",
      "Batch loss: 0.1556866317987442\n",
      "Batch loss: 0.1450541764497757\n",
      "Batch loss: 0.12377851456403732\n",
      "Batch loss: 0.1486157476902008\n",
      "Batch loss: 0.22349382936954498\n",
      "Batch loss: 0.1320924162864685\n",
      "Batch loss: 0.14223158359527588\n",
      "Batch loss: 0.10630961507558823\n",
      "Batch loss: 0.08684263378381729\n",
      "Batch loss: 0.17055007815361023\n",
      "Batch loss: 0.16981254518032074\n",
      "Batch loss: 0.11834581941366196\n",
      "Batch loss: 0.20076288282871246\n",
      "Batch loss: 0.09739714115858078\n",
      "Batch loss: 0.10407036542892456\n",
      "Batch loss: 0.16067790985107422\n",
      "Batch loss: 0.15060243010520935\n",
      "Batch loss: 0.10031287372112274\n",
      "Batch loss: 0.1513127088546753\n",
      "Batch loss: 0.12496913224458694\n",
      "Batch loss: 0.13319134712219238\n",
      "Batch loss: 0.09773474931716919\n",
      "Batch loss: 0.10034746676683426\n",
      "Batch loss: 0.10576997697353363\n",
      "Batch loss: 0.15114614367485046\n",
      "Batch loss: 0.13324497640132904\n",
      "Batch loss: 0.0848967432975769\n",
      "Batch loss: 0.12460532784461975\n",
      "Batch loss: 0.12364691495895386\n",
      "Batch loss: 0.1671219915151596\n",
      "Batch loss: 0.1102132797241211\n",
      "Batch loss: 0.13541777431964874\n",
      "Batch loss: 0.10393322259187698\n",
      "Batch loss: 0.10217303782701492\n",
      "Batch loss: 0.11949378252029419\n",
      "Batch loss: 0.12302641570568085\n",
      "Batch loss: 0.14522318542003632\n",
      "Batch loss: 0.1705445647239685\n",
      "Batch loss: 0.0496794618666172\n",
      "Batch loss: 0.18646378815174103\n",
      "Batch loss: 0.1603255271911621\n",
      "Batch loss: 0.22222177684307098\n",
      "Batch loss: 0.11011292040348053\n",
      "Batch loss: 0.11606468260288239\n",
      "Batch loss: 0.19604633748531342\n",
      "Batch loss: 0.1063530445098877\n",
      "Batch loss: 0.07920457422733307\n",
      "Batch loss: 0.11604027450084686\n",
      "Batch loss: 0.09990426152944565\n",
      "Batch loss: 0.11221471428871155\n",
      "Batch loss: 0.1793370246887207\n",
      "Batch loss: 0.12700577080249786\n",
      "Batch loss: 0.18701498210430145\n",
      "Batch loss: 0.13325613737106323\n",
      "Batch loss: 0.05775531381368637\n",
      "Batch loss: 0.11028163880109787\n",
      "Batch loss: 0.14502303302288055\n",
      "Batch loss: 0.2393931746482849\n",
      "Batch loss: 0.13208255171775818\n",
      "Batch loss: 0.1417226344347\n",
      "Batch loss: 0.04526040330529213\n",
      "Batch loss: 0.1396530568599701\n",
      "Batch loss: 0.08377759158611298\n",
      "Batch loss: 0.11906597018241882\n",
      "Batch loss: 0.09232339262962341\n",
      "Batch loss: 0.08669035136699677\n",
      "Batch loss: 0.22264893352985382\n",
      "Batch loss: 0.13645507395267487\n",
      "Batch loss: 0.0988842025399208\n",
      "Batch loss: 0.1097281202673912\n",
      "Batch loss: 0.08734071999788284\n",
      "Batch loss: 0.14175665378570557\n",
      "Batch loss: 0.11867351084947586\n",
      "Batch loss: 0.13462166488170624\n",
      "Batch loss: 0.10477247834205627\n",
      "Batch loss: 0.05323827266693115\n",
      "Batch loss: 0.10808142274618149\n",
      "Batch loss: 0.13339728116989136\n",
      "Batch loss: 0.11727025359869003\n",
      "Batch loss: 0.11228708177804947\n",
      "Batch loss: 0.1304335594177246\n",
      "Batch loss: 0.11134480684995651\n",
      "Batch loss: 0.10521209985017776\n",
      "Batch loss: 0.18023677170276642\n",
      "Batch loss: 0.125999316573143\n",
      "Batch loss: 0.07537516951560974\n",
      "Batch loss: 0.07205288857221603\n",
      "Batch loss: 0.06349208950996399\n",
      "Batch loss: 0.10203872621059418\n",
      "Batch loss: 0.15101680159568787\n",
      "Batch loss: 0.08040563762187958\n",
      "Batch loss: 0.04813196137547493\n",
      "Batch loss: 0.07648131251335144\n",
      "Batch loss: 0.12539653480052948\n",
      "Batch loss: 0.0949835553765297\n",
      "Batch loss: 0.1079007014632225\n",
      "Batch loss: 0.1843888759613037\n",
      "Batch loss: 0.12157144397497177\n",
      "Batch loss: 0.14935865998268127\n",
      "Batch loss: 0.20261125266551971\n",
      "Batch loss: 0.11739549785852432\n",
      "Batch loss: 0.11234992742538452\n",
      "Batch loss: 0.11570877581834793\n",
      "Batch loss: 0.16865329444408417\n",
      "Batch loss: 0.142010897397995\n",
      "Batch loss: 0.20124724507331848\n",
      "Batch loss: 0.14510034024715424\n",
      "Batch loss: 0.15601789951324463\n",
      "Batch loss: 0.20083190500736237\n",
      "Batch loss: 0.08950305730104446\n",
      "Batch loss: 0.09774269908666611\n",
      "Batch loss: 0.163574680685997\n",
      "Batch loss: 0.1540856659412384\n",
      "Batch loss: 0.07532874494791031\n",
      "Batch loss: 0.09933508932590485\n",
      "Batch loss: 0.06522300094366074\n",
      "Batch loss: 0.16225649416446686\n",
      "Batch loss: 0.16028879582881927\n",
      "Batch loss: 0.1059064120054245\n",
      "Batch loss: 0.11389182507991791\n",
      "Batch loss: 0.1411377191543579\n",
      "Batch loss: 0.11383292078971863\n",
      "Batch loss: 0.1505146622657776\n",
      "Batch loss: 0.12556737661361694\n",
      "Batch loss: 0.1482195109128952\n",
      "Batch loss: 0.2091095894575119\n",
      "Batch loss: 0.15786342322826385\n",
      "Batch loss: 0.1939120590686798\n",
      "Batch loss: 0.23829485476016998\n",
      "Batch loss: 0.14635342359542847\n",
      "Batch loss: 0.16242195665836334\n",
      "Batch loss: 0.11720401793718338\n",
      "Batch loss: 0.12336292862892151\n",
      "Batch loss: 0.1608128398656845\n",
      "Batch loss: 0.07548673450946808\n",
      "Batch loss: 0.16810858249664307\n",
      "Batch loss: 0.12588746845722198\n",
      "Batch loss: 0.15713010728359222\n",
      "Batch loss: 0.08789798617362976\n",
      "Batch loss: 0.16072580218315125\n",
      "Batch loss: 0.13408035039901733\n",
      "Batch loss: 0.13669084012508392\n",
      "Batch loss: 0.09495886415243149\n",
      "Batch loss: 0.13811953365802765\n",
      "Batch loss: 0.13090385496616364\n",
      "Batch loss: 0.14696913957595825\n",
      "Batch loss: 0.08573665469884872\n",
      "Batch loss: 0.13784365355968475\n",
      "Batch loss: 0.09012769162654877\n",
      "Batch loss: 0.20785817503929138\n",
      "Batch loss: 0.2539510428905487\n",
      "Batch loss: 0.10769177973270416\n",
      "Batch loss: 0.18128187954425812\n",
      "Batch loss: 0.10705394297838211\n",
      "Batch loss: 0.16412317752838135\n",
      "Batch loss: 0.07460524886846542\n",
      "Batch loss: 0.07536632567644119\n",
      "Batch loss: 0.22123932838439941\n",
      "Batch loss: 0.15447452664375305\n",
      "Batch loss: 0.10428960621356964\n",
      "Batch loss: 0.12205716967582703\n",
      "Batch loss: 0.1069595068693161\n",
      "Batch loss: 0.1743413209915161\n",
      "Batch loss: 0.11018095165491104\n",
      "Batch loss: 0.11148908734321594\n",
      "Batch loss: 0.10936243832111359\n",
      "Batch loss: 0.10650575906038284\n",
      "Batch loss: 0.13884052634239197\n",
      "Batch loss: 0.09866483509540558\n",
      "Batch loss: 0.11535096913576126\n",
      "Batch loss: 0.08334276080131531\n",
      "Batch loss: 0.16777874529361725\n",
      "Batch loss: 0.14670248329639435\n",
      "Batch loss: 0.09572437405586243\n",
      "Batch loss: 0.17861349880695343\n",
      "Batch loss: 0.20389324426651\n",
      "Batch loss: 0.1073625236749649\n",
      "Batch loss: 0.09371308982372284\n",
      "Batch loss: 0.1740705668926239\n",
      "Batch loss: 0.23187097907066345\n",
      "Batch loss: 0.14830559492111206\n",
      "Batch loss: 0.12404117733240128\n",
      "Batch loss: 0.07791193574666977\n",
      "Batch loss: 0.11107054352760315\n",
      "Batch loss: 0.13107648491859436\n",
      "Batch loss: 0.1063447892665863\n",
      "Batch loss: 0.12147428095340729\n",
      "Batch loss: 0.18267671763896942\n",
      "Batch loss: 0.146108016371727\n",
      "Batch loss: 0.07126434892416\n",
      "Batch loss: 0.09385448694229126\n",
      "Batch loss: 0.1259724646806717\n",
      "Batch loss: 0.05324413254857063\n",
      "Batch loss: 0.10741163790225983\n",
      "Batch loss: 0.04204734042286873\n",
      "Batch loss: 0.10710414499044418\n",
      "Batch loss: 0.072437584400177\n",
      "Batch loss: 0.24095626175403595\n",
      "Batch loss: 0.08999231457710266\n",
      "Batch loss: 0.20002296566963196\n",
      "Batch loss: 0.14025376737117767\n",
      "Batch loss: 0.1624036282300949\n",
      "Batch loss: 0.2758675813674927\n",
      "Batch loss: 0.0730845183134079\n",
      "Batch loss: 0.2086375206708908\n",
      "Batch loss: 0.12139002978801727\n",
      "Batch loss: 0.10884829610586166\n",
      "Batch loss: 0.09500854462385178\n",
      "Batch loss: 0.11343256384134293\n",
      "Batch loss: 0.15939843654632568\n",
      "Batch loss: 0.10110950469970703\n",
      "Batch loss: 0.2385546863079071\n",
      "Batch loss: 0.07872097194194794\n",
      "Batch loss: 0.1538958102464676\n",
      "Batch loss: 0.1479630172252655\n",
      "Batch loss: 0.16492219269275665\n",
      "Batch loss: 0.059465378522872925\n",
      "Batch loss: 0.09230312705039978\n",
      "Batch loss: 0.10370504856109619\n",
      "Batch loss: 0.08355285972356796\n",
      "Batch loss: 0.16026794910430908\n",
      "Batch loss: 0.16823174059391022\n",
      "Batch loss: 0.13550728559494019\n",
      "Batch loss: 0.08655353635549545\n",
      "Batch loss: 0.1585378348827362\n",
      "Batch loss: 0.1368158608675003\n",
      "Batch loss: 0.20520596206188202\n",
      "Batch loss: 0.19102062284946442\n",
      "Batch loss: 0.12903201580047607\n",
      "Batch loss: 0.34059056639671326\n",
      "Batch loss: 0.12629267573356628\n",
      "Batch loss: 0.1639518141746521\n",
      "Batch loss: 0.12031298130750656\n",
      "Batch loss: 0.10635878890752792\n",
      "Batch loss: 0.13182547688484192\n",
      "Batch loss: 0.11194270104169846\n",
      "Batch loss: 0.19167505204677582\n",
      "Batch loss: 0.12306477129459381\n",
      "Batch loss: 0.14760035276412964\n",
      "Batch loss: 0.08676900714635849\n",
      "Batch loss: 0.12508976459503174\n",
      "Batch loss: 0.11474272608757019\n",
      "Batch loss: 0.13598483800888062\n",
      "Batch loss: 0.10074455291032791\n",
      "Batch loss: 0.14534185826778412\n",
      "Batch loss: 0.15399090945720673\n",
      "Batch loss: 0.0986686572432518\n",
      "Batch loss: 0.16357485949993134\n",
      "Batch loss: 0.1798778772354126\n",
      "Batch loss: 0.08455338329076767\n",
      "Batch loss: 0.11379443109035492\n",
      "Batch loss: 0.19085387885570526\n",
      "Batch loss: 0.12145008891820908\n",
      "Batch loss: 0.14573267102241516\n",
      "Batch loss: 0.09493451565504074\n",
      "Batch loss: 0.10153058171272278\n",
      "Batch loss: 0.18784229457378387\n",
      "Batch loss: 0.2388477623462677\n",
      "Batch loss: 0.13740400969982147\n",
      "Batch loss: 0.17988330125808716\n",
      "Batch loss: 0.11310435086488724\n",
      "Batch loss: 0.07934308797121048\n",
      "Batch loss: 0.10829479992389679\n",
      "Batch loss: 0.13602855801582336\n",
      "Batch loss: 0.09743617475032806\n",
      "Batch loss: 0.10725319385528564\n",
      "Batch loss: 0.07313039898872375\n",
      "Batch loss: 0.10438968241214752\n",
      "Batch loss: 0.12411753833293915\n",
      "Batch loss: 0.1608414649963379\n",
      "Batch loss: 0.10602685064077377\n",
      "Batch loss: 0.11762065440416336\n",
      "Batch loss: 0.15594540536403656\n",
      "Batch loss: 0.08774907886981964\n",
      "Batch loss: 0.12810219824314117\n",
      "Batch loss: 0.1001027524471283\n",
      "Batch loss: 0.09089861810207367\n",
      "Batch loss: 0.07491880655288696\n",
      "Batch loss: 0.15443256497383118\n",
      "Batch loss: 0.09057410061359406\n",
      "Batch loss: 0.04629181697964668\n",
      "Batch loss: 0.08252686262130737\n",
      "Batch loss: 0.07524758577346802\n",
      "Batch loss: 0.11684096604585648\n",
      "Batch loss: 0.09723576158285141\n",
      "Batch loss: 0.11396051943302155\n",
      "Batch loss: 0.1214720606803894\n",
      "Batch loss: 0.14817826449871063\n",
      "Batch loss: 0.18311820924282074\n",
      "Batch loss: 0.2185794860124588\n",
      "Batch loss: 0.14080967009067535\n",
      "Batch loss: 0.09429625421762466\n",
      "Batch loss: 0.1441301852464676\n",
      "Batch loss: 0.09771847724914551\n",
      "Batch loss: 0.13108009099960327\n",
      "Batch loss: 0.1084291934967041\n",
      "Batch loss: 0.08135105669498444\n",
      "Batch loss: 0.2117292732000351\n",
      "Batch loss: 0.15827102959156036\n",
      "Batch loss: 0.1677199900150299\n",
      "Batch loss: 0.13823342323303223\n",
      "Batch loss: 0.23280012607574463\n",
      "Batch loss: 0.14062923192977905\n",
      "Batch loss: 0.18317165970802307\n",
      "Batch loss: 0.10025264322757721\n",
      "Batch loss: 0.15181857347488403\n",
      "Batch loss: 0.06621142476797104\n",
      "Batch loss: 0.1299038678407669\n",
      "Batch loss: 0.1550927609205246\n",
      "Batch loss: 0.13046707212924957\n",
      "Batch loss: 0.04721846058964729\n",
      "Batch loss: 0.05864877998828888\n",
      "Batch loss: 0.19363798201084137\n",
      "Batch loss: 0.08128097653388977\n",
      "Batch loss: 0.16287364065647125\n",
      "Batch loss: 0.04002343490719795\n",
      "Batch loss: 0.08898545056581497\n",
      "Batch loss: 0.2090330719947815\n",
      "Batch loss: 0.0842767208814621\n",
      "Batch loss: 0.13784478604793549\n",
      "Batch loss: 0.1266057938337326\n",
      "Batch loss: 0.17697830498218536\n",
      "Batch loss: 0.14455518126487732\n",
      "Batch loss: 0.08968810737133026\n",
      "Batch loss: 0.14323188364505768\n",
      "Batch loss: 0.1752937287092209\n",
      "Batch loss: 0.05285888537764549\n",
      "Batch loss: 0.08269981294870377\n",
      "Batch loss: 0.12570805847644806\n",
      "Batch loss: 0.08005575090646744\n",
      "Batch loss: 0.07534430921077728\n",
      "Batch loss: 0.1372273713350296\n",
      "Batch loss: 0.21455185115337372\n",
      "Batch loss: 0.1698741465806961\n",
      "Batch loss: 0.085417740046978\n",
      "Batch loss: 0.1449529528617859\n",
      "Batch loss: 0.09038182348012924\n",
      "Batch loss: 0.15208496153354645\n",
      "Batch loss: 0.17286460101604462\n",
      "Batch loss: 0.140803262591362\n",
      "Batch loss: 0.18355661630630493\n",
      "Batch loss: 0.11974058300256729\n",
      "Batch loss: 0.07583144307136536\n",
      "Batch loss: 0.19359901547431946\n",
      "Batch loss: 0.0766930803656578\n",
      "Batch loss: 0.12041478604078293\n",
      "Batch loss: 0.13739943504333496\n",
      "Batch loss: 0.08640880882740021\n",
      "Batch loss: 0.15841756761074066\n",
      "Batch loss: 0.11413796991109848\n",
      "Batch loss: 0.1658376008272171\n",
      "Batch loss: 0.07588714361190796\n",
      "Batch loss: 0.15472181141376495\n",
      "Batch loss: 0.20606742799282074\n",
      "Batch loss: 0.2545621693134308\n",
      "Batch loss: 0.0719425156712532\n",
      "Batch loss: 0.11143900454044342\n",
      "Batch loss: 0.1414160430431366\n",
      "Batch loss: 0.06783686578273773\n",
      "Batch loss: 0.11588361859321594\n",
      "Batch loss: 0.14842131733894348\n",
      "Batch loss: 0.13272714614868164\n",
      "Batch loss: 0.09168878942728043\n",
      "Batch loss: 0.15356232225894928\n",
      "Batch loss: 0.10205775499343872\n",
      "Batch loss: 0.10978256165981293\n",
      "Batch loss: 0.08811258524656296\n",
      "Batch loss: 0.10049019753932953\n",
      "Batch loss: 0.2028723806142807\n",
      "Batch loss: 0.15768426656723022\n",
      "Batch loss: 0.05405905097723007\n",
      "Batch loss: 0.06901586800813675\n",
      "Batch loss: 0.09811956435441971\n",
      "Batch loss: 0.08015937358140945\n",
      "Batch loss: 0.0885651633143425\n",
      "Batch loss: 0.06952209770679474\n",
      "Batch loss: 0.2070648968219757\n",
      "Batch loss: 0.10098940134048462\n",
      "Batch loss: 0.0653698742389679\n",
      "Batch loss: 0.19887635111808777\n",
      "Batch loss: 0.13969115912914276\n",
      "Batch loss: 0.10855643451213837\n",
      "Batch loss: 0.07743274420499802\n",
      "Batch loss: 0.1220807358622551\n",
      "Batch loss: 0.1381692737340927\n",
      "Batch loss: 0.1236405149102211\n",
      "Batch loss: 0.07297436147928238\n",
      "Batch loss: 0.06183542683720589\n",
      "Batch loss: 0.096083864569664\n",
      "Batch loss: 0.08029495924711227\n",
      "Batch loss: 0.07361117005348206\n",
      "Epoch [4/100], Loss: 0.1309\n",
      "Validation Loss: 0.2271, Accuracy: 93.07%\n",
      "Batch loss: 0.19820000231266022\n",
      "Batch loss: 0.1180153414607048\n",
      "Batch loss: 0.13019855320453644\n",
      "Batch loss: 0.11101395636796951\n",
      "Batch loss: 0.11393362283706665\n",
      "Batch loss: 0.1414889246225357\n",
      "Batch loss: 0.14994236826896667\n",
      "Batch loss: 0.16764220595359802\n",
      "Batch loss: 0.18735729157924652\n",
      "Batch loss: 0.06166916713118553\n",
      "Batch loss: 0.1330062597990036\n",
      "Batch loss: 0.14867348968982697\n",
      "Batch loss: 0.13225804269313812\n",
      "Batch loss: 0.17459674179553986\n",
      "Batch loss: 0.12744355201721191\n",
      "Batch loss: 0.18747900426387787\n",
      "Batch loss: 0.173658087849617\n",
      "Batch loss: 0.14708083868026733\n",
      "Batch loss: 0.051453959196805954\n",
      "Batch loss: 0.12997572124004364\n",
      "Batch loss: 0.10783787816762924\n",
      "Batch loss: 0.15735986828804016\n",
      "Batch loss: 0.08382037281990051\n",
      "Batch loss: 0.12054366618394852\n",
      "Batch loss: 0.07672969251871109\n",
      "Batch loss: 0.08521717041730881\n",
      "Batch loss: 0.20571187138557434\n",
      "Batch loss: 0.17617885768413544\n",
      "Batch loss: 0.10074760764837265\n",
      "Batch loss: 0.14432542026042938\n",
      "Batch loss: 0.18907135725021362\n",
      "Batch loss: 0.12088015675544739\n",
      "Batch loss: 0.11750064045190811\n",
      "Batch loss: 0.09289874136447906\n",
      "Batch loss: 0.0937923863530159\n",
      "Batch loss: 0.24430271983146667\n",
      "Batch loss: 0.10503928363323212\n",
      "Batch loss: 0.10559718310832977\n",
      "Batch loss: 0.15875841677188873\n",
      "Batch loss: 0.11213365942239761\n",
      "Batch loss: 0.09148862957954407\n",
      "Batch loss: 0.1122298315167427\n",
      "Batch loss: 0.15166737139225006\n",
      "Batch loss: 0.09801378101110458\n",
      "Batch loss: 0.17760668694972992\n",
      "Batch loss: 0.16407793760299683\n",
      "Batch loss: 0.19397269189357758\n",
      "Batch loss: 0.08651437610387802\n",
      "Batch loss: 0.11062589287757874\n",
      "Batch loss: 0.14240142703056335\n",
      "Batch loss: 0.16132162511348724\n",
      "Batch loss: 0.1249566525220871\n",
      "Batch loss: 0.06912826746702194\n",
      "Batch loss: 0.10395272821187973\n",
      "Batch loss: 0.12659260630607605\n",
      "Batch loss: 0.17690472304821014\n",
      "Batch loss: 0.09031356126070023\n",
      "Batch loss: 0.12995797395706177\n",
      "Batch loss: 0.08682303875684738\n",
      "Batch loss: 0.09510158002376556\n",
      "Batch loss: 0.12023796141147614\n",
      "Batch loss: 0.12756207585334778\n",
      "Batch loss: 0.18220703303813934\n",
      "Batch loss: 0.20918336510658264\n",
      "Batch loss: 0.08631190657615662\n",
      "Batch loss: 0.19473639130592346\n",
      "Batch loss: 0.14635439217090607\n",
      "Batch loss: 0.17644815146923065\n",
      "Batch loss: 0.09445949643850327\n",
      "Batch loss: 0.1354140043258667\n",
      "Batch loss: 0.10298432409763336\n",
      "Batch loss: 0.116899773478508\n",
      "Batch loss: 0.06536293029785156\n",
      "Batch loss: 0.1047847792506218\n",
      "Batch loss: 0.07598467171192169\n",
      "Batch loss: 0.1349603533744812\n",
      "Batch loss: 0.1462697833776474\n",
      "Batch loss: 0.1442755162715912\n",
      "Batch loss: 0.09931450337171555\n",
      "Batch loss: 0.1227927878499031\n",
      "Batch loss: 0.040860142558813095\n",
      "Batch loss: 0.09077171981334686\n",
      "Batch loss: 0.1112256869673729\n",
      "Batch loss: 0.17013351619243622\n",
      "Batch loss: 0.08647127449512482\n",
      "Batch loss: 0.11513735353946686\n",
      "Batch loss: 0.057839278131723404\n",
      "Batch loss: 0.10591989755630493\n",
      "Batch loss: 0.08764775097370148\n",
      "Batch loss: 0.08971837162971497\n",
      "Batch loss: 0.06239903345704079\n",
      "Batch loss: 0.08682898432016373\n",
      "Batch loss: 0.16996026039123535\n",
      "Batch loss: 0.1143123134970665\n",
      "Batch loss: 0.13198620080947876\n",
      "Batch loss: 0.08363978564739227\n",
      "Batch loss: 0.05196205899119377\n",
      "Batch loss: 0.15655019879341125\n",
      "Batch loss: 0.07476536929607391\n",
      "Batch loss: 0.09080471843481064\n",
      "Batch loss: 0.10251377522945404\n",
      "Batch loss: 0.06852021813392639\n",
      "Batch loss: 0.1343705952167511\n",
      "Batch loss: 0.13181424140930176\n",
      "Batch loss: 0.07502780109643936\n",
      "Batch loss: 0.133985698223114\n",
      "Batch loss: 0.10214482247829437\n",
      "Batch loss: 0.13288284838199615\n",
      "Batch loss: 0.06519950181245804\n",
      "Batch loss: 0.17643247544765472\n",
      "Batch loss: 0.10675884783267975\n",
      "Batch loss: 0.06256302446126938\n",
      "Batch loss: 0.05898250266909599\n",
      "Batch loss: 0.06386712938547134\n",
      "Batch loss: 0.08058863133192062\n",
      "Batch loss: 0.1364719718694687\n",
      "Batch loss: 0.08872196078300476\n",
      "Batch loss: 0.06934726238250732\n",
      "Batch loss: 0.07245194911956787\n",
      "Batch loss: 0.1007823646068573\n",
      "Batch loss: 0.07397298514842987\n",
      "Batch loss: 0.09258917719125748\n",
      "Batch loss: 0.19199667870998383\n",
      "Batch loss: 0.11757943779230118\n",
      "Batch loss: 0.17422033846378326\n",
      "Batch loss: 0.10937164723873138\n",
      "Batch loss: 0.09800668060779572\n",
      "Batch loss: 0.09196290373802185\n",
      "Batch loss: 0.057631850242614746\n",
      "Batch loss: 0.09986457973718643\n",
      "Batch loss: 0.11852293461561203\n",
      "Batch loss: 0.0847693607211113\n",
      "Batch loss: 0.09633580595254898\n",
      "Batch loss: 0.12240196019411087\n",
      "Batch loss: 0.13233976066112518\n",
      "Batch loss: 0.06486213207244873\n",
      "Batch loss: 0.08831587433815002\n",
      "Batch loss: 0.13870477676391602\n",
      "Batch loss: 0.11468309164047241\n",
      "Batch loss: 0.06268841028213501\n",
      "Batch loss: 0.08682341128587723\n",
      "Batch loss: 0.07671039551496506\n",
      "Batch loss: 0.08834004402160645\n",
      "Batch loss: 0.0920242965221405\n",
      "Batch loss: 0.06086776778101921\n",
      "Batch loss: 0.0808677151799202\n",
      "Batch loss: 0.06910154223442078\n",
      "Batch loss: 0.0828709527850151\n",
      "Batch loss: 0.05835577845573425\n",
      "Batch loss: 0.08857202529907227\n",
      "Batch loss: 0.20465685427188873\n",
      "Batch loss: 0.11138564348220825\n",
      "Batch loss: 0.22268085181713104\n",
      "Batch loss: 0.12908631563186646\n",
      "Batch loss: 0.11930247396230698\n",
      "Batch loss: 0.17921817302703857\n",
      "Batch loss: 0.1756250560283661\n",
      "Batch loss: 0.1688043773174286\n",
      "Batch loss: 0.12514416873455048\n",
      "Batch loss: 0.12160974740982056\n",
      "Batch loss: 0.08283432573080063\n",
      "Batch loss: 0.17645007371902466\n",
      "Batch loss: 0.10286328196525574\n",
      "Batch loss: 0.13269759714603424\n",
      "Batch loss: 0.08350571244955063\n",
      "Batch loss: 0.18756599724292755\n",
      "Batch loss: 0.08610236644744873\n",
      "Batch loss: 0.08097574859857559\n",
      "Batch loss: 0.08095724135637283\n",
      "Batch loss: 0.12861119210720062\n",
      "Batch loss: 0.15675467252731323\n",
      "Batch loss: 0.12655134499073029\n",
      "Batch loss: 0.09000904858112335\n",
      "Batch loss: 0.10621267557144165\n",
      "Batch loss: 0.10962089896202087\n",
      "Batch loss: 0.13533362746238708\n",
      "Batch loss: 0.15373145043849945\n",
      "Batch loss: 0.07956931740045547\n",
      "Batch loss: 0.14919176697731018\n",
      "Batch loss: 0.0670366883277893\n",
      "Batch loss: 0.12622635066509247\n",
      "Batch loss: 0.06874644756317139\n",
      "Batch loss: 0.06628493964672089\n",
      "Batch loss: 0.20954836905002594\n",
      "Batch loss: 0.14391465485095978\n",
      "Batch loss: 0.12597574293613434\n",
      "Batch loss: 0.1427319198846817\n",
      "Batch loss: 0.10826940834522247\n",
      "Batch loss: 0.14509651064872742\n",
      "Batch loss: 0.10881012678146362\n",
      "Batch loss: 0.1271854192018509\n",
      "Batch loss: 0.09831932932138443\n",
      "Batch loss: 0.17392629384994507\n",
      "Batch loss: 0.12575127184391022\n",
      "Batch loss: 0.11332381516695023\n",
      "Batch loss: 0.10159695148468018\n",
      "Batch loss: 0.10878129303455353\n",
      "Batch loss: 0.11957813799381256\n",
      "Batch loss: 0.12165194004774094\n",
      "Batch loss: 0.14464055001735687\n",
      "Batch loss: 0.13784383237361908\n",
      "Batch loss: 0.18454955518245697\n",
      "Batch loss: 0.08319619297981262\n",
      "Batch loss: 0.10598405450582504\n",
      "Batch loss: 0.1386171579360962\n",
      "Batch loss: 0.17116008698940277\n",
      "Batch loss: 0.09923708438873291\n",
      "Batch loss: 0.11766048520803452\n",
      "Batch loss: 0.08631139248609543\n",
      "Batch loss: 0.08810834586620331\n",
      "Batch loss: 0.13025932013988495\n",
      "Batch loss: 0.12474872916936874\n",
      "Batch loss: 0.1354512721300125\n",
      "Batch loss: 0.11749396473169327\n",
      "Batch loss: 0.15774166584014893\n",
      "Batch loss: 0.08404505252838135\n",
      "Batch loss: 0.08146881312131882\n",
      "Batch loss: 0.04327218979597092\n",
      "Batch loss: 0.0496465340256691\n",
      "Batch loss: 0.09871236234903336\n",
      "Batch loss: 0.047100018709897995\n",
      "Batch loss: 0.09462957084178925\n",
      "Batch loss: 0.08717208355665207\n",
      "Batch loss: 0.14068731665611267\n",
      "Batch loss: 0.05807361379265785\n",
      "Batch loss: 0.12702925503253937\n",
      "Batch loss: 0.10392040759325027\n",
      "Batch loss: 0.13660043478012085\n",
      "Batch loss: 0.1550489366054535\n",
      "Batch loss: 0.11849318444728851\n",
      "Batch loss: 0.12975923717021942\n",
      "Batch loss: 0.07774645835161209\n",
      "Batch loss: 0.06658854335546494\n",
      "Batch loss: 0.06424455344676971\n",
      "Batch loss: 0.08758779615163803\n",
      "Batch loss: 0.08339187502861023\n",
      "Batch loss: 0.057886965572834015\n",
      "Batch loss: 0.1498940885066986\n",
      "Batch loss: 0.07909796386957169\n",
      "Batch loss: 0.09940192103385925\n",
      "Batch loss: 0.21390728652477264\n",
      "Batch loss: 0.13947294652462006\n",
      "Batch loss: 0.07657720148563385\n",
      "Batch loss: 0.06934381276369095\n",
      "Batch loss: 0.05934453383088112\n",
      "Batch loss: 0.09050826728343964\n",
      "Batch loss: 0.07540256530046463\n",
      "Batch loss: 0.1848602592945099\n",
      "Batch loss: 0.06328640133142471\n",
      "Batch loss: 0.1400597095489502\n",
      "Batch loss: 0.10345464944839478\n",
      "Batch loss: 0.13011017441749573\n",
      "Batch loss: 0.1782751828432083\n",
      "Batch loss: 0.1580067276954651\n",
      "Batch loss: 0.11112021654844284\n",
      "Batch loss: 0.22626462578773499\n",
      "Batch loss: 0.10976018011569977\n",
      "Batch loss: 0.1291642040014267\n",
      "Batch loss: 0.10251005738973618\n",
      "Batch loss: 0.14850296080112457\n",
      "Batch loss: 0.10513480752706528\n",
      "Batch loss: 0.10150301456451416\n",
      "Batch loss: 0.15781626105308533\n",
      "Batch loss: 0.10164690017700195\n",
      "Batch loss: 0.13775770366191864\n",
      "Batch loss: 0.06378161162137985\n",
      "Batch loss: 0.11668769270181656\n",
      "Batch loss: 0.10326342284679413\n",
      "Batch loss: 0.11538798362016678\n",
      "Batch loss: 0.06685402989387512\n",
      "Batch loss: 0.16356387734413147\n",
      "Batch loss: 0.12270786613225937\n",
      "Batch loss: 0.06857031583786011\n",
      "Batch loss: 0.16417990624904633\n",
      "Batch loss: 0.20196637511253357\n",
      "Batch loss: 0.07455331832170486\n",
      "Batch loss: 0.14663082361221313\n",
      "Batch loss: 0.12047481536865234\n",
      "Batch loss: 0.08286012709140778\n",
      "Batch loss: 0.1585427224636078\n",
      "Batch loss: 0.09790374338626862\n",
      "Batch loss: 0.11611325293779373\n",
      "Batch loss: 0.18439719080924988\n",
      "Batch loss: 0.3969944715499878\n",
      "Batch loss: 0.15412934124469757\n",
      "Batch loss: 0.12316945940256119\n",
      "Batch loss: 0.1091182753443718\n",
      "Batch loss: 0.07931916415691376\n",
      "Batch loss: 0.06724678725004196\n",
      "Batch loss: 0.11578772962093353\n",
      "Batch loss: 0.0917186439037323\n",
      "Batch loss: 0.0832551121711731\n",
      "Batch loss: 0.07928469777107239\n",
      "Batch loss: 0.10461098700761795\n",
      "Batch loss: 0.12704552710056305\n",
      "Batch loss: 0.17239542305469513\n",
      "Batch loss: 0.126798614859581\n",
      "Batch loss: 0.12238512188196182\n",
      "Batch loss: 0.15323616564273834\n",
      "Batch loss: 0.08430451899766922\n",
      "Batch loss: 0.1309453845024109\n",
      "Batch loss: 0.05993693694472313\n",
      "Batch loss: 0.12281318008899689\n",
      "Batch loss: 0.10429567843675613\n",
      "Batch loss: 0.10965216159820557\n",
      "Batch loss: 0.06433611363172531\n",
      "Batch loss: 0.06607358157634735\n",
      "Batch loss: 0.04964028298854828\n",
      "Batch loss: 0.06523764133453369\n",
      "Batch loss: 0.119723841547966\n",
      "Batch loss: 0.07958181947469711\n",
      "Batch loss: 0.08229507505893707\n",
      "Batch loss: 0.08892285078763962\n",
      "Batch loss: 0.14153128862380981\n",
      "Batch loss: 0.06908567249774933\n",
      "Batch loss: 0.13885433971881866\n",
      "Batch loss: 0.11889254301786423\n",
      "Batch loss: 0.11805514246225357\n",
      "Batch loss: 0.09490502625703812\n",
      "Batch loss: 0.06795621663331985\n",
      "Batch loss: 0.08928798139095306\n",
      "Batch loss: 0.08760872483253479\n",
      "Batch loss: 0.08559118211269379\n",
      "Batch loss: 0.16924385726451874\n",
      "Batch loss: 0.14673683047294617\n",
      "Batch loss: 0.1214439645409584\n",
      "Batch loss: 0.15466661751270294\n",
      "Batch loss: 0.21816600859165192\n",
      "Batch loss: 0.10278359800577164\n",
      "Batch loss: 0.1298464685678482\n",
      "Batch loss: 0.09112393856048584\n",
      "Batch loss: 0.16244278848171234\n",
      "Batch loss: 0.06445632129907608\n",
      "Batch loss: 0.11973442882299423\n",
      "Batch loss: 0.16643469035625458\n",
      "Batch loss: 0.11406178027391434\n",
      "Batch loss: 0.030796542763710022\n",
      "Batch loss: 0.06664662063121796\n",
      "Batch loss: 0.15326263010501862\n",
      "Batch loss: 0.0874783918261528\n",
      "Batch loss: 0.11787264794111252\n",
      "Batch loss: 0.09368010610342026\n",
      "Batch loss: 0.07902830094099045\n",
      "Batch loss: 0.15996675193309784\n",
      "Batch loss: 0.09905917197465897\n",
      "Batch loss: 0.09688189625740051\n",
      "Batch loss: 0.14310863614082336\n",
      "Batch loss: 0.12915895879268646\n",
      "Batch loss: 0.08845110237598419\n",
      "Batch loss: 0.09951277822256088\n",
      "Batch loss: 0.10323262214660645\n",
      "Batch loss: 0.13321352005004883\n",
      "Batch loss: 0.0414246991276741\n",
      "Batch loss: 0.08512560278177261\n",
      "Batch loss: 0.12430613487958908\n",
      "Batch loss: 0.07254364341497421\n",
      "Batch loss: 0.07073229551315308\n",
      "Batch loss: 0.12365125864744186\n",
      "Batch loss: 0.15263517200946808\n",
      "Batch loss: 0.08132127672433853\n",
      "Batch loss: 0.09583185613155365\n",
      "Batch loss: 0.11203628033399582\n",
      "Batch loss: 0.056449826806783676\n",
      "Batch loss: 0.12845516204833984\n",
      "Batch loss: 0.16829608380794525\n",
      "Batch loss: 0.0861002653837204\n",
      "Batch loss: 0.14213117957115173\n",
      "Batch loss: 0.10489387810230255\n",
      "Batch loss: 0.049214377999305725\n",
      "Batch loss: 0.15583552420139313\n",
      "Batch loss: 0.05982278659939766\n",
      "Batch loss: 0.08240984380245209\n",
      "Batch loss: 0.13460835814476013\n",
      "Batch loss: 0.07914277911186218\n",
      "Batch loss: 0.07356750965118408\n",
      "Batch loss: 0.13313737511634827\n",
      "Batch loss: 0.16468718647956848\n",
      "Batch loss: 0.08290071040391922\n",
      "Batch loss: 0.16066975891590118\n",
      "Batch loss: 0.13893581926822662\n",
      "Batch loss: 0.2890284061431885\n",
      "Batch loss: 0.07469143718481064\n",
      "Batch loss: 0.1036902517080307\n",
      "Batch loss: 0.1280362755060196\n",
      "Batch loss: 0.05231987312436104\n",
      "Batch loss: 0.1276593804359436\n",
      "Batch loss: 0.16057944297790527\n",
      "Batch loss: 0.11789365112781525\n",
      "Batch loss: 0.0783420279622078\n",
      "Batch loss: 0.12343199551105499\n",
      "Batch loss: 0.07633888721466064\n",
      "Batch loss: 0.10653363168239594\n",
      "Batch loss: 0.11057913303375244\n",
      "Batch loss: 0.0769893154501915\n",
      "Batch loss: 0.16025131940841675\n",
      "Batch loss: 0.12230286002159119\n",
      "Batch loss: 0.039552222937345505\n",
      "Batch loss: 0.08208371698856354\n",
      "Batch loss: 0.09720461070537567\n",
      "Batch loss: 0.040466949343681335\n",
      "Batch loss: 0.09185872972011566\n",
      "Batch loss: 0.056679800152778625\n",
      "Batch loss: 0.21621952950954437\n",
      "Batch loss: 0.08418494462966919\n",
      "Batch loss: 0.05068131163716316\n",
      "Batch loss: 0.19159556925296783\n",
      "Batch loss: 0.14853470027446747\n",
      "Batch loss: 0.13948829472064972\n",
      "Batch loss: 0.0760718584060669\n",
      "Batch loss: 0.11671289056539536\n",
      "Batch loss: 0.11672236770391464\n",
      "Batch loss: 0.1393694132566452\n",
      "Batch loss: 0.043793339282274246\n",
      "Batch loss: 0.12265945971012115\n",
      "Batch loss: 0.06657716631889343\n",
      "Batch loss: 0.09275232255458832\n",
      "Batch loss: 0.053903866559267044\n",
      "Epoch [5/100], Loss: 0.1148\n",
      "Validation Loss: 0.7958, Accuracy: 86.26%\n",
      "Batch loss: 0.10895531624555588\n",
      "Batch loss: 0.1068306639790535\n",
      "Batch loss: 0.10733629763126373\n",
      "Batch loss: 0.11025487631559372\n",
      "Batch loss: 0.10172303020954132\n",
      "Batch loss: 0.07299122959375381\n",
      "Batch loss: 0.2156599462032318\n",
      "Batch loss: 0.08457035571336746\n",
      "Batch loss: 0.11108429729938507\n",
      "Batch loss: 0.04141370579600334\n",
      "Batch loss: 0.11698723584413528\n",
      "Batch loss: 0.08193890005350113\n",
      "Batch loss: 0.08443296700716019\n",
      "Batch loss: 0.12308181077241898\n",
      "Batch loss: 0.10708780586719513\n",
      "Batch loss: 0.099331796169281\n",
      "Batch loss: 0.08727200329303741\n",
      "Batch loss: 0.10592750459909439\n",
      "Batch loss: 0.04609762504696846\n",
      "Batch loss: 0.139351949095726\n",
      "Batch loss: 0.0890020951628685\n",
      "Batch loss: 0.09250582009553909\n",
      "Batch loss: 0.08159966766834259\n",
      "Batch loss: 0.06654199212789536\n",
      "Batch loss: 0.06380811333656311\n",
      "Batch loss: 0.05992124602198601\n",
      "Batch loss: 0.10983278602361679\n",
      "Batch loss: 0.053203143179416656\n",
      "Batch loss: 0.09855684638023376\n",
      "Batch loss: 0.0842156857252121\n",
      "Batch loss: 0.1770934909582138\n",
      "Batch loss: 0.11928826570510864\n",
      "Batch loss: 0.09263131767511368\n",
      "Batch loss: 0.12053912878036499\n",
      "Batch loss: 0.11416733264923096\n",
      "Batch loss: 0.0897514671087265\n",
      "Batch loss: 0.14141608774662018\n",
      "Batch loss: 0.1020476371049881\n",
      "Batch loss: 0.1465887576341629\n",
      "Batch loss: 0.06737466901540756\n",
      "Batch loss: 0.05823114886879921\n",
      "Batch loss: 0.09718652069568634\n",
      "Batch loss: 0.1598428189754486\n",
      "Batch loss: 0.10398997366428375\n",
      "Batch loss: 0.10631891340017319\n",
      "Batch loss: 0.10279997438192368\n",
      "Batch loss: 0.12012829631567001\n",
      "Batch loss: 0.06351188570261002\n",
      "Batch loss: 0.08101056516170502\n",
      "Batch loss: 0.13453492522239685\n",
      "Batch loss: 0.0999661535024643\n",
      "Batch loss: 0.09747684746980667\n",
      "Batch loss: 0.07855056971311569\n",
      "Batch loss: 0.0610482282936573\n",
      "Batch loss: 0.12765763700008392\n",
      "Batch loss: 0.15292300283908844\n",
      "Batch loss: 0.09515345096588135\n",
      "Batch loss: 0.07729437947273254\n",
      "Batch loss: 0.07909828424453735\n",
      "Batch loss: 0.07024994492530823\n",
      "Batch loss: 0.10588168352842331\n",
      "Batch loss: 0.09754239022731781\n",
      "Batch loss: 0.15045978128910065\n",
      "Batch loss: 0.1608731597661972\n",
      "Batch loss: 0.08239360898733139\n",
      "Batch loss: 0.15905196964740753\n",
      "Batch loss: 0.16047564148902893\n",
      "Batch loss: 0.14122162759304047\n",
      "Batch loss: 0.07772808521986008\n",
      "Batch loss: 0.08071928471326828\n",
      "Batch loss: 0.12346170842647552\n",
      "Batch loss: 0.07677038013935089\n",
      "Batch loss: 0.040663428604602814\n",
      "Batch loss: 0.05769956484436989\n",
      "Batch loss: 0.053653981536626816\n",
      "Batch loss: 0.052031729370355606\n",
      "Batch loss: 0.07381106913089752\n",
      "Batch loss: 0.08078194409608841\n",
      "Batch loss: 0.10395938158035278\n",
      "Batch loss: 0.0690639317035675\n",
      "Batch loss: 0.03395700082182884\n",
      "Batch loss: 0.057995446026325226\n",
      "Batch loss: 0.1261543333530426\n",
      "Batch loss: 0.22794213891029358\n",
      "Batch loss: 0.08726215362548828\n",
      "Batch loss: 0.0894838273525238\n",
      "Batch loss: 0.03170694038271904\n",
      "Batch loss: 0.10737792402505875\n",
      "Batch loss: 0.08348197489976883\n",
      "Batch loss: 0.07340783625841141\n",
      "Batch loss: 0.08912525326013565\n",
      "Batch loss: 0.054487504065036774\n",
      "Batch loss: 0.15539653599262238\n",
      "Batch loss: 0.09218375384807587\n",
      "Batch loss: 0.07281060516834259\n",
      "Batch loss: 0.05118916556239128\n",
      "Batch loss: 0.04796870797872543\n",
      "Batch loss: 0.09525333344936371\n",
      "Batch loss: 0.0875210165977478\n",
      "Batch loss: 0.04092772305011749\n",
      "Batch loss: 0.07463914155960083\n",
      "Batch loss: 0.036377113312482834\n",
      "Batch loss: 0.0743071585893631\n",
      "Batch loss: 0.056124184280633926\n",
      "Batch loss: 0.0649467185139656\n",
      "Batch loss: 0.09797047823667526\n",
      "Batch loss: 0.07561007887125015\n",
      "Batch loss: 0.07129786163568497\n",
      "Batch loss: 0.08590470254421234\n",
      "Batch loss: 0.16455329954624176\n",
      "Batch loss: 0.05530107021331787\n",
      "Batch loss: 0.03588118031620979\n",
      "Batch loss: 0.044042572379112244\n",
      "Batch loss: 0.07093183696269989\n",
      "Batch loss: 0.08344421535730362\n",
      "Batch loss: 0.1115063950419426\n",
      "Batch loss: 0.05985245853662491\n",
      "Batch loss: 0.04639387130737305\n",
      "Batch loss: 0.07532516866922379\n",
      "Batch loss: 0.11095205694437027\n",
      "Batch loss: 0.0874524712562561\n",
      "Batch loss: 0.04395741969347\n",
      "Batch loss: 0.1518855094909668\n",
      "Batch loss: 0.06358487904071808\n",
      "Batch loss: 0.07614002376794815\n",
      "Batch loss: 0.1407848298549652\n",
      "Batch loss: 0.05058714747428894\n",
      "Batch loss: 0.06661690771579742\n",
      "Batch loss: 0.03352182358503342\n",
      "Batch loss: 0.12452894449234009\n",
      "Batch loss: 0.12589585781097412\n",
      "Batch loss: 0.09244701266288757\n",
      "Batch loss: 0.12002043426036835\n",
      "Batch loss: 0.08705535531044006\n",
      "Batch loss: 0.10048165172338486\n",
      "Batch loss: 0.051384761929512024\n",
      "Batch loss: 0.08209593594074249\n",
      "Batch loss: 0.08060644567012787\n",
      "Batch loss: 0.06216086819767952\n",
      "Batch loss: 0.04923802986741066\n",
      "Batch loss: 0.08486824482679367\n",
      "Batch loss: 0.08214078843593597\n",
      "Batch loss: 0.10102714598178864\n",
      "Batch loss: 0.10957624018192291\n",
      "Batch loss: 0.1202249825000763\n",
      "Batch loss: 0.05579594150185585\n",
      "Batch loss: 0.1050896942615509\n",
      "Batch loss: 0.06910839676856995\n",
      "Batch loss: 0.06692743301391602\n",
      "Batch loss: 0.04499203711748123\n",
      "Batch loss: 0.08776524662971497\n",
      "Batch loss: 0.09417548030614853\n",
      "Batch loss: 0.10579223930835724\n",
      "Batch loss: 0.10351298749446869\n",
      "Batch loss: 0.17777518928050995\n",
      "Batch loss: 0.1666305512189865\n",
      "Batch loss: 0.1749923974275589\n",
      "Batch loss: 0.07232967764139175\n",
      "Batch loss: 0.08250868320465088\n",
      "Batch loss: 0.0981292873620987\n",
      "Batch loss: 0.05830657109618187\n",
      "Batch loss: 0.18165284395217896\n",
      "Batch loss: 0.06705322116613388\n",
      "Batch loss: 0.10530047118663788\n",
      "Batch loss: 0.0991397574543953\n",
      "Batch loss: 0.13593748211860657\n",
      "Batch loss: 0.1222592145204544\n",
      "Batch loss: 0.10185054689645767\n",
      "Batch loss: 0.08622652292251587\n",
      "Batch loss: 0.077644482254982\n",
      "Batch loss: 0.149033322930336\n",
      "Batch loss: 0.14259769022464752\n",
      "Batch loss: 0.08681178092956543\n",
      "Batch loss: 0.10250718146562576\n",
      "Batch loss: 0.0822453573346138\n",
      "Batch loss: 0.10928412526845932\n",
      "Batch loss: 0.17430129647254944\n",
      "Batch loss: 0.08100318908691406\n",
      "Batch loss: 0.12052102386951447\n",
      "Batch loss: 0.09115756303071976\n",
      "Batch loss: 0.0964394137263298\n",
      "Batch loss: 0.042520828545093536\n",
      "Batch loss: 0.03057137131690979\n",
      "Batch loss: 0.17194409668445587\n",
      "Batch loss: 0.17197255790233612\n",
      "Batch loss: 0.08900681138038635\n",
      "Batch loss: 0.11534471809864044\n",
      "Batch loss: 0.11043515801429749\n",
      "Batch loss: 0.0912405252456665\n",
      "Batch loss: 0.09969893097877502\n",
      "Batch loss: 0.11184299737215042\n",
      "Batch loss: 0.08640982955694199\n",
      "Batch loss: 0.09264084696769714\n",
      "Batch loss: 0.05828741192817688\n",
      "Batch loss: 0.05840054899454117\n",
      "Batch loss: 0.08647950738668442\n",
      "Batch loss: 0.04041028395295143\n",
      "Batch loss: 0.07658078521490097\n",
      "Batch loss: 0.10182970017194748\n",
      "Batch loss: 0.09934848546981812\n",
      "Batch loss: 0.17735660076141357\n",
      "Batch loss: 0.1562933325767517\n",
      "Batch loss: 0.07192279398441315\n",
      "Batch loss: 0.06510430574417114\n",
      "Batch loss: 0.10402119904756546\n",
      "Batch loss: 0.14607472717761993\n",
      "Batch loss: 0.14973610639572144\n",
      "Batch loss: 0.08490483462810516\n",
      "Batch loss: 0.07105464488267899\n",
      "Batch loss: 0.09207741916179657\n",
      "Batch loss: 0.1172768697142601\n",
      "Batch loss: 0.1170799732208252\n",
      "Batch loss: 0.106481172144413\n",
      "Batch loss: 0.11009802669286728\n",
      "Batch loss: 0.1205151379108429\n",
      "Batch loss: 0.04905761033296585\n",
      "Batch loss: 0.05020320788025856\n",
      "Batch loss: 0.05181330442428589\n",
      "Batch loss: 0.03253152593970299\n",
      "Batch loss: 0.04990999028086662\n",
      "Batch loss: 0.029159003868699074\n",
      "Batch loss: 0.06877488642930984\n",
      "Batch loss: 0.06302984803915024\n",
      "Batch loss: 0.07692129164934158\n",
      "Batch loss: 0.13746659457683563\n",
      "Batch loss: 0.11329233646392822\n",
      "Batch loss: 0.035280872136354446\n",
      "Batch loss: 0.13100814819335938\n",
      "Batch loss: 0.09230783581733704\n",
      "Batch loss: 0.060950133949518204\n",
      "Batch loss: 0.13071154057979584\n",
      "Batch loss: 0.05053342878818512\n",
      "Batch loss: 0.05075536668300629\n",
      "Batch loss: 0.018401198089122772\n",
      "Batch loss: 0.08096408098936081\n",
      "Batch loss: 0.053966522216796875\n",
      "Batch loss: 0.05262047052383423\n",
      "Batch loss: 0.17502595484256744\n",
      "Batch loss: 0.051842134445905685\n",
      "Batch loss: 0.08379325270652771\n",
      "Batch loss: 0.13621780276298523\n",
      "Batch loss: 0.09513939172029495\n",
      "Batch loss: 0.07876809686422348\n",
      "Batch loss: 0.07990216463804245\n",
      "Batch loss: 0.10302920639514923\n",
      "Batch loss: 0.049051348119974136\n",
      "Batch loss: 0.07313325256109238\n",
      "Batch loss: 0.07968117296695709\n",
      "Batch loss: 0.038391634821891785\n",
      "Batch loss: 0.06394437700510025\n",
      "Batch loss: 0.1538604497909546\n",
      "Batch loss: 0.1811569631099701\n",
      "Batch loss: 0.1363913118839264\n",
      "Batch loss: 0.14273202419281006\n",
      "Batch loss: 0.13548998534679413\n",
      "Batch loss: 0.15132558345794678\n",
      "Batch loss: 0.13760018348693848\n",
      "Batch loss: 0.14115770161151886\n",
      "Batch loss: 0.05571109429001808\n",
      "Batch loss: 0.143622487783432\n",
      "Batch loss: 0.06776881963014603\n",
      "Batch loss: 0.056568656116724014\n",
      "Batch loss: 0.08337908238172531\n",
      "Batch loss: 0.08221699297428131\n",
      "Batch loss: 0.07014629989862442\n",
      "Batch loss: 0.06388109177350998\n",
      "Batch loss: 0.08960177004337311\n",
      "Batch loss: 0.10951533913612366\n",
      "Batch loss: 0.07466688752174377\n",
      "Batch loss: 0.11170569807291031\n",
      "Batch loss: 0.11317332088947296\n",
      "Batch loss: 0.10853458940982819\n",
      "Batch loss: 0.0752125158905983\n",
      "Batch loss: 0.11009038239717484\n",
      "Batch loss: 0.11170492321252823\n",
      "Batch loss: 0.06857031583786011\n",
      "Batch loss: 0.09309375286102295\n",
      "Batch loss: 0.10737202316522598\n",
      "Batch loss: 0.08314403891563416\n",
      "Batch loss: 0.21450433135032654\n",
      "Batch loss: 0.11297895759344101\n",
      "Batch loss: 0.13304509222507477\n",
      "Batch loss: 0.150795117020607\n",
      "Batch loss: 0.3121916353702545\n",
      "Batch loss: 0.12717176973819733\n",
      "Batch loss: 0.1516389101743698\n",
      "Batch loss: 0.07837145030498505\n",
      "Batch loss: 0.048601143062114716\n",
      "Batch loss: 0.08043555915355682\n",
      "Batch loss: 0.057029664516448975\n",
      "Batch loss: 0.05989988520741463\n",
      "Batch loss: 0.1000157967209816\n",
      "Batch loss: 0.0730234757065773\n",
      "Batch loss: 0.06163948029279709\n",
      "Batch loss: 0.11885812878608704\n",
      "Batch loss: 0.09109676629304886\n",
      "Batch loss: 0.06906549632549286\n",
      "Batch loss: 0.04460419714450836\n",
      "Batch loss: 0.10931583493947983\n",
      "Batch loss: 0.08792068064212799\n",
      "Batch loss: 0.127452090382576\n",
      "Batch loss: 0.049746472388505936\n",
      "Batch loss: 0.0660681501030922\n",
      "Batch loss: 0.05202428624033928\n",
      "Batch loss: 0.10753145813941956\n",
      "Batch loss: 0.0772506445646286\n",
      "Batch loss: 0.04193432256579399\n",
      "Batch loss: 0.04802994430065155\n",
      "Batch loss: 0.06234484910964966\n",
      "Batch loss: 0.09820842742919922\n",
      "Batch loss: 0.05820365995168686\n",
      "Batch loss: 0.06902763247489929\n",
      "Batch loss: 0.07402458041906357\n",
      "Batch loss: 0.07136042416095734\n",
      "Batch loss: 0.09063758701086044\n",
      "Batch loss: 0.1449357569217682\n",
      "Batch loss: 0.12174978107213974\n",
      "Batch loss: 0.05494892969727516\n",
      "Batch loss: 0.0740746259689331\n",
      "Batch loss: 0.054563045501708984\n",
      "Batch loss: 0.05780785530805588\n",
      "Batch loss: 0.06321309506893158\n",
      "Batch loss: 0.11298063397407532\n",
      "Batch loss: 0.14088940620422363\n",
      "Batch loss: 0.05801808461546898\n",
      "Batch loss: 0.0896516814827919\n",
      "Batch loss: 0.08247464150190353\n",
      "Batch loss: 0.1292148232460022\n",
      "Batch loss: 0.07354772835969925\n",
      "Batch loss: 0.10767830163240433\n",
      "Batch loss: 0.09900615364313126\n",
      "Batch loss: 0.1265881359577179\n",
      "Batch loss: 0.05120991915464401\n",
      "Batch loss: 0.10918471217155457\n",
      "Batch loss: 0.11564342677593231\n",
      "Batch loss: 0.0630001649260521\n",
      "Batch loss: 0.046869125217199326\n",
      "Batch loss: 0.0767926499247551\n",
      "Batch loss: 0.155036062002182\n",
      "Batch loss: 0.04043234512209892\n",
      "Batch loss: 0.12108387053012848\n",
      "Batch loss: 0.10511820763349533\n",
      "Batch loss: 0.0780140683054924\n",
      "Batch loss: 0.1113257184624672\n",
      "Batch loss: 0.07979413121938705\n",
      "Batch loss: 0.11559827625751495\n",
      "Batch loss: 0.1332184076309204\n",
      "Batch loss: 0.0501570999622345\n",
      "Batch loss: 0.08705930411815643\n",
      "Batch loss: 0.056842878460884094\n",
      "Batch loss: 0.08119477331638336\n",
      "Batch loss: 0.13230721652507782\n",
      "Batch loss: 0.03628896176815033\n",
      "Batch loss: 0.05278165638446808\n",
      "Batch loss: 0.10834056884050369\n",
      "Batch loss: 0.052219491451978683\n",
      "Batch loss: 0.03864345699548721\n",
      "Batch loss: 0.08227130770683289\n",
      "Batch loss: 0.12422426789999008\n",
      "Batch loss: 0.07320155203342438\n",
      "Batch loss: 0.037524327635765076\n",
      "Batch loss: 0.12828445434570312\n",
      "Batch loss: 0.07681683450937271\n",
      "Batch loss: 0.10405676811933517\n",
      "Batch loss: 0.09950074553489685\n",
      "Batch loss: 0.07092919945716858\n",
      "Batch loss: 0.10650991648435593\n",
      "Batch loss: 0.09718538820743561\n",
      "Batch loss: 0.06989849358797073\n",
      "Batch loss: 0.11386655271053314\n",
      "Batch loss: 0.05480622127652168\n",
      "Batch loss: 0.06395763903856277\n",
      "Batch loss: 0.07034749537706375\n",
      "Batch loss: 0.10777771472930908\n",
      "Batch loss: 0.09876493364572525\n",
      "Batch loss: 0.08955644071102142\n",
      "Batch loss: 0.11926963180303574\n",
      "Batch loss: 0.0670517161488533\n",
      "Batch loss: 0.14292025566101074\n",
      "Batch loss: 0.0796298161149025\n",
      "Batch loss: 0.14971065521240234\n",
      "Batch loss: 0.04909079521894455\n",
      "Batch loss: 0.06052331253886223\n",
      "Batch loss: 0.08538827300071716\n",
      "Batch loss: 0.03476957604289055\n",
      "Batch loss: 0.0794740840792656\n",
      "Batch loss: 0.11900787055492401\n",
      "Batch loss: 0.07256031036376953\n",
      "Batch loss: 0.05025915801525116\n",
      "Batch loss: 0.09933146089315414\n",
      "Batch loss: 0.08779511600732803\n",
      "Batch loss: 0.06632388383150101\n",
      "Batch loss: 0.07658258825540543\n",
      "Batch loss: 0.15454640984535217\n",
      "Batch loss: 0.1101541668176651\n",
      "Batch loss: 0.05107406899333\n",
      "Batch loss: 0.033940840512514114\n",
      "Batch loss: 0.06022174656391144\n",
      "Batch loss: 0.06945887207984924\n",
      "Batch loss: 0.053017064929008484\n",
      "Batch loss: 0.09390489012002945\n",
      "Batch loss: 0.07455166429281235\n",
      "Batch loss: 0.17375238239765167\n",
      "Batch loss: 0.06006619706749916\n",
      "Batch loss: 0.04198991879820824\n",
      "Batch loss: 0.15782125294208527\n",
      "Batch loss: 0.10702016204595566\n",
      "Batch loss: 0.12077199667692184\n",
      "Batch loss: 0.04636614769697189\n",
      "Batch loss: 0.1395684778690338\n",
      "Batch loss: 0.11690261960029602\n",
      "Batch loss: 0.04901116341352463\n",
      "Batch loss: 0.06875234097242355\n",
      "Batch loss: 0.08260607719421387\n",
      "Batch loss: 0.08842931687831879\n",
      "Batch loss: 0.06634838134050369\n",
      "Batch loss: 0.03543621301651001\n",
      "Epoch [6/100], Loss: 0.0921\n",
      "Validation Loss: 0.5571, Accuracy: 78.88%\n",
      "Batch loss: 0.12057816982269287\n",
      "Batch loss: 0.07594690471887589\n",
      "Batch loss: 0.07405659556388855\n",
      "Batch loss: 0.10759609937667847\n",
      "Batch loss: 0.11985717713832855\n",
      "Batch loss: 0.11114314198493958\n",
      "Batch loss: 0.15125195682048798\n",
      "Batch loss: 0.08800781518220901\n",
      "Batch loss: 0.06809362024068832\n",
      "Batch loss: 0.025022532790899277\n",
      "Batch loss: 0.062174003571271896\n",
      "Batch loss: 0.06423480808734894\n",
      "Batch loss: 0.054947927594184875\n",
      "Batch loss: 0.14249566197395325\n",
      "Batch loss: 0.093808114528656\n",
      "Batch loss: 0.05853517726063728\n",
      "Batch loss: 0.08332040905952454\n",
      "Batch loss: 0.08924805372953415\n",
      "Batch loss: 0.02386503852903843\n",
      "Batch loss: 0.11192381381988525\n",
      "Batch loss: 0.058716271072626114\n",
      "Batch loss: 0.03985617309808731\n",
      "Batch loss: 0.04968537762761116\n",
      "Batch loss: 0.09835315495729446\n",
      "Batch loss: 0.11275067925453186\n",
      "Batch loss: 0.06184355542063713\n",
      "Batch loss: 0.03159086033701897\n",
      "Batch loss: 0.0877208411693573\n",
      "Batch loss: 0.10069942474365234\n",
      "Batch loss: 0.06816694885492325\n",
      "Batch loss: 0.14925137162208557\n",
      "Batch loss: 0.08910156786441803\n",
      "Batch loss: 0.07683722674846649\n",
      "Batch loss: 0.08301295340061188\n",
      "Batch loss: 0.06528077274560928\n",
      "Batch loss: 0.14462442696094513\n",
      "Batch loss: 0.08482716232538223\n",
      "Batch loss: 0.059124816209077835\n",
      "Batch loss: 0.13876476883888245\n",
      "Batch loss: 0.06948664039373398\n",
      "Batch loss: 0.06120529770851135\n",
      "Batch loss: 0.1620430201292038\n",
      "Batch loss: 0.07417837530374527\n",
      "Batch loss: 0.07563932240009308\n",
      "Batch loss: 0.07609818875789642\n",
      "Batch loss: 0.07720552384853363\n",
      "Batch loss: 0.0833338275551796\n",
      "Batch loss: 0.06380969285964966\n",
      "Batch loss: 0.06252431124448776\n",
      "Batch loss: 0.1002974733710289\n",
      "Batch loss: 0.11670603603124619\n",
      "Batch loss: 0.12284698337316513\n",
      "Batch loss: 0.04881963133811951\n",
      "Batch loss: 0.07662633061408997\n",
      "Batch loss: 0.11013531684875488\n",
      "Batch loss: 0.12465935945510864\n",
      "Batch loss: 0.07108867913484573\n",
      "Batch loss: 0.06456907093524933\n",
      "Batch loss: 0.04490028694272041\n",
      "Batch loss: 0.12711961567401886\n",
      "Batch loss: 0.11686103790998459\n",
      "Batch loss: 0.05766927823424339\n",
      "Batch loss: 0.10458680987358093\n",
      "Batch loss: 0.09145267307758331\n",
      "Batch loss: 0.035180602222681046\n",
      "Batch loss: 0.1504659652709961\n",
      "Batch loss: 0.06292041391134262\n",
      "Batch loss: 0.10216505080461502\n",
      "Batch loss: 0.06689698994159698\n",
      "Batch loss: 0.10724574327468872\n",
      "Batch loss: 0.06475519388914108\n",
      "Batch loss: 0.0537106916308403\n",
      "Batch loss: 0.05858984217047691\n",
      "Batch loss: 0.0885491669178009\n",
      "Batch loss: 0.08503303676843643\n",
      "Batch loss: 0.06874704360961914\n",
      "Batch loss: 0.10478504747152328\n",
      "Batch loss: 0.09697714447975159\n",
      "Batch loss: 0.06431706994771957\n",
      "Batch loss: 0.0636310949921608\n",
      "Batch loss: 0.05125970020890236\n",
      "Batch loss: 0.0469716377556324\n",
      "Batch loss: 0.13265210390090942\n",
      "Batch loss: 0.19193319976329803\n",
      "Batch loss: 0.06717853993177414\n",
      "Batch loss: 0.10691846162080765\n",
      "Batch loss: 0.026315756142139435\n",
      "Batch loss: 0.09140442311763763\n",
      "Batch loss: 0.079010009765625\n",
      "Batch loss: 0.02720540203154087\n",
      "Batch loss: 0.03624821826815605\n",
      "Batch loss: 0.0803036242723465\n",
      "Batch loss: 0.18449999392032623\n",
      "Batch loss: 0.10104429721832275\n",
      "Batch loss: 0.07525410503149033\n",
      "Batch loss: 0.10715200006961823\n",
      "Batch loss: 0.051844410598278046\n",
      "Batch loss: 0.13863520324230194\n",
      "Batch loss: 0.1118331179022789\n",
      "Batch loss: 0.09243500232696533\n",
      "Batch loss: 0.07016303390264511\n",
      "Batch loss: 0.022950613871216774\n",
      "Batch loss: 0.09986498206853867\n",
      "Batch loss: 0.07598751038312912\n",
      "Batch loss: 0.06175868958234787\n",
      "Batch loss: 0.09757450222969055\n",
      "Batch loss: 0.0877591073513031\n",
      "Batch loss: 0.10077638924121857\n",
      "Batch loss: 0.08545581251382828\n",
      "Batch loss: 0.06672587990760803\n",
      "Batch loss: 0.10034479200839996\n",
      "Batch loss: 0.032649993896484375\n",
      "Batch loss: 0.048842813819646835\n",
      "Batch loss: 0.0882725864648819\n",
      "Batch loss: 0.0834253579378128\n",
      "Batch loss: 0.0813499242067337\n",
      "Batch loss: 0.03071277029812336\n",
      "Batch loss: 0.018514737486839294\n",
      "Batch loss: 0.050276018679142\n",
      "Batch loss: 0.09607725590467453\n",
      "Batch loss: 0.059845175594091415\n",
      "Batch loss: 0.05787084624171257\n",
      "Batch loss: 0.1264532506465912\n",
      "Batch loss: 0.09793686866760254\n",
      "Batch loss: 0.12349917739629745\n",
      "Batch loss: 0.08339764922857285\n",
      "Batch loss: 0.07105804234743118\n",
      "Batch loss: 0.10088573396205902\n",
      "Batch loss: 0.07778136432170868\n",
      "Batch loss: 0.08860975503921509\n",
      "Batch loss: 0.06278891116380692\n",
      "Batch loss: 0.0971725583076477\n",
      "Batch loss: 0.08631423115730286\n",
      "Batch loss: 0.07463360577821732\n",
      "Batch loss: 0.10779847949743271\n",
      "Batch loss: 0.03505553677678108\n",
      "Batch loss: 0.054773107171058655\n",
      "Batch loss: 0.07500073313713074\n",
      "Batch loss: 0.07170451432466507\n",
      "Batch loss: 0.03482302278280258\n",
      "Batch loss: 0.09942996501922607\n",
      "Batch loss: 0.05552845075726509\n",
      "Batch loss: 0.051036857068538666\n",
      "Batch loss: 0.08084532618522644\n",
      "Batch loss: 0.08693923056125641\n",
      "Batch loss: 0.08061032742261887\n",
      "Batch loss: 0.06969652324914932\n",
      "Batch loss: 0.08683926612138748\n",
      "Batch loss: 0.13023076951503754\n",
      "Batch loss: 0.04884403571486473\n",
      "Batch loss: 0.09677927941083908\n",
      "Batch loss: 0.05991673469543457\n",
      "Batch loss: 0.18028295040130615\n",
      "Batch loss: 0.07463446259498596\n",
      "Batch loss: 0.098782978951931\n",
      "Batch loss: 0.15807890892028809\n",
      "Batch loss: 0.10622210800647736\n",
      "Batch loss: 0.06758682429790497\n",
      "Batch loss: 0.06861721724271774\n",
      "Batch loss: 0.07929658889770508\n",
      "Batch loss: 0.08558274805545807\n",
      "Batch loss: 0.10555743426084518\n",
      "Batch loss: 0.0862429216504097\n",
      "Batch loss: 0.08104460686445236\n",
      "Batch loss: 0.07116563618183136\n",
      "Batch loss: 0.08988860249519348\n",
      "Batch loss: 0.07333182543516159\n",
      "Batch loss: 0.0655316635966301\n",
      "Batch loss: 0.05911528691649437\n",
      "Batch loss: 0.07316826283931732\n",
      "Batch loss: 0.08714847266674042\n",
      "Batch loss: 0.11599328368902206\n",
      "Batch loss: 0.030504556372761726\n",
      "Batch loss: 0.09987353533506393\n",
      "Batch loss: 0.07817672938108444\n",
      "Batch loss: 0.15967011451721191\n",
      "Batch loss: 0.131942480802536\n",
      "Batch loss: 0.03593399375677109\n",
      "Batch loss: 0.08558500558137894\n",
      "Batch loss: 0.08190907537937164\n",
      "Batch loss: 0.06539256870746613\n",
      "Batch loss: 0.05912594869732857\n",
      "Batch loss: 0.06625368446111679\n",
      "Batch loss: 0.13170616328716278\n",
      "Batch loss: 0.09720850735902786\n",
      "Batch loss: 0.06575538963079453\n",
      "Batch loss: 0.054708804935216904\n",
      "Batch loss: 0.11144271492958069\n",
      "Batch loss: 0.08187279105186462\n",
      "Batch loss: 0.09983386099338531\n",
      "Batch loss: 0.08525382727384567\n",
      "Batch loss: 0.09500850737094879\n",
      "Batch loss: 0.05744181200861931\n",
      "Batch loss: 0.09865192323923111\n",
      "Batch loss: 0.044212568551301956\n",
      "Batch loss: 0.04365728422999382\n",
      "Batch loss: 0.07097040116786957\n",
      "Batch loss: 0.09199973940849304\n",
      "Batch loss: 0.09707213938236237\n",
      "Batch loss: 0.07654304802417755\n",
      "Batch loss: 0.08982498198747635\n",
      "Batch loss: 0.09021379053592682\n",
      "Batch loss: 0.03724098950624466\n",
      "Batch loss: 0.09395990520715714\n",
      "Batch loss: 0.1218080073595047\n",
      "Batch loss: 0.14496245980262756\n",
      "Batch loss: 0.0859084501862526\n",
      "Batch loss: 0.10772072523832321\n",
      "Batch loss: 0.06897856295108795\n",
      "Batch loss: 0.15754756331443787\n",
      "Batch loss: 0.05406254157423973\n",
      "Batch loss: 0.08527989685535431\n",
      "Batch loss: 0.0851777121424675\n",
      "Batch loss: 0.06822570413351059\n",
      "Batch loss: 0.10714544355869293\n",
      "Batch loss: 0.052641063928604126\n",
      "Batch loss: 0.147816464304924\n",
      "Batch loss: 0.08213166892528534\n",
      "Batch loss: 0.023879097774624825\n",
      "Batch loss: 0.06493334472179413\n",
      "Batch loss: 0.06104998290538788\n",
      "Batch loss: 0.04404338821768761\n",
      "Batch loss: 0.04060893505811691\n",
      "Batch loss: 0.059935830533504486\n",
      "Batch loss: 0.042952246963977814\n",
      "Batch loss: 0.14332935214042664\n",
      "Batch loss: 0.05229053273797035\n",
      "Batch loss: 0.14509254693984985\n",
      "Batch loss: 0.1539856493473053\n",
      "Batch loss: 0.03821433708071709\n",
      "Batch loss: 0.13064934313297272\n",
      "Batch loss: 0.030087055638432503\n",
      "Batch loss: 0.05141783878207207\n",
      "Batch loss: 0.1149747371673584\n",
      "Batch loss: 0.07376351952552795\n",
      "Batch loss: 0.046488113701343536\n",
      "Batch loss: 0.06058407202363014\n",
      "Batch loss: 0.1082637831568718\n",
      "Batch loss: 0.07555098831653595\n",
      "Batch loss: 0.10062524676322937\n",
      "Batch loss: 0.09266369789838791\n",
      "Batch loss: 0.12667426466941833\n",
      "Batch loss: 0.05190947651863098\n",
      "Batch loss: 0.09287508577108383\n",
      "Batch loss: 0.10510758310556412\n",
      "Batch loss: 0.06510669738054276\n",
      "Batch loss: 0.06001873314380646\n",
      "Batch loss: 0.07961117476224899\n",
      "Batch loss: 0.06794418394565582\n",
      "Batch loss: 0.046022508293390274\n",
      "Batch loss: 0.14854392409324646\n",
      "Batch loss: 0.1426496058702469\n",
      "Batch loss: 0.09300807863473892\n",
      "Batch loss: 0.10134384781122208\n",
      "Batch loss: 0.05796441435813904\n",
      "Batch loss: 0.1090613305568695\n",
      "Batch loss: 0.054208576679229736\n",
      "Batch loss: 0.08923506736755371\n",
      "Batch loss: 0.07414250820875168\n",
      "Batch loss: 0.1112220361828804\n",
      "Batch loss: 0.10646183043718338\n",
      "Batch loss: 0.055460017174482346\n",
      "Batch loss: 0.05424526333808899\n",
      "Batch loss: 0.07116889953613281\n",
      "Batch loss: 0.0740034207701683\n",
      "Batch loss: 0.04286189004778862\n",
      "Batch loss: 0.06992052495479584\n",
      "Batch loss: 0.057254452258348465\n",
      "Batch loss: 0.07006264477968216\n",
      "Batch loss: 0.020868971943855286\n",
      "Batch loss: 0.07357078790664673\n",
      "Batch loss: 0.055260732769966125\n",
      "Batch loss: 0.08164577931165695\n",
      "Batch loss: 0.036617670208215714\n",
      "Batch loss: 0.1030932143330574\n",
      "Batch loss: 0.05180276930332184\n",
      "Batch loss: 0.085464708507061\n",
      "Batch loss: 0.11925359070301056\n",
      "Batch loss: 0.10924602299928665\n",
      "Batch loss: 0.10927898436784744\n",
      "Batch loss: 0.08065114915370941\n",
      "Batch loss: 0.048176318407058716\n",
      "Batch loss: 0.10561174154281616\n",
      "Batch loss: 0.2239581048488617\n",
      "Batch loss: 0.08117947727441788\n",
      "Batch loss: 0.0794735997915268\n",
      "Batch loss: 0.07075535506010056\n",
      "Batch loss: 0.03849252313375473\n",
      "Batch loss: 0.0740358829498291\n",
      "Batch loss: 0.09410195797681808\n",
      "Batch loss: 0.06374864280223846\n",
      "Batch loss: 0.072442427277565\n",
      "Batch loss: 0.08116991072893143\n",
      "Batch loss: 0.05696843937039375\n",
      "Batch loss: 0.13679158687591553\n",
      "Batch loss: 0.08870889991521835\n",
      "Batch loss: 0.04910992830991745\n",
      "Batch loss: 0.038697049021720886\n",
      "Batch loss: 0.10211457312107086\n",
      "Batch loss: 0.07700657844543457\n",
      "Batch loss: 0.08313398063182831\n",
      "Batch loss: 0.041681885719299316\n",
      "Batch loss: 0.08640269190073013\n",
      "Batch loss: 0.08765628188848495\n",
      "Batch loss: 0.11651511490345001\n",
      "Batch loss: 0.07134497910737991\n",
      "Batch loss: 0.022710859775543213\n",
      "Batch loss: 0.08044116199016571\n",
      "Batch loss: 0.10035927593708038\n",
      "Batch loss: 0.06740816682577133\n",
      "Batch loss: 0.08696581423282623\n",
      "Batch loss: 0.06873530149459839\n",
      "Batch loss: 0.12792900204658508\n",
      "Batch loss: 0.10216004401445389\n",
      "Batch loss: 0.06104934588074684\n",
      "Batch loss: 0.062208279967308044\n",
      "Batch loss: 0.07494281977415085\n",
      "Batch loss: 0.048272740095853806\n",
      "Batch loss: 0.06843464076519012\n",
      "Batch loss: 0.06908313930034637\n",
      "Batch loss: 0.04641762748360634\n",
      "Batch loss: 0.10249300301074982\n",
      "Batch loss: 0.05053120478987694\n",
      "Batch loss: 0.13385291397571564\n",
      "Batch loss: 0.054433729499578476\n",
      "Batch loss: 0.06882963329553604\n",
      "Batch loss: 0.03948100656270981\n",
      "Batch loss: 0.11928588151931763\n",
      "Batch loss: 0.10679399222135544\n",
      "Batch loss: 0.06448041647672653\n",
      "Batch loss: 0.07019936293363571\n",
      "Batch loss: 0.07861750572919846\n",
      "Batch loss: 0.06700283288955688\n",
      "Batch loss: 0.06161165609955788\n",
      "Batch loss: 0.10191699862480164\n",
      "Batch loss: 0.046023063361644745\n",
      "Batch loss: 0.013185618445277214\n",
      "Batch loss: 0.059316907078027725\n",
      "Batch loss: 0.11449868977069855\n",
      "Batch loss: 0.04459812119603157\n",
      "Batch loss: 0.11945101618766785\n",
      "Batch loss: 0.025699960067868233\n",
      "Batch loss: 0.07389131188392639\n",
      "Batch loss: 0.11695829033851624\n",
      "Batch loss: 0.07260128855705261\n",
      "Batch loss: 0.05800050497055054\n",
      "Batch loss: 0.07451723515987396\n",
      "Batch loss: 0.0655011534690857\n",
      "Batch loss: 0.07796251028776169\n",
      "Batch loss: 0.06893137097358704\n",
      "Batch loss: 0.08542723208665848\n",
      "Batch loss: 0.0723065510392189\n",
      "Batch loss: 0.04080330580472946\n",
      "Batch loss: 0.06126118823885918\n",
      "Batch loss: 0.07538565248250961\n",
      "Batch loss: 0.023648997768759727\n",
      "Batch loss: 0.030428240075707436\n",
      "Batch loss: 0.12542416155338287\n",
      "Batch loss: 0.1217568889260292\n",
      "Batch loss: 0.04960217699408531\n",
      "Batch loss: 0.04368334263563156\n",
      "Batch loss: 0.07852736115455627\n",
      "Batch loss: 0.0400024950504303\n",
      "Batch loss: 0.12861403822898865\n",
      "Batch loss: 0.11591892689466476\n",
      "Batch loss: 0.04201972484588623\n",
      "Batch loss: 0.09905049949884415\n",
      "Batch loss: 0.0766681656241417\n",
      "Batch loss: 0.03414776176214218\n",
      "Batch loss: 0.10244850814342499\n",
      "Batch loss: 0.023249497637152672\n",
      "Batch loss: 0.05312501639127731\n",
      "Batch loss: 0.09214609861373901\n",
      "Batch loss: 0.05085057392716408\n",
      "Batch loss: 0.07511986792087555\n",
      "Batch loss: 0.05568335950374603\n",
      "Batch loss: 0.05477530509233475\n",
      "Batch loss: 0.034012485295534134\n",
      "Batch loss: 0.06789793819189072\n",
      "Batch loss: 0.12532037496566772\n",
      "Batch loss: 0.14237092435359955\n",
      "Batch loss: 0.06790272891521454\n",
      "Batch loss: 0.09639221429824829\n",
      "Batch loss: 0.06785759329795837\n",
      "Batch loss: 0.02335561253130436\n",
      "Batch loss: 0.04058186337351799\n",
      "Batch loss: 0.10902395099401474\n",
      "Batch loss: 0.07261002063751221\n",
      "Batch loss: 0.045003075152635574\n",
      "Batch loss: 0.09221963584423065\n",
      "Batch loss: 0.04422569274902344\n",
      "Batch loss: 0.05049997568130493\n",
      "Batch loss: 0.08022879809141159\n",
      "Batch loss: 0.043378796428442\n",
      "Batch loss: 0.14918099343776703\n",
      "Batch loss: 0.051280368119478226\n",
      "Batch loss: 0.03872387856245041\n",
      "Batch loss: 0.042057231068611145\n",
      "Batch loss: 0.09426888078451157\n",
      "Batch loss: 0.041370291262865067\n",
      "Batch loss: 0.04434836655855179\n",
      "Batch loss: 0.06804219633340836\n",
      "Batch loss: 0.12835946679115295\n",
      "Batch loss: 0.04321086406707764\n",
      "Batch loss: 0.06927171349525452\n",
      "Batch loss: 0.14508290588855743\n",
      "Batch loss: 0.0367000550031662\n",
      "Batch loss: 0.07249553501605988\n",
      "Batch loss: 0.036063309758901596\n",
      "Batch loss: 0.0913480743765831\n",
      "Batch loss: 0.0668586790561676\n",
      "Batch loss: 0.07684383541345596\n",
      "Batch loss: 0.023144742473959923\n",
      "Batch loss: 0.09316502511501312\n",
      "Batch loss: 0.04048752784729004\n",
      "Batch loss: 0.052991997450590134\n",
      "Batch loss: 0.07404462993144989\n",
      "Epoch [7/100], Loss: 0.0797\n",
      "Validation Loss: 0.5863, Accuracy: 76.21%\n",
      "Batch loss: 0.09580788761377335\n",
      "Batch loss: 0.020758293569087982\n",
      "Batch loss: 0.048882268369197845\n",
      "Batch loss: 0.042818207293748856\n",
      "Batch loss: 0.05899374559521675\n",
      "Batch loss: 0.06332628428936005\n",
      "Batch loss: 0.1231197938323021\n",
      "Batch loss: 0.088031105697155\n",
      "Batch loss: 0.08932306617498398\n",
      "Batch loss: 0.07912477850914001\n",
      "Batch loss: 0.1085442528128624\n",
      "Batch loss: 0.10958591848611832\n",
      "Batch loss: 0.06646960973739624\n",
      "Batch loss: 0.05363754555583\n",
      "Batch loss: 0.05053357034921646\n",
      "Batch loss: 0.06531597673892975\n",
      "Batch loss: 0.08392392098903656\n",
      "Batch loss: 0.05252368003129959\n",
      "Batch loss: 0.05041002854704857\n",
      "Batch loss: 0.08557599037885666\n",
      "Batch loss: 0.033332597464323044\n",
      "Batch loss: 0.08078063279390335\n",
      "Batch loss: 0.05644289031624794\n",
      "Batch loss: 0.05959074944257736\n",
      "Batch loss: 0.02681570127606392\n",
      "Batch loss: 0.05210235342383385\n",
      "Batch loss: 0.055066563189029694\n",
      "Batch loss: 0.07716670632362366\n",
      "Batch loss: 0.0422745980322361\n",
      "Batch loss: 0.02904348634183407\n",
      "Batch loss: 0.17161200940608978\n",
      "Batch loss: 0.03923109918832779\n",
      "Batch loss: 0.09005472809076309\n",
      "Batch loss: 0.05293230339884758\n",
      "Batch loss: 0.052909333258867264\n",
      "Batch loss: 0.07419388741254807\n",
      "Batch loss: 0.049598511308431625\n",
      "Batch loss: 0.08066307753324509\n",
      "Batch loss: 0.13188397884368896\n",
      "Batch loss: 0.07155754417181015\n",
      "Batch loss: 0.07995234429836273\n",
      "Batch loss: 0.03751566633582115\n",
      "Batch loss: 0.08981750905513763\n",
      "Batch loss: 0.12659470736980438\n",
      "Batch loss: 0.07462308555841446\n",
      "Batch loss: 0.06427015364170074\n",
      "Batch loss: 0.12250413000583649\n",
      "Batch loss: 0.0918668732047081\n",
      "Batch loss: 0.08012372255325317\n",
      "Batch loss: 0.06910546123981476\n",
      "Batch loss: 0.06620503962039948\n",
      "Batch loss: 0.07409750670194626\n",
      "Batch loss: 0.054692987352609634\n",
      "Batch loss: 0.09058180451393127\n",
      "Batch loss: 0.06313508749008179\n",
      "Batch loss: 0.11944437026977539\n",
      "Batch loss: 0.07013393193483353\n",
      "Batch loss: 0.05766342580318451\n",
      "Batch loss: 0.02775207906961441\n",
      "Batch loss: 0.0527087040245533\n",
      "Batch loss: 0.07800734788179398\n",
      "Batch loss: 0.04755573347210884\n",
      "Batch loss: 0.08760017901659012\n",
      "Batch loss: 0.05531303584575653\n",
      "Batch loss: 0.06013166904449463\n",
      "Batch loss: 0.08359166979789734\n",
      "Batch loss: 0.0883355587720871\n",
      "Batch loss: 0.09982999414205551\n",
      "Batch loss: 0.07149045914411545\n",
      "Batch loss: 0.04026439040899277\n",
      "Batch loss: 0.07361464947462082\n",
      "Batch loss: 0.03980718553066254\n",
      "Batch loss: 0.031371135264635086\n",
      "Batch loss: 0.06208102032542229\n",
      "Batch loss: 0.017030956223607063\n",
      "Batch loss: 0.09821295738220215\n",
      "Batch loss: 0.09337921440601349\n",
      "Batch loss: 0.10799550265073776\n",
      "Batch loss: 0.07341811060905457\n",
      "Batch loss: 0.06904256343841553\n",
      "Batch loss: 0.016706200316548347\n",
      "Batch loss: 0.13990479707717896\n",
      "Batch loss: 0.07496950030326843\n",
      "Batch loss: 0.07912081480026245\n",
      "Batch loss: 0.07244398444890976\n",
      "Batch loss: 0.12004443258047104\n",
      "Batch loss: 0.030589932575821877\n",
      "Batch loss: 0.1630609780550003\n",
      "Batch loss: 0.13503354787826538\n",
      "Batch loss: 0.039726462215185165\n",
      "Batch loss: 0.06047558784484863\n",
      "Batch loss: 0.03067603148519993\n",
      "Batch loss: 0.12036461383104324\n",
      "Batch loss: 0.07252628356218338\n",
      "Batch loss: 0.06754541397094727\n",
      "Batch loss: 0.03677309304475784\n",
      "Batch loss: 0.03416888788342476\n",
      "Batch loss: 0.09837675094604492\n",
      "Batch loss: 0.07198531925678253\n",
      "Batch loss: 0.03132862597703934\n",
      "Batch loss: 0.044748228043317795\n",
      "Batch loss: 0.0346781387925148\n",
      "Batch loss: 0.09704109281301498\n",
      "Batch loss: 0.06097644940018654\n",
      "Batch loss: 0.04942440614104271\n",
      "Batch loss: 0.04983096197247505\n",
      "Batch loss: 0.05058345943689346\n",
      "Batch loss: 0.09481620043516159\n",
      "Batch loss: 0.04743111878633499\n",
      "Batch loss: 0.08963140845298767\n",
      "Batch loss: 0.08579356223344803\n",
      "Batch loss: 0.03988664969801903\n",
      "Batch loss: 0.06181231886148453\n",
      "Batch loss: 0.06423395872116089\n",
      "Batch loss: 0.08359427005052567\n",
      "Batch loss: 0.05183512717485428\n",
      "Batch loss: 0.06379321962594986\n",
      "Batch loss: 0.0127452053129673\n",
      "Batch loss: 0.03569560870528221\n",
      "Batch loss: 0.056095968931913376\n",
      "Batch loss: 0.05724461376667023\n",
      "Batch loss: 0.07502248138189316\n",
      "Batch loss: 0.11946871131658554\n",
      "Batch loss: 0.06471575796604156\n",
      "Batch loss: 0.06890688836574554\n",
      "Batch loss: 0.04685128480195999\n",
      "Batch loss: 0.05159171670675278\n",
      "Batch loss: 0.06722608953714371\n",
      "Batch loss: 0.031026406213641167\n",
      "Batch loss: 0.1321326196193695\n",
      "Batch loss: 0.046340178698301315\n",
      "Batch loss: 0.053450245410203934\n",
      "Batch loss: 0.05977252125740051\n",
      "Batch loss: 0.1018480733036995\n",
      "Batch loss: 0.09438543766736984\n",
      "Batch loss: 0.021862370893359184\n",
      "Batch loss: 0.06541476398706436\n",
      "Batch loss: 0.05111312121152878\n",
      "Batch loss: 0.055466536432504654\n",
      "Batch loss: 0.02380909025669098\n",
      "Batch loss: 0.07215619087219238\n",
      "Batch loss: 0.05604572594165802\n",
      "Batch loss: 0.0405869260430336\n",
      "Batch loss: 0.09032738953828812\n",
      "Batch loss: 0.018849093466997147\n",
      "Batch loss: 0.056807901710271835\n",
      "Batch loss: 0.055892642587423325\n",
      "Batch loss: 0.10581254959106445\n",
      "Batch loss: 0.046065833419561386\n",
      "Batch loss: 0.029786262661218643\n",
      "Batch loss: 0.07426604628562927\n",
      "Batch loss: 0.0730542540550232\n",
      "Batch loss: 0.07732345163822174\n",
      "Batch loss: 0.08744202554225922\n",
      "Batch loss: 0.07455926388502121\n",
      "Batch loss: 0.05159484222531319\n",
      "Batch loss: 0.09915424883365631\n",
      "Batch loss: 0.08125074952840805\n",
      "Batch loss: 0.07174956798553467\n",
      "Batch loss: 0.08588644862174988\n",
      "Batch loss: 0.01789298839867115\n",
      "Batch loss: 0.06233680620789528\n",
      "Batch loss: 0.04827702045440674\n",
      "Batch loss: 0.07299146056175232\n",
      "Batch loss: 0.06729541718959808\n",
      "Batch loss: 0.09610004723072052\n",
      "Batch loss: 0.06140966713428497\n",
      "Batch loss: 0.07943249493837357\n",
      "Batch loss: 0.10568825900554657\n",
      "Batch loss: 0.08818202465772629\n",
      "Batch loss: 0.05032173916697502\n",
      "Batch loss: 0.13036218285560608\n",
      "Batch loss: 0.06040177121758461\n",
      "Batch loss: 0.06931323558092117\n",
      "Batch loss: 0.07984120398759842\n",
      "Batch loss: 0.04614141955971718\n",
      "Batch loss: 0.06635899841785431\n",
      "Batch loss: 0.06502262502908707\n",
      "Batch loss: 0.07164750248193741\n",
      "Batch loss: 0.02553500421345234\n",
      "Batch loss: 0.0619065947830677\n",
      "Batch loss: 0.05672536790370941\n",
      "Batch loss: 0.04897429794073105\n",
      "Batch loss: 0.1335451900959015\n",
      "Batch loss: 0.0770738422870636\n",
      "Batch loss: 0.07740205526351929\n",
      "Batch loss: 0.08712942153215408\n",
      "Batch loss: 0.0454772524535656\n",
      "Batch loss: 0.043410055339336395\n",
      "Batch loss: 0.06162166967988014\n",
      "Batch loss: 0.05085498467087746\n",
      "Batch loss: 0.06796563416719437\n",
      "Batch loss: 0.09298019111156464\n",
      "Batch loss: 0.09133388102054596\n",
      "Batch loss: 0.03249455243349075\n",
      "Batch loss: 0.06170113384723663\n",
      "Batch loss: 0.032240573316812515\n",
      "Batch loss: 0.05864967778325081\n",
      "Batch loss: 0.15666717290878296\n",
      "Batch loss: 0.061400048434734344\n",
      "Batch loss: 0.0785866379737854\n",
      "Batch loss: 0.13766926527023315\n",
      "Batch loss: 0.017825867980718613\n",
      "Batch loss: 0.06907710433006287\n",
      "Batch loss: 0.08331693708896637\n",
      "Batch loss: 0.1486586034297943\n",
      "Batch loss: 0.09005861729383469\n",
      "Batch loss: 0.06801044195890427\n",
      "Batch loss: 0.0546162985265255\n",
      "Batch loss: 0.08512066304683685\n",
      "Batch loss: 0.06322693824768066\n",
      "Batch loss: 0.075494684278965\n",
      "Batch loss: 0.04298742488026619\n",
      "Batch loss: 0.11218399554491043\n",
      "Batch loss: 0.08368290215730667\n",
      "Batch loss: 0.03630482777953148\n",
      "Batch loss: 0.04235493764281273\n",
      "Batch loss: 0.07494685053825378\n",
      "Batch loss: 0.04738474264740944\n",
      "Batch loss: 0.04536419361829758\n",
      "Batch loss: 0.04230508208274841\n",
      "Batch loss: 0.084980808198452\n",
      "Batch loss: 0.03545999526977539\n",
      "Batch loss: 0.11329060047864914\n",
      "Batch loss: 0.01863708347082138\n",
      "Batch loss: 0.08758861571550369\n",
      "Batch loss: 0.0865168571472168\n",
      "Batch loss: 0.0813034251332283\n",
      "Batch loss: 0.07214219868183136\n",
      "Batch loss: 0.04381952062249184\n",
      "Batch loss: 0.040759481489658356\n",
      "Batch loss: 0.06610941886901855\n",
      "Batch loss: 0.061267685145139694\n",
      "Batch loss: 0.028001483529806137\n",
      "Batch loss: 0.05561874061822891\n",
      "Batch loss: 0.05589007958769798\n",
      "Batch loss: 0.0944962278008461\n",
      "Batch loss: 0.07828060537576675\n",
      "Batch loss: 0.025663165375590324\n",
      "Batch loss: 0.04538017883896828\n",
      "Batch loss: 0.13451755046844482\n",
      "Batch loss: 0.0795769914984703\n",
      "Batch loss: 0.05970896780490875\n",
      "Batch loss: 0.06219072639942169\n",
      "Batch loss: 0.057101666927337646\n",
      "Batch loss: 0.026340167969465256\n",
      "Batch loss: 0.05739544704556465\n",
      "Batch loss: 0.1470988243818283\n",
      "Batch loss: 0.038886718451976776\n",
      "Batch loss: 0.04990599304437637\n",
      "Batch loss: 0.11338373273611069\n",
      "Batch loss: 0.06459081918001175\n",
      "Batch loss: 0.10841924697160721\n",
      "Batch loss: 0.04992123693227768\n",
      "Batch loss: 0.06993919610977173\n",
      "Batch loss: 0.07799940556287766\n",
      "Batch loss: 0.0608585961163044\n",
      "Batch loss: 0.058611638844013214\n",
      "Batch loss: 0.051656775176525116\n",
      "Batch loss: 0.17126141488552094\n",
      "Batch loss: 0.07609158754348755\n",
      "Batch loss: 0.09686081856489182\n",
      "Batch loss: 0.0584346242249012\n",
      "Batch loss: 0.09186242520809174\n",
      "Batch loss: 0.018294161185622215\n",
      "Batch loss: 0.026731982827186584\n",
      "Batch loss: 0.03679376468062401\n",
      "Batch loss: 0.03370603173971176\n",
      "Batch loss: 0.03926882520318031\n",
      "Batch loss: 0.032759472727775574\n",
      "Batch loss: 0.04754940792918205\n",
      "Batch loss: 0.07943417131900787\n",
      "Batch loss: 0.07552678138017654\n",
      "Batch loss: 0.1201680600643158\n",
      "Batch loss: 0.12908726930618286\n",
      "Batch loss: 0.024823764339089394\n",
      "Batch loss: 0.058919791132211685\n",
      "Batch loss: 0.05115313082933426\n",
      "Batch loss: 0.04195089265704155\n",
      "Batch loss: 0.06693296134471893\n",
      "Batch loss: 0.032709673047065735\n",
      "Batch loss: 0.07552389800548553\n",
      "Batch loss: 0.07422962039709091\n",
      "Batch loss: 0.1438187211751938\n",
      "Batch loss: 0.12234844267368317\n",
      "Batch loss: 0.08584032952785492\n",
      "Batch loss: 0.05504045635461807\n",
      "Batch loss: 0.035827603191137314\n",
      "Batch loss: 0.03256477043032646\n",
      "Batch loss: 0.12066151201725006\n",
      "Batch loss: 0.08872625976800919\n",
      "Batch loss: 0.05843532457947731\n",
      "Batch loss: 0.04689106345176697\n",
      "Batch loss: 0.04019864648580551\n",
      "Batch loss: 0.07898435741662979\n",
      "Batch loss: 0.08551155030727386\n",
      "Batch loss: 0.04376719892024994\n",
      "Batch loss: 0.03322314843535423\n",
      "Batch loss: 0.10000458359718323\n",
      "Batch loss: 0.0938858687877655\n",
      "Batch loss: 0.07950294762849808\n",
      "Batch loss: 0.04096806421875954\n",
      "Batch loss: 0.08724424988031387\n",
      "Batch loss: 0.03931819647550583\n",
      "Batch loss: 0.09399034082889557\n",
      "Batch loss: 0.0703224241733551\n",
      "Batch loss: 0.011132784187793732\n",
      "Batch loss: 0.044766683131456375\n",
      "Batch loss: 0.0727924183011055\n",
      "Batch loss: 0.1356966197490692\n",
      "Batch loss: 0.09120843559503555\n",
      "Batch loss: 0.05673108994960785\n",
      "Batch loss: 0.059880781918764114\n",
      "Batch loss: 0.0816655158996582\n",
      "Batch loss: 0.06514895707368851\n",
      "Batch loss: 0.2050526738166809\n",
      "Batch loss: 0.10043290257453918\n",
      "Batch loss: 0.03636985272169113\n",
      "Batch loss: 0.10909970104694366\n",
      "Batch loss: 0.0631483718752861\n",
      "Batch loss: 0.022511623799800873\n",
      "Batch loss: 0.05913901701569557\n",
      "Batch loss: 0.049933578819036484\n",
      "Batch loss: 0.1330827921628952\n",
      "Batch loss: 0.06789648532867432\n",
      "Batch loss: 0.06574403494596481\n",
      "Batch loss: 0.14453968405723572\n",
      "Batch loss: 0.15571729838848114\n",
      "Batch loss: 0.09248913824558258\n",
      "Batch loss: 0.11662004888057709\n",
      "Batch loss: 0.06792358309030533\n",
      "Batch loss: 0.13217346370220184\n",
      "Batch loss: 0.044078536331653595\n",
      "Batch loss: 0.05064019933342934\n",
      "Batch loss: 0.09735981374979019\n",
      "Batch loss: 0.07722113281488419\n",
      "Batch loss: 0.014306062832474709\n",
      "Batch loss: 0.049966420978307724\n",
      "Batch loss: 0.10082610696554184\n",
      "Batch loss: 0.07888185977935791\n",
      "Batch loss: 0.07006028294563293\n",
      "Batch loss: 0.029139846563339233\n",
      "Batch loss: 0.04556681960821152\n",
      "Batch loss: 0.11784930527210236\n",
      "Batch loss: 0.05412878841161728\n",
      "Batch loss: 0.07446886599063873\n",
      "Batch loss: 0.051040731370449066\n",
      "Batch loss: 0.09042485058307648\n",
      "Batch loss: 0.07305406779050827\n",
      "Batch loss: 0.07808384299278259\n",
      "Batch loss: 0.06596609950065613\n",
      "Batch loss: 0.051232095807790756\n",
      "Batch loss: 0.026692762970924377\n",
      "Batch loss: 0.062430303543806076\n",
      "Batch loss: 0.054647281765937805\n",
      "Batch loss: 0.043071478605270386\n",
      "Batch loss: 0.03642649948596954\n",
      "Batch loss: 0.037717849016189575\n",
      "Batch loss: 0.06649304926395416\n",
      "Batch loss: 0.06053881719708443\n",
      "Batch loss: 0.03959954157471657\n",
      "Batch loss: 0.10126695036888123\n",
      "Batch loss: 0.11527376621961594\n",
      "Batch loss: 0.0645618662238121\n",
      "Batch loss: 0.05884923040866852\n",
      "Batch loss: 0.04204327613115311\n",
      "Batch loss: 0.058369409292936325\n",
      "Batch loss: 0.05392208695411682\n",
      "Batch loss: 0.02188945934176445\n",
      "Batch loss: 0.06038838252425194\n",
      "Batch loss: 0.01790209859609604\n",
      "Batch loss: 0.11322955042123795\n",
      "Batch loss: 0.11221343278884888\n",
      "Batch loss: 0.08015809953212738\n",
      "Batch loss: 0.043942805379629135\n",
      "Batch loss: 0.044899653643369675\n",
      "Batch loss: 0.05020798370242119\n",
      "Batch loss: 0.05087003484368324\n",
      "Batch loss: 0.08346608281135559\n",
      "Batch loss: 0.10532340407371521\n",
      "Batch loss: 0.10196172446012497\n",
      "Batch loss: 0.0450698584318161\n",
      "Batch loss: 0.08209332078695297\n",
      "Batch loss: 0.05774540826678276\n",
      "Batch loss: 0.02940712869167328\n",
      "Batch loss: 0.050542451441287994\n",
      "Batch loss: 0.10480840504169464\n",
      "Batch loss: 0.06819681078195572\n",
      "Batch loss: 0.03760715574026108\n",
      "Batch loss: 0.03146649897098541\n",
      "Batch loss: 0.055336520075798035\n",
      "Batch loss: 0.05798708274960518\n",
      "Batch loss: 0.07368502020835876\n",
      "Batch loss: 0.09199598431587219\n",
      "Batch loss: 0.09782455861568451\n",
      "Batch loss: 0.060740016400814056\n",
      "Batch loss: 0.018688036128878593\n",
      "Batch loss: 0.060662176460027695\n",
      "Batch loss: 0.05355977639555931\n",
      "Batch loss: 0.051448922604322433\n",
      "Batch loss: 0.03936456888914108\n",
      "Batch loss: 0.02305745705962181\n",
      "Batch loss: 0.11068113148212433\n",
      "Batch loss: 0.05151678994297981\n",
      "Batch loss: 0.04532492160797119\n",
      "Batch loss: 0.05792543664574623\n",
      "Batch loss: 0.06843764334917068\n",
      "Batch loss: 0.05686252936720848\n",
      "Batch loss: 0.04984312132000923\n",
      "Batch loss: 0.07117775827646255\n",
      "Batch loss: 0.11414719372987747\n",
      "Batch loss: 0.03112955018877983\n",
      "Batch loss: 0.014270402491092682\n",
      "Batch loss: 0.06872104853391647\n",
      "Batch loss: 0.07168326526880264\n",
      "Batch loss: 0.05288079380989075\n",
      "Batch loss: 0.03394768014550209\n",
      "Epoch [8/100], Loss: 0.0682\n",
      "Validation Loss: 0.3365, Accuracy: 92.65%\n",
      "Batch loss: 0.03371720761060715\n",
      "Batch loss: 0.030653102323412895\n",
      "Batch loss: 0.09234848618507385\n",
      "Batch loss: 0.11375158280134201\n",
      "Batch loss: 0.07862258702516556\n",
      "Batch loss: 0.0419357493519783\n",
      "Batch loss: 0.16314683854579926\n",
      "Batch loss: 0.08420614898204803\n",
      "Batch loss: 0.0510941818356514\n",
      "Batch loss: 0.021497292444109917\n",
      "Batch loss: 0.03837156295776367\n",
      "Batch loss: 0.08647360652685165\n",
      "Batch loss: 0.032279208302497864\n",
      "Batch loss: 0.07586054503917694\n",
      "Batch loss: 0.08046722412109375\n",
      "Batch loss: 0.06698314845561981\n",
      "Batch loss: 0.06818324327468872\n",
      "Batch loss: 0.08880142867565155\n",
      "Batch loss: 0.026410777121782303\n",
      "Batch loss: 0.09992678463459015\n",
      "Batch loss: 0.030814679339528084\n",
      "Batch loss: 0.15692846477031708\n",
      "Batch loss: 0.03986266255378723\n",
      "Batch loss: 0.07332489639520645\n",
      "Batch loss: 0.049491945654153824\n",
      "Batch loss: 0.02503180503845215\n",
      "Batch loss: 0.044580671936273575\n",
      "Batch loss: 0.05361868068575859\n",
      "Batch loss: 0.08479161560535431\n",
      "Batch loss: 0.07558511942625046\n",
      "Batch loss: 0.1688244640827179\n",
      "Batch loss: 0.05096141993999481\n",
      "Batch loss: 0.08105164766311646\n",
      "Batch loss: 0.039487920701503754\n",
      "Batch loss: 0.03575293347239494\n",
      "Batch loss: 0.0796332061290741\n",
      "Batch loss: 0.0377393402159214\n",
      "Batch loss: 0.02987167052924633\n",
      "Batch loss: 0.060794707387685776\n",
      "Batch loss: 0.033159565180540085\n",
      "Batch loss: 0.03465083986520767\n",
      "Batch loss: 0.03093899041414261\n",
      "Batch loss: 0.08446875959634781\n",
      "Batch loss: 0.08375568687915802\n",
      "Batch loss: 0.07800845056772232\n",
      "Batch loss: 0.06514715403318405\n",
      "Batch loss: 0.05644015595316887\n",
      "Batch loss: 0.06954264640808105\n",
      "Batch loss: 0.07200521230697632\n",
      "Batch loss: 0.06630413979291916\n",
      "Batch loss: 0.026350611820816994\n",
      "Batch loss: 0.03289974108338356\n",
      "Batch loss: 0.03263681381940842\n",
      "Batch loss: 0.057556942105293274\n",
      "Batch loss: 0.05587693303823471\n",
      "Batch loss: 0.06246449798345566\n",
      "Batch loss: 0.03817800059914589\n",
      "Batch loss: 0.031413037329912186\n",
      "Batch loss: 0.03387408331036568\n",
      "Batch loss: 0.10504799336194992\n",
      "Batch loss: 0.08618106693029404\n",
      "Batch loss: 0.04157240316271782\n",
      "Batch loss: 0.04859008640050888\n",
      "Batch loss: 0.10038428753614426\n",
      "Batch loss: 0.013937010429799557\n",
      "Batch loss: 0.022195948287844658\n",
      "Batch loss: 0.04101236164569855\n",
      "Batch loss: 0.10081709921360016\n",
      "Batch loss: 0.03948025405406952\n",
      "Batch loss: 0.05044982209801674\n",
      "Batch loss: 0.06252738833427429\n",
      "Batch loss: 0.01596667803823948\n",
      "Batch loss: 0.06551461666822433\n",
      "Batch loss: 0.029410777613520622\n",
      "Batch loss: 0.034713324159383774\n",
      "Batch loss: 0.016054684296250343\n",
      "Batch loss: 0.06792992353439331\n",
      "Batch loss: 0.025910302996635437\n",
      "Batch loss: 0.013797953724861145\n",
      "Batch loss: 0.15070569515228271\n",
      "Batch loss: 0.004098484758287668\n",
      "Batch loss: 0.08530134707689285\n",
      "Batch loss: 0.039347365498542786\n",
      "Batch loss: 0.10270026326179504\n",
      "Batch loss: 0.03286251798272133\n",
      "Batch loss: 0.07157909125089645\n",
      "Batch loss: 0.019475702196359634\n",
      "Batch loss: 0.0663532093167305\n",
      "Batch loss: 0.054354533553123474\n",
      "Batch loss: 0.06326538324356079\n",
      "Batch loss: 0.09946733713150024\n",
      "Batch loss: 0.04815419390797615\n",
      "Batch loss: 0.048014089465141296\n",
      "Batch loss: 0.0621388703584671\n",
      "Batch loss: 0.07780485600233078\n",
      "Batch loss: 0.10480158776044846\n",
      "Batch loss: 0.022007329389452934\n",
      "Batch loss: 0.09289099276065826\n",
      "Batch loss: 0.09590250998735428\n",
      "Batch loss: 0.019148429855704308\n",
      "Batch loss: 0.058288633823394775\n",
      "Batch loss: 0.03308398276567459\n",
      "Batch loss: 0.13525834679603577\n",
      "Batch loss: 0.051551442593336105\n",
      "Batch loss: 0.11045154184103012\n",
      "Batch loss: 0.034535929560661316\n",
      "Batch loss: 0.03797532245516777\n",
      "Batch loss: 0.0495176762342453\n",
      "Batch loss: 0.05050186440348625\n",
      "Batch loss: 0.09878796339035034\n",
      "Batch loss: 0.08556386083364487\n",
      "Batch loss: 0.037595901638269424\n",
      "Batch loss: 0.018927395343780518\n",
      "Batch loss: 0.04374950751662254\n",
      "Batch loss: 0.039243828505277634\n",
      "Batch loss: 0.03793451189994812\n",
      "Batch loss: 0.06685368716716766\n",
      "Batch loss: 0.03581281751394272\n",
      "Batch loss: 0.03926394507288933\n",
      "Batch loss: 0.05582210421562195\n",
      "Batch loss: 0.03042382001876831\n",
      "Batch loss: 0.05789702385663986\n",
      "Batch loss: 0.03476938232779503\n",
      "Batch loss: 0.03149278089404106\n",
      "Batch loss: 0.10761525481939316\n",
      "Batch loss: 0.06925904750823975\n",
      "Batch loss: 0.033239465206861496\n",
      "Batch loss: 0.043261028826236725\n",
      "Batch loss: 0.028054222464561462\n",
      "Batch loss: 0.040594104677438736\n",
      "Batch loss: 0.06584592908620834\n",
      "Batch loss: 0.0355864092707634\n",
      "Batch loss: 0.11147625744342804\n",
      "Batch loss: 0.05807671695947647\n",
      "Batch loss: 0.09568189084529877\n",
      "Batch loss: 0.02660384587943554\n",
      "Batch loss: 0.027104806154966354\n",
      "Batch loss: 0.061087511479854584\n",
      "Batch loss: 0.050755906850099564\n",
      "Batch loss: 0.011320923455059528\n",
      "Batch loss: 0.07596137374639511\n",
      "Batch loss: 0.051467590034008026\n",
      "Batch loss: 0.07412414997816086\n",
      "Batch loss: 0.07111058384180069\n",
      "Batch loss: 0.057897333055734634\n",
      "Batch loss: 0.03281174227595329\n",
      "Batch loss: 0.03983568027615547\n",
      "Batch loss: 0.06812872737646103\n",
      "Batch loss: 0.04030745103955269\n",
      "Batch loss: 0.035393550992012024\n",
      "Batch loss: 0.038115087896585464\n",
      "Batch loss: 0.01902097277343273\n",
      "Batch loss: 0.07878069579601288\n",
      "Batch loss: 0.022372277453541756\n",
      "Batch loss: 0.08663459867238998\n",
      "Batch loss: 0.05918482318520546\n",
      "Batch loss: 0.12227978557348251\n",
      "Batch loss: 0.0545315146446228\n",
      "Batch loss: 0.06220266595482826\n",
      "Batch loss: 0.030688777565956116\n",
      "Batch loss: 0.031585805118083954\n",
      "Batch loss: 0.11528530716896057\n",
      "Batch loss: 0.01775358058512211\n",
      "Batch loss: 0.08336742967367172\n",
      "Batch loss: 0.020649749785661697\n",
      "Batch loss: 0.04719260707497597\n",
      "Batch loss: 0.022358451038599014\n",
      "Batch loss: 0.012554165907204151\n",
      "Batch loss: 0.04238050431013107\n",
      "Batch loss: 0.04006866738200188\n",
      "Batch loss: 0.04446672648191452\n",
      "Batch loss: 0.09632047265768051\n",
      "Batch loss: 0.04037633538246155\n",
      "Batch loss: 0.05001477897167206\n",
      "Batch loss: 0.0404682382941246\n",
      "Batch loss: 0.09153345972299576\n",
      "Batch loss: 0.07594034820795059\n",
      "Batch loss: 0.02078099548816681\n",
      "Batch loss: 0.1619945615530014\n",
      "Batch loss: 0.05511439964175224\n",
      "Batch loss: 0.044189926236867905\n",
      "Batch loss: 0.045005250722169876\n",
      "Batch loss: 0.013099901378154755\n",
      "Batch loss: 0.10386648029088974\n",
      "Batch loss: 0.09593761712312698\n",
      "Batch loss: 0.09836849570274353\n",
      "Batch loss: 0.09007497876882553\n",
      "Batch loss: 0.09959547221660614\n",
      "Batch loss: 0.03148828446865082\n",
      "Batch loss: 0.04229682311415672\n",
      "Batch loss: 0.031090619042515755\n",
      "Batch loss: 0.045160312205553055\n",
      "Batch loss: 0.036679577082395554\n",
      "Batch loss: 0.03082817792892456\n",
      "Batch loss: 0.030596524477005005\n",
      "Batch loss: 0.024909930303692818\n",
      "Batch loss: 0.03495411574840546\n",
      "Batch loss: 0.040057212114334106\n",
      "Batch loss: 0.04415259137749672\n",
      "Batch loss: 0.06530705094337463\n",
      "Batch loss: 0.044646043330430984\n",
      "Batch loss: 0.06925078481435776\n",
      "Batch loss: 0.09715016931295395\n",
      "Batch loss: 0.06380907446146011\n",
      "Batch loss: 0.07877525687217712\n",
      "Batch loss: 0.08504997193813324\n",
      "Batch loss: 0.030158378183841705\n",
      "Batch loss: 0.053048502653837204\n",
      "Batch loss: 0.04846346750855446\n",
      "Batch loss: 0.08183623850345612\n",
      "Batch loss: 0.019593050703406334\n",
      "Batch loss: 0.04608207195997238\n",
      "Batch loss: 0.0912792757153511\n",
      "Batch loss: 0.09080444276332855\n",
      "Batch loss: 0.039502594619989395\n",
      "Batch loss: 0.06363895535469055\n",
      "Batch loss: 0.03378066420555115\n",
      "Batch loss: 0.02685442380607128\n",
      "Batch loss: 0.00748209934681654\n",
      "Batch loss: 0.015830108895897865\n",
      "Batch loss: 0.024852141737937927\n",
      "Batch loss: 0.025944337248802185\n",
      "Batch loss: 0.03539120778441429\n",
      "Batch loss: 0.03124406933784485\n",
      "Batch loss: 0.021269729360938072\n",
      "Batch loss: 0.04689573496580124\n",
      "Batch loss: 0.04684801772236824\n",
      "Batch loss: 0.08441720902919769\n",
      "Batch loss: 0.14219890534877777\n",
      "Batch loss: 0.0754873976111412\n",
      "Batch loss: 0.07867027074098587\n",
      "Batch loss: 0.0303292628377676\n",
      "Batch loss: 0.019885875284671783\n",
      "Batch loss: 0.045981813222169876\n",
      "Batch loss: 0.03702298551797867\n",
      "Batch loss: 0.03798559308052063\n",
      "Batch loss: 0.06288512051105499\n",
      "Batch loss: 0.09039255976676941\n",
      "Batch loss: 0.03123536892235279\n",
      "Batch loss: 0.023963985964655876\n",
      "Batch loss: 0.08618298918008804\n",
      "Batch loss: 0.07950294017791748\n",
      "Batch loss: 0.027005407959222794\n",
      "Batch loss: 0.05098748579621315\n",
      "Batch loss: 0.029573604464530945\n",
      "Batch loss: 0.04096626862883568\n",
      "Batch loss: 0.05850248411297798\n",
      "Batch loss: 0.03909222036600113\n",
      "Batch loss: 0.039374735206365585\n",
      "Batch loss: 0.04499775543808937\n",
      "Batch loss: 0.07777376472949982\n",
      "Batch loss: 0.10002189129590988\n",
      "Batch loss: 0.04867911711335182\n",
      "Batch loss: 0.055660709738731384\n",
      "Batch loss: 0.04896816983819008\n",
      "Batch loss: 0.06749511510133743\n",
      "Batch loss: 0.04804910346865654\n",
      "Batch loss: 0.05829501152038574\n",
      "Batch loss: 0.04064646735787392\n",
      "Batch loss: 0.03136562556028366\n",
      "Batch loss: 0.06549917161464691\n",
      "Batch loss: 0.05205843970179558\n",
      "Batch loss: 0.03871743381023407\n",
      "Batch loss: 0.038650717586278915\n",
      "Batch loss: 0.03618815913796425\n",
      "Batch loss: 0.026661159470677376\n",
      "Batch loss: 0.11630745232105255\n",
      "Batch loss: 0.10021012276411057\n",
      "Batch loss: 0.05955507233738899\n",
      "Batch loss: 0.03622722998261452\n",
      "Batch loss: 0.050659287720918655\n",
      "Batch loss: 0.035452499985694885\n",
      "Batch loss: 0.04486365243792534\n",
      "Batch loss: 0.03980391100049019\n",
      "Batch loss: 0.0627223327755928\n",
      "Batch loss: 0.027547813951969147\n",
      "Batch loss: 0.02517792209982872\n",
      "Batch loss: 0.10163405537605286\n",
      "Batch loss: 0.04459765926003456\n",
      "Batch loss: 0.10119988024234772\n",
      "Batch loss: 0.03604160249233246\n",
      "Batch loss: 0.07947410643100739\n",
      "Batch loss: 0.11192429065704346\n",
      "Batch loss: 0.13878470659255981\n",
      "Batch loss: 0.035478223115205765\n",
      "Batch loss: 0.07508599013090134\n",
      "Batch loss: 0.032599885016679764\n",
      "Batch loss: 0.039959125220775604\n",
      "Batch loss: 0.03124411031603813\n",
      "Batch loss: 0.036877524107694626\n",
      "Batch loss: 0.03820135071873665\n",
      "Batch loss: 0.05306663736701012\n",
      "Batch loss: 0.026153119280934334\n",
      "Batch loss: 0.06430134922266006\n",
      "Batch loss: 0.03978519141674042\n",
      "Batch loss: 0.06798859685659409\n",
      "Batch loss: 0.018202686682343483\n",
      "Batch loss: 0.019152246415615082\n",
      "Batch loss: 0.07055419683456421\n",
      "Batch loss: 0.050705812871456146\n",
      "Batch loss: 0.036722224205732346\n",
      "Batch loss: 0.031583189964294434\n",
      "Batch loss: 0.06538022309541702\n",
      "Batch loss: 0.07004310935735703\n",
      "Batch loss: 0.034991148859262466\n",
      "Batch loss: 0.05852852761745453\n",
      "Batch loss: 0.03080662526190281\n",
      "Batch loss: 0.011158348992466927\n",
      "Batch loss: 0.04397863522171974\n",
      "Batch loss: 0.02144515886902809\n",
      "Batch loss: 0.014836463145911694\n",
      "Batch loss: 0.029558146372437477\n",
      "Batch loss: 0.0458846241235733\n",
      "Batch loss: 0.05354756861925125\n",
      "Batch loss: 0.04422552138566971\n",
      "Batch loss: 0.06591082364320755\n",
      "Batch loss: 0.020643213763833046\n",
      "Batch loss: 0.05233382433652878\n",
      "Batch loss: 0.02688475325703621\n",
      "Batch loss: 0.057911526411771774\n",
      "Batch loss: 0.11612275242805481\n",
      "Batch loss: 0.12033937871456146\n",
      "Batch loss: 0.04613909125328064\n",
      "Batch loss: 0.0867767333984375\n",
      "Batch loss: 0.027468014508485794\n",
      "Batch loss: 0.05928938835859299\n",
      "Batch loss: 0.031015558168292046\n",
      "Batch loss: 0.05460863932967186\n",
      "Batch loss: 0.07856007665395737\n",
      "Batch loss: 0.031244952231645584\n",
      "Batch loss: 0.05705687031149864\n",
      "Batch loss: 0.05110112950205803\n",
      "Batch loss: 0.029286246746778488\n",
      "Batch loss: 0.04774787649512291\n",
      "Batch loss: 0.06636977195739746\n",
      "Batch loss: 0.051170527935028076\n",
      "Batch loss: 0.005895300302654505\n",
      "Batch loss: 0.027384087443351746\n",
      "Batch loss: 0.0855831429362297\n",
      "Batch loss: 0.0222903024405241\n",
      "Batch loss: 0.11237712949514389\n",
      "Batch loss: 0.03449774906039238\n",
      "Batch loss: 0.051259711384773254\n",
      "Batch loss: 0.06146064028143883\n",
      "Batch loss: 0.05408366024494171\n",
      "Batch loss: 0.05950223654508591\n",
      "Batch loss: 0.06097770482301712\n",
      "Batch loss: 0.06250499188899994\n",
      "Batch loss: 0.030138321220874786\n",
      "Batch loss: 0.05483788251876831\n",
      "Batch loss: 0.06743738055229187\n",
      "Batch loss: 0.095237135887146\n",
      "Batch loss: 0.05057726427912712\n",
      "Batch loss: 0.03428039327263832\n",
      "Batch loss: 0.07293449342250824\n",
      "Batch loss: 0.026369476690888405\n",
      "Batch loss: 0.06229977682232857\n",
      "Batch loss: 0.10169058293104172\n",
      "Batch loss: 0.051537178456783295\n",
      "Batch loss: 0.04090917855501175\n",
      "Batch loss: 0.023431546986103058\n",
      "Batch loss: 0.0922393649816513\n",
      "Batch loss: 0.018557021394371986\n",
      "Batch loss: 0.03422442078590393\n",
      "Batch loss: 0.04856674000620842\n",
      "Batch loss: 0.0517706573009491\n",
      "Batch loss: 0.045812372118234634\n",
      "Batch loss: 0.045074451714754105\n",
      "Batch loss: 0.050114165991544724\n",
      "Batch loss: 0.021776258945465088\n",
      "Batch loss: 0.049210816621780396\n",
      "Batch loss: 0.04129290580749512\n",
      "Batch loss: 0.04644154757261276\n",
      "Batch loss: 0.06329948455095291\n",
      "Batch loss: 0.06995531171560287\n",
      "Batch loss: 0.02826264686882496\n",
      "Batch loss: 0.06571988761425018\n",
      "Batch loss: 0.049333132803440094\n",
      "Batch loss: 0.031148094683885574\n",
      "Batch loss: 0.08530008047819138\n",
      "Batch loss: 0.1285289227962494\n",
      "Batch loss: 0.09216544777154922\n",
      "Batch loss: 0.08132962137460709\n",
      "Batch loss: 0.03928389027714729\n",
      "Batch loss: 0.041172709316015244\n",
      "Batch loss: 0.027212001383304596\n",
      "Batch loss: 0.11869525164365768\n",
      "Batch loss: 0.02533998154103756\n",
      "Batch loss: 0.05299696698784828\n",
      "Batch loss: 0.09236685931682587\n",
      "Batch loss: 0.027397992089390755\n",
      "Batch loss: 0.052336856722831726\n",
      "Batch loss: 0.0654972642660141\n",
      "Batch loss: 0.038222916424274445\n",
      "Batch loss: 0.1649143397808075\n",
      "Batch loss: 0.028209492564201355\n",
      "Batch loss: 0.00943844299763441\n",
      "Batch loss: 0.04901016876101494\n",
      "Batch loss: 0.043505582958459854\n",
      "Batch loss: 0.03340226039290428\n",
      "Batch loss: 0.03962878882884979\n",
      "Batch loss: 0.05258242040872574\n",
      "Batch loss: 0.07447899878025055\n",
      "Batch loss: 0.028910363093018532\n",
      "Batch loss: 0.04907871037721634\n",
      "Batch loss: 0.09202222526073456\n",
      "Batch loss: 0.04964543506503105\n",
      "Batch loss: 0.03557775542140007\n",
      "Batch loss: 0.04624171182513237\n",
      "Batch loss: 0.04508039727807045\n",
      "Batch loss: 0.037902772426605225\n",
      "Batch loss: 0.023112738505005836\n",
      "Batch loss: 0.07098021358251572\n",
      "Batch loss: 0.0380486436188221\n",
      "Batch loss: 0.04383848235011101\n",
      "Batch loss: 0.06637665629386902\n",
      "Batch loss: 0.03227134421467781\n",
      "Epoch [9/100], Loss: 0.0547\n",
      "Validation Loss: 0.5792, Accuracy: 84.04%\n",
      "Batch loss: 0.0264605600386858\n",
      "Batch loss: 0.015751613304018974\n",
      "Batch loss: 0.044636886566877365\n",
      "Batch loss: 0.03901994600892067\n",
      "Batch loss: 0.02740682102739811\n",
      "Batch loss: 0.10168738663196564\n",
      "Batch loss: 0.08914489299058914\n",
      "Batch loss: 0.020178936421871185\n",
      "Batch loss: 0.08753924816846848\n",
      "Batch loss: 0.014007399789988995\n",
      "Batch loss: 0.025198310613632202\n",
      "Batch loss: 0.020363347604870796\n",
      "Batch loss: 0.03296848386526108\n",
      "Batch loss: 0.072163887321949\n",
      "Batch loss: 0.021947044879198074\n",
      "Batch loss: 0.02341349422931671\n",
      "Batch loss: 0.03573686629533768\n",
      "Batch loss: 0.028252778574824333\n",
      "Batch loss: 0.033939067274332047\n",
      "Batch loss: 0.039665475487709045\n",
      "Batch loss: 0.056792002171278\n",
      "Batch loss: 0.02958652563393116\n",
      "Batch loss: 0.029045555740594864\n",
      "Batch loss: 0.021198436617851257\n",
      "Batch loss: 0.020395031198859215\n",
      "Batch loss: 0.0063018761575222015\n",
      "Batch loss: 0.04743241146206856\n",
      "Batch loss: 0.020473988726735115\n",
      "Batch loss: 0.049816396087408066\n",
      "Batch loss: 0.03379328176379204\n",
      "Batch loss: 0.14231446385383606\n",
      "Batch loss: 0.0726630911231041\n",
      "Batch loss: 0.025232769548892975\n",
      "Batch loss: 0.039814431220293045\n",
      "Batch loss: 0.015459049493074417\n",
      "Batch loss: 0.08947137743234634\n",
      "Batch loss: 0.03177082538604736\n",
      "Batch loss: 0.0413326621055603\n",
      "Batch loss: 0.04783812165260315\n",
      "Batch loss: 0.01969941332936287\n",
      "Batch loss: 0.02947617508471012\n",
      "Batch loss: 0.026619568467140198\n",
      "Batch loss: 0.035721924155950546\n",
      "Batch loss: 0.04156308248639107\n",
      "Batch loss: 0.07476493716239929\n",
      "Batch loss: 0.1134364977478981\n",
      "Batch loss: 0.022588349878787994\n",
      "Batch loss: 0.03220053389668465\n",
      "Batch loss: 0.038429081439971924\n",
      "Batch loss: 0.03570414334535599\n",
      "Batch loss: 0.06507895141839981\n",
      "Batch loss: 0.04937728866934776\n",
      "Batch loss: 0.023288248106837273\n",
      "Batch loss: 0.023024825379252434\n",
      "Batch loss: 0.04292578622698784\n",
      "Batch loss: 0.07192061841487885\n",
      "Batch loss: 0.06517311930656433\n",
      "Batch loss: 0.059498146176338196\n",
      "Batch loss: 0.02890736609697342\n",
      "Batch loss: 0.013447694480419159\n",
      "Batch loss: 0.0611187145113945\n",
      "Batch loss: 0.06708704680204391\n",
      "Batch loss: 0.03701247274875641\n",
      "Batch loss: 0.09527305513620377\n",
      "Batch loss: 0.01579759456217289\n",
      "Batch loss: 0.050420742481946945\n",
      "Batch loss: 0.05121944472193718\n",
      "Batch loss: 0.04554910585284233\n",
      "Batch loss: 0.0735006108880043\n",
      "Batch loss: 0.05408217012882233\n",
      "Batch loss: 0.046766649931669235\n",
      "Batch loss: 0.04406372457742691\n",
      "Batch loss: 0.05735788494348526\n",
      "Batch loss: 0.03825323283672333\n",
      "Batch loss: 0.03379400447010994\n",
      "Batch loss: 0.026531632989645004\n",
      "Batch loss: 0.04653564468026161\n",
      "Batch loss: 0.027444513514637947\n",
      "Batch loss: 0.03417516127228737\n",
      "Batch loss: 0.03229869157075882\n",
      "Batch loss: 0.04225749149918556\n",
      "Batch loss: 0.033539317548274994\n",
      "Batch loss: 0.033638887107372284\n",
      "Batch loss: 0.06244121119379997\n",
      "Batch loss: 0.02377317100763321\n",
      "Batch loss: 0.07897768914699554\n",
      "Batch loss: 0.01192083302885294\n",
      "Batch loss: 0.03190334886312485\n",
      "Batch loss: 0.03788795694708824\n",
      "Batch loss: 0.014601480215787888\n",
      "Batch loss: 0.013956635259091854\n",
      "Batch loss: 0.01767587475478649\n",
      "Batch loss: 0.07809321582317352\n",
      "Batch loss: 0.02335335500538349\n",
      "Batch loss: 0.07946695387363434\n",
      "Batch loss: 0.020083842799067497\n",
      "Batch loss: 0.010165819898247719\n",
      "Batch loss: 0.06434859335422516\n",
      "Batch loss: 0.040087390691041946\n",
      "Batch loss: 0.06071200966835022\n",
      "Batch loss: 0.04463249444961548\n",
      "Batch loss: 0.017667187377810478\n",
      "Batch loss: 0.020583996549248695\n",
      "Batch loss: 0.06068992614746094\n",
      "Batch loss: 0.026954494416713715\n",
      "Batch loss: 0.07172314822673798\n",
      "Batch loss: 0.11418813467025757\n",
      "Batch loss: 0.09456151723861694\n",
      "Batch loss: 0.01598329469561577\n",
      "Batch loss: 0.07861384004354477\n",
      "Batch loss: 0.14745855331420898\n",
      "Batch loss: 0.07272899895906448\n",
      "Batch loss: 0.08310528099536896\n",
      "Batch loss: 0.04198981449007988\n",
      "Batch loss: 0.04734751209616661\n",
      "Batch loss: 0.06794024258852005\n",
      "Batch loss: 0.02095271274447441\n",
      "Batch loss: 0.006125143729150295\n",
      "Batch loss: 0.020577384158968925\n",
      "Batch loss: 0.06219514459371567\n",
      "Batch loss: 0.07842159271240234\n",
      "Batch loss: 0.06488694250583649\n",
      "Batch loss: 0.051008567214012146\n",
      "Batch loss: 0.032253291457891464\n",
      "Batch loss: 0.06395281106233597\n",
      "Batch loss: 0.03915412351489067\n",
      "Batch loss: 0.009982810355722904\n",
      "Batch loss: 0.06392882764339447\n",
      "Batch loss: 0.07405673712491989\n",
      "Batch loss: 0.06677273660898209\n",
      "Batch loss: 0.021136974915862083\n",
      "Batch loss: 0.08020678162574768\n",
      "Batch loss: 0.04416048526763916\n",
      "Batch loss: 0.053708113729953766\n",
      "Batch loss: 0.04369337484240532\n",
      "Batch loss: 0.030644359067082405\n",
      "Batch loss: 0.02328299917280674\n",
      "Batch loss: 0.03384138643741608\n",
      "Batch loss: 0.10110455006361008\n",
      "Batch loss: 0.047162625938653946\n",
      "Batch loss: 0.030017094686627388\n",
      "Batch loss: 0.04587302356958389\n",
      "Batch loss: 0.0424928218126297\n",
      "Batch loss: 0.03936034068465233\n",
      "Batch loss: 0.02193738892674446\n",
      "Batch loss: 0.0315193347632885\n",
      "Batch loss: 0.013232569210231304\n",
      "Batch loss: 0.02369130216538906\n",
      "Batch loss: 0.03682619333267212\n",
      "Batch loss: 0.041439130902290344\n",
      "Batch loss: 0.04978867620229721\n",
      "Batch loss: 0.03423932567238808\n",
      "Batch loss: 0.08312017470598221\n",
      "Batch loss: 0.06133102998137474\n",
      "Batch loss: 0.0925530269742012\n",
      "Batch loss: 0.09168706834316254\n",
      "Batch loss: 0.03450049087405205\n",
      "Batch loss: 0.028104308992624283\n",
      "Batch loss: 0.0337429903447628\n",
      "Batch loss: 0.016021525487303734\n",
      "Batch loss: 0.019874580204486847\n",
      "Batch loss: 0.03476293012499809\n",
      "Batch loss: 0.037508249282836914\n",
      "Batch loss: 0.02628205716609955\n",
      "Batch loss: 0.028686461970210075\n",
      "Batch loss: 0.059824924916028976\n",
      "Batch loss: 0.05220137536525726\n",
      "Batch loss: 0.08664362877607346\n",
      "Batch loss: 0.04790613800287247\n",
      "Batch loss: 0.018607934936881065\n",
      "Batch loss: 0.03155955299735069\n",
      "Batch loss: 0.08767931163311005\n",
      "Batch loss: 0.02223833091557026\n",
      "Batch loss: 0.03910033777356148\n",
      "Batch loss: 0.07126947492361069\n",
      "Batch loss: 0.10592549294233322\n",
      "Batch loss: 0.06610745936632156\n",
      "Batch loss: 0.020374326035380363\n",
      "Batch loss: 0.06503228098154068\n",
      "Batch loss: 0.10235673189163208\n",
      "Batch loss: 0.03995111957192421\n",
      "Batch loss: 0.04488610476255417\n",
      "Batch loss: 0.022184869274497032\n",
      "Batch loss: 0.07373460382223129\n",
      "Batch loss: 0.076445072889328\n",
      "Batch loss: 0.05316884070634842\n",
      "Batch loss: 0.07105742394924164\n",
      "Batch loss: 0.03967783972620964\n",
      "Batch loss: 0.04990920424461365\n",
      "Batch loss: 0.0279262512922287\n",
      "Batch loss: 0.062041688710451126\n",
      "Batch loss: 0.05373638495802879\n",
      "Batch loss: 0.04160281643271446\n",
      "Batch loss: 0.1306764930486679\n",
      "Batch loss: 0.08384709060192108\n",
      "Batch loss: 0.01622086577117443\n",
      "Batch loss: 0.010693978518247604\n",
      "Batch loss: 0.03977745771408081\n",
      "Batch loss: 0.06846185028553009\n",
      "Batch loss: 0.08619394153356552\n",
      "Batch loss: 0.058574192225933075\n",
      "Batch loss: 0.11499308794736862\n",
      "Batch loss: 0.01112729124724865\n",
      "Batch loss: 0.025682825595140457\n",
      "Batch loss: 0.023035813122987747\n",
      "Batch loss: 0.06401151418685913\n",
      "Batch loss: 0.01105034351348877\n",
      "Batch loss: 0.022476091980934143\n",
      "Batch loss: 0.024423323571681976\n",
      "Batch loss: 0.04685274139046669\n",
      "Batch loss: 0.014904842711985111\n",
      "Batch loss: 0.02752615138888359\n",
      "Batch loss: 0.037485238164663315\n",
      "Batch loss: 0.12248170375823975\n",
      "Batch loss: 0.03954535350203514\n",
      "Batch loss: 0.03284962847828865\n",
      "Batch loss: 0.05293416231870651\n",
      "Batch loss: 0.05560360476374626\n",
      "Batch loss: 0.026880281046032906\n",
      "Batch loss: 0.07113423198461533\n",
      "Batch loss: 0.016108239069581032\n",
      "Batch loss: 0.01839921623468399\n",
      "Batch loss: 0.010120075196027756\n",
      "Batch loss: 0.060871802270412445\n",
      "Batch loss: 0.034496892243623734\n",
      "Batch loss: 0.02404937520623207\n",
      "Batch loss: 0.016430525109171867\n",
      "Batch loss: 0.12498685717582703\n",
      "Batch loss: 0.05198725685477257\n",
      "Batch loss: 0.009658114984631538\n",
      "Batch loss: 0.0347750261425972\n",
      "Batch loss: 0.011123081669211388\n",
      "Batch loss: 0.03239208832383156\n",
      "Batch loss: 0.008251547813415527\n",
      "Batch loss: 0.027257584035396576\n",
      "Batch loss: 0.10762947052717209\n",
      "Batch loss: 0.05010060966014862\n",
      "Batch loss: 0.07242335379123688\n",
      "Batch loss: 0.03474965691566467\n",
      "Batch loss: 0.04300253838300705\n",
      "Batch loss: 0.0520712211728096\n",
      "Batch loss: 0.04686432331800461\n",
      "Batch loss: 0.011213142424821854\n",
      "Batch loss: 0.05129799246788025\n",
      "Batch loss: 0.00856851227581501\n",
      "Batch loss: 0.07306621223688126\n",
      "Batch loss: 0.0393301285803318\n",
      "Batch loss: 0.032060232013463974\n",
      "Batch loss: 0.02699178457260132\n",
      "Batch loss: 0.020084785297513008\n",
      "Batch loss: 0.055603668093681335\n",
      "Batch loss: 0.09380338340997696\n",
      "Batch loss: 0.09288652241230011\n",
      "Batch loss: 0.059078533202409744\n",
      "Batch loss: 0.03964671120047569\n",
      "Batch loss: 0.053792208433151245\n",
      "Batch loss: 0.04467013105750084\n",
      "Batch loss: 0.0495692677795887\n",
      "Batch loss: 0.05697759613394737\n",
      "Batch loss: 0.028713608160614967\n",
      "Batch loss: 0.019961966201663017\n",
      "Batch loss: 0.06997185200452805\n",
      "Batch loss: 0.027555303648114204\n",
      "Batch loss: 0.021006520837545395\n",
      "Batch loss: 0.04105745255947113\n",
      "Batch loss: 0.04305010661482811\n",
      "Batch loss: 0.06913827359676361\n",
      "Batch loss: 0.05227229744195938\n",
      "Batch loss: 0.049594003707170486\n",
      "Batch loss: 0.05012311041355133\n",
      "Batch loss: 0.09212025254964828\n",
      "Batch loss: 0.03941578045487404\n",
      "Batch loss: 0.02052554488182068\n",
      "Batch loss: 0.06601545214653015\n",
      "Batch loss: 0.07615359872579575\n",
      "Batch loss: 0.029659274965524673\n",
      "Batch loss: 0.04765476658940315\n",
      "Batch loss: 0.02299676463007927\n",
      "Batch loss: 0.03905444219708443\n",
      "Batch loss: 0.10414040833711624\n",
      "Batch loss: 0.039971984922885895\n",
      "Batch loss: 0.016533873975276947\n",
      "Batch loss: 0.08502829819917679\n",
      "Batch loss: 0.16107416152954102\n",
      "Batch loss: 0.04716352000832558\n",
      "Batch loss: 0.07517483085393906\n",
      "Batch loss: 0.0424446165561676\n",
      "Batch loss: 0.025392374023795128\n",
      "Batch loss: 0.027858315035700798\n",
      "Batch loss: 0.0727003738284111\n",
      "Batch loss: 0.012425877153873444\n",
      "Batch loss: 0.06376239657402039\n",
      "Batch loss: 0.040267765522003174\n",
      "Batch loss: 0.07328261435031891\n",
      "Batch loss: 0.06259506940841675\n",
      "Batch loss: 0.13730676472187042\n",
      "Batch loss: 0.08192630857229233\n",
      "Batch loss: 0.011232652701437473\n",
      "Batch loss: 0.034456588327884674\n",
      "Batch loss: 0.04926598072052002\n",
      "Batch loss: 0.10424213856458664\n",
      "Batch loss: 0.017781779170036316\n",
      "Batch loss: 0.026895904913544655\n",
      "Batch loss: 0.06920882314443588\n",
      "Batch loss: 0.02636519819498062\n",
      "Batch loss: 0.02169085294008255\n",
      "Batch loss: 0.016934379935264587\n",
      "Batch loss: 0.024138951674103737\n",
      "Batch loss: 0.03068779781460762\n",
      "Batch loss: 0.020921578630805016\n",
      "Batch loss: 0.10321780294179916\n",
      "Batch loss: 0.018170541152358055\n",
      "Batch loss: 0.036638062447309494\n",
      "Batch loss: 0.038470733910799026\n",
      "Batch loss: 0.007453952915966511\n",
      "Batch loss: 0.03705442324280739\n",
      "Batch loss: 0.02071646973490715\n",
      "Batch loss: 0.020087674260139465\n",
      "Batch loss: 0.04551703482866287\n",
      "Batch loss: 0.06809519231319427\n",
      "Batch loss: 0.027070309966802597\n",
      "Batch loss: 0.06973475217819214\n",
      "Batch loss: 0.005154517479240894\n",
      "Batch loss: 0.09851997345685959\n",
      "Batch loss: 0.016468537971377373\n",
      "Batch loss: 0.019347302615642548\n",
      "Batch loss: 0.036158375442028046\n",
      "Batch loss: 0.07954458147287369\n",
      "Batch loss: 0.048083581030368805\n",
      "Batch loss: 0.0686226338148117\n",
      "Batch loss: 0.024408215656876564\n",
      "Batch loss: 0.02890552394092083\n",
      "Batch loss: 0.05892306938767433\n",
      "Batch loss: 0.057446662336587906\n",
      "Batch loss: 0.04570477083325386\n",
      "Batch loss: 0.0288426261395216\n",
      "Batch loss: 0.026061587035655975\n",
      "Batch loss: 0.06004906818270683\n",
      "Batch loss: 0.1143900454044342\n",
      "Batch loss: 0.016490675508975983\n",
      "Batch loss: 0.05040300264954567\n",
      "Batch loss: 0.058652084320783615\n",
      "Batch loss: 0.013218000531196594\n",
      "Batch loss: 0.044386476278305054\n",
      "Batch loss: 0.01446760818362236\n",
      "Batch loss: 0.03703578934073448\n",
      "Batch loss: 0.01756652444601059\n",
      "Batch loss: 0.047388531267642975\n",
      "Batch loss: 0.027469316497445107\n",
      "Batch loss: 0.021332290023565292\n",
      "Batch loss: 0.040369149297475815\n",
      "Batch loss: 0.024250103160738945\n",
      "Batch loss: 0.009369206614792347\n",
      "Batch loss: 0.0276841651648283\n",
      "Batch loss: 0.10193807631731033\n",
      "Batch loss: 0.013249787501990795\n",
      "Batch loss: 0.020106211304664612\n",
      "Batch loss: 0.15281611680984497\n",
      "Batch loss: 0.09829472005367279\n",
      "Batch loss: 0.05005939304828644\n",
      "Batch loss: 0.050449974834918976\n",
      "Batch loss: 0.034390490502119064\n",
      "Batch loss: 0.01548744272440672\n",
      "Batch loss: 0.08564504235982895\n",
      "Batch loss: 0.041012853384017944\n",
      "Batch loss: 0.061962712556123734\n",
      "Batch loss: 0.035336196422576904\n",
      "Batch loss: 0.034874431788921356\n",
      "Batch loss: 0.04902391508221626\n",
      "Batch loss: 0.04092954471707344\n",
      "Batch loss: 0.018786968663334846\n",
      "Batch loss: 0.08521635830402374\n",
      "Batch loss: 0.019943401217460632\n",
      "Batch loss: 0.02870091050863266\n",
      "Batch loss: 0.05090244859457016\n",
      "Batch loss: 0.05874207988381386\n",
      "Batch loss: 0.017487019300460815\n",
      "Batch loss: 0.032642535865306854\n",
      "Batch loss: 0.09438465535640717\n",
      "Batch loss: 0.052305180579423904\n",
      "Batch loss: 0.05556607246398926\n",
      "Batch loss: 0.046093668788671494\n",
      "Batch loss: 0.025658372789621353\n",
      "Batch loss: 0.03919798880815506\n",
      "Batch loss: 0.011255435645580292\n",
      "Batch loss: 0.032738279551267624\n",
      "Batch loss: 0.05557353422045708\n",
      "Batch loss: 0.015624593943357468\n",
      "Batch loss: 0.028137778863310814\n",
      "Batch loss: 0.024793127551674843\n",
      "Batch loss: 0.041405659168958664\n",
      "Batch loss: 0.062401704490184784\n",
      "Batch loss: 0.02846253477036953\n",
      "Batch loss: 0.026929328218102455\n",
      "Batch loss: 0.05958947539329529\n",
      "Batch loss: 0.05019599199295044\n",
      "Batch loss: 0.011943378485739231\n",
      "Batch loss: 0.027767350897192955\n",
      "Batch loss: 0.04179142415523529\n",
      "Batch loss: 0.007570152170956135\n",
      "Batch loss: 0.02208656445145607\n",
      "Batch loss: 0.029328828677535057\n",
      "Batch loss: 0.09912896901369095\n",
      "Batch loss: 0.016160570085048676\n",
      "Batch loss: 0.04441683739423752\n",
      "Batch loss: 0.019617022946476936\n",
      "Batch loss: 0.018493490293622017\n",
      "Batch loss: 0.04859345778822899\n",
      "Batch loss: 0.026285400614142418\n",
      "Batch loss: 0.04792680963873863\n",
      "Batch loss: 0.01751979999244213\n",
      "Batch loss: 0.021116571500897408\n",
      "Batch loss: 0.010637610219419003\n",
      "Batch loss: 0.022243300452828407\n",
      "Batch loss: 0.023747045546770096\n",
      "Batch loss: 0.03518739342689514\n",
      "Batch loss: 0.01763623207807541\n",
      "Epoch [10/100], Loss: 0.0450\n",
      "Validation Loss: 0.4978, Accuracy: 91.08%\n",
      "Batch loss: 0.04838176071643829\n",
      "Batch loss: 0.024530695751309395\n",
      "Batch loss: 0.01525632943958044\n",
      "Batch loss: 0.04865022376179695\n",
      "Batch loss: 0.03504185378551483\n",
      "Batch loss: 0.03193502873182297\n",
      "Batch loss: 0.15168698132038116\n",
      "Batch loss: 0.00863291323184967\n",
      "Batch loss: 0.0514264740049839\n",
      "Batch loss: 0.015539856627583504\n",
      "Batch loss: 0.03828011080622673\n",
      "Batch loss: 0.014363962225615978\n",
      "Batch loss: 0.006544264033436775\n",
      "Batch loss: 0.06799181550741196\n",
      "Batch loss: 0.09417106956243515\n",
      "Batch loss: 0.0647548958659172\n",
      "Batch loss: 0.05310570448637009\n",
      "Batch loss: 0.017381642013788223\n",
      "Batch loss: 0.007011736277490854\n",
      "Batch loss: 0.02856718935072422\n",
      "Batch loss: 0.03311057388782501\n",
      "Batch loss: 0.020663313567638397\n",
      "Batch loss: 0.011393307708203793\n",
      "Batch loss: 0.009381632320582867\n",
      "Batch loss: 0.034350331872701645\n",
      "Batch loss: 0.025255640968680382\n",
      "Batch loss: 0.0033775141928344965\n",
      "Batch loss: 0.03555222973227501\n",
      "Batch loss: 0.06265527009963989\n",
      "Batch loss: 0.05721878260374069\n",
      "Batch loss: 0.108395516872406\n",
      "Batch loss: 0.022258687764406204\n",
      "Batch loss: 0.0857788622379303\n",
      "Batch loss: 0.05542730912566185\n",
      "Batch loss: 0.07535849511623383\n",
      "Batch loss: 0.04276486858725548\n",
      "Batch loss: 0.05364993214607239\n",
      "Batch loss: 0.04318707808852196\n",
      "Batch loss: 0.06800270825624466\n",
      "Batch loss: 0.019321857020258904\n",
      "Batch loss: 0.05255236476659775\n",
      "Batch loss: 0.0652584657073021\n",
      "Batch loss: 0.10196622461080551\n",
      "Batch loss: 0.05789612606167793\n",
      "Batch loss: 0.03745042532682419\n",
      "Batch loss: 0.1358586996793747\n",
      "Batch loss: 0.09127169847488403\n",
      "Batch loss: 0.02530018985271454\n",
      "Batch loss: 0.061863310635089874\n",
      "Batch loss: 0.02061840146780014\n",
      "Batch loss: 0.050292447209358215\n",
      "Batch loss: 0.07534579187631607\n",
      "Batch loss: 0.0375649631023407\n",
      "Batch loss: 0.06356435269117355\n",
      "Batch loss: 0.036525554955005646\n",
      "Batch loss: 0.04604389891028404\n",
      "Batch loss: 0.02030680887401104\n",
      "Batch loss: 0.04000581428408623\n",
      "Batch loss: 0.047392722219228745\n",
      "Batch loss: 0.05615063011646271\n",
      "Batch loss: 0.024754930287599564\n",
      "Batch loss: 0.10798900574445724\n",
      "Batch loss: 0.05864003300666809\n",
      "Batch loss: 0.060457147657871246\n",
      "Batch loss: 0.02103082649409771\n",
      "Batch loss: 0.04929165914654732\n",
      "Batch loss: 0.03570375591516495\n",
      "Batch loss: 0.0860014408826828\n",
      "Batch loss: 0.030420353636145592\n",
      "Batch loss: 0.021017253398895264\n",
      "Batch loss: 0.14690494537353516\n",
      "Batch loss: 0.10635776072740555\n",
      "Batch loss: 0.026886915788054466\n",
      "Batch loss: 0.012898428365588188\n",
      "Batch loss: 0.03372485935688019\n",
      "Batch loss: 0.0620771087706089\n",
      "Batch loss: 0.03393435850739479\n",
      "Batch loss: 0.03653903678059578\n",
      "Batch loss: 0.015270254574716091\n",
      "Batch loss: 0.0919918492436409\n",
      "Batch loss: 0.0073716770857572556\n",
      "Batch loss: 0.04502727463841438\n",
      "Batch loss: 0.048142701387405396\n",
      "Batch loss: 0.06783539056777954\n",
      "Batch loss: 0.029214637354016304\n",
      "Batch loss: 0.041221022605895996\n",
      "Batch loss: 0.03846950829029083\n",
      "Batch loss: 0.017216555774211884\n",
      "Batch loss: 0.030094759538769722\n",
      "Batch loss: 0.01036093570291996\n",
      "Batch loss: 0.02105119451880455\n",
      "Batch loss: 0.06009929999709129\n",
      "Batch loss: 0.059096675366163254\n",
      "Batch loss: 0.0478680245578289\n",
      "Batch loss: 0.02720009535551071\n",
      "Batch loss: 0.03899207338690758\n",
      "Batch loss: 0.06521924585103989\n",
      "Batch loss: 0.05012119933962822\n",
      "Batch loss: 0.03304228559136391\n",
      "Batch loss: 0.005573964677751064\n",
      "Batch loss: 0.03212031349539757\n",
      "Batch loss: 0.012779397889971733\n",
      "Batch loss: 0.04791061952710152\n",
      "Batch loss: 0.021969087421894073\n",
      "Batch loss: 0.04989199340343475\n",
      "Batch loss: 0.01851513795554638\n",
      "Batch loss: 0.09520937502384186\n",
      "Batch loss: 0.11363573372364044\n",
      "Batch loss: 0.03013715334236622\n",
      "Batch loss: 0.11404245346784592\n",
      "Batch loss: 0.052444662898778915\n",
      "Batch loss: 0.022048143669962883\n",
      "Batch loss: 0.005203112959861755\n",
      "Batch loss: 0.05040888115763664\n",
      "Batch loss: 0.022880762815475464\n",
      "Batch loss: 0.05312161147594452\n",
      "Batch loss: 0.022860949859023094\n",
      "Batch loss: 0.005313726607710123\n",
      "Batch loss: 0.012581460177898407\n",
      "Batch loss: 0.01920640841126442\n",
      "Batch loss: 0.006911654956638813\n",
      "Batch loss: 0.011432857252657413\n",
      "Batch loss: 0.032141245901584625\n",
      "Batch loss: 0.013531907461583614\n",
      "Batch loss: 0.027057552710175514\n",
      "Batch loss: 0.059885911643505096\n",
      "Batch loss: 0.008614929392933846\n",
      "Batch loss: 0.030427319929003716\n",
      "Batch loss: 0.010522904805839062\n",
      "Batch loss: 0.03303499519824982\n",
      "Batch loss: 0.03642220050096512\n",
      "Batch loss: 0.0505065955221653\n",
      "Batch loss: 0.0769214853644371\n",
      "Batch loss: 0.04310610890388489\n",
      "Batch loss: 0.04900949075818062\n",
      "Batch loss: 0.007044251076877117\n",
      "Batch loss: 0.043283458799123764\n",
      "Batch loss: 0.07643395662307739\n",
      "Batch loss: 0.03132115304470062\n",
      "Batch loss: 0.011210629716515541\n",
      "Batch loss: 0.03107226826250553\n",
      "Batch loss: 0.04216901585459709\n",
      "Batch loss: 0.054361291229724884\n",
      "Batch loss: 0.04514170065522194\n",
      "Batch loss: 0.074106864631176\n",
      "Batch loss: 0.04808347299695015\n",
      "Batch loss: 0.029636360704898834\n",
      "Batch loss: 0.013281692750751972\n",
      "Batch loss: 0.01652587577700615\n",
      "Batch loss: 0.07713241130113602\n",
      "Batch loss: 0.025192268192768097\n",
      "Batch loss: 0.02882770635187626\n",
      "Batch loss: 0.07048387825489044\n",
      "Batch loss: 0.020293746143579483\n",
      "Batch loss: 0.029882505536079407\n",
      "Batch loss: 0.05357236787676811\n",
      "Batch loss: 0.03272250294685364\n",
      "Batch loss: 0.025592070072889328\n",
      "Batch loss: 0.020673897117376328\n",
      "Batch loss: 0.04481004178524017\n",
      "Batch loss: 0.01783026196062565\n",
      "Batch loss: 0.06309770047664642\n",
      "Batch loss: 0.02406487800180912\n",
      "Batch loss: 0.06807687133550644\n",
      "Batch loss: 0.041173920035362244\n",
      "Batch loss: 0.02225460857152939\n",
      "Batch loss: 0.01491635013371706\n",
      "Batch loss: 0.013718202710151672\n",
      "Batch loss: 0.020940518006682396\n",
      "Batch loss: 0.02212458848953247\n",
      "Batch loss: 0.02055073156952858\n",
      "Batch loss: 0.028082897886633873\n",
      "Batch loss: 0.01201893761754036\n",
      "Batch loss: 0.013066857121884823\n",
      "Batch loss: 0.046333249658346176\n",
      "Batch loss: 0.07035338878631592\n",
      "Batch loss: 0.03860628232359886\n",
      "Batch loss: 0.018450936302542686\n",
      "Batch loss: 0.03842464089393616\n",
      "Batch loss: 0.005792845971882343\n",
      "Batch loss: 0.03180781379342079\n",
      "Batch loss: 0.028261452913284302\n",
      "Batch loss: 0.008735088631510735\n",
      "Batch loss: 0.08427686244249344\n",
      "Batch loss: 0.06082644686102867\n",
      "Batch loss: 0.018393412232398987\n",
      "Batch loss: 0.01724771410226822\n",
      "Batch loss: 0.008574046194553375\n",
      "Batch loss: 0.03958263248205185\n",
      "Batch loss: 0.04561054706573486\n",
      "Batch loss: 0.017301244661211967\n",
      "Batch loss: 0.06478042155504227\n",
      "Batch loss: 0.054806649684906006\n",
      "Batch loss: 0.03230857104063034\n",
      "Batch loss: 0.028404170647263527\n",
      "Batch loss: 0.024881010875105858\n",
      "Batch loss: 0.012201807461678982\n",
      "Batch loss: 0.016192320734262466\n",
      "Batch loss: 0.03566644340753555\n",
      "Batch loss: 0.05185515433549881\n",
      "Batch loss: 0.03043539449572563\n",
      "Batch loss: 0.06548634171485901\n",
      "Batch loss: 0.023322859779000282\n",
      "Batch loss: 0.00925472192466259\n",
      "Batch loss: 0.06552208214998245\n",
      "Batch loss: 0.042060043662786484\n",
      "Batch loss: 0.030229974538087845\n",
      "Batch loss: 0.037709347903728485\n",
      "Batch loss: 0.014225170016288757\n",
      "Batch loss: 0.027065372094511986\n",
      "Batch loss: 0.020511547103524208\n",
      "Batch loss: 0.06514089554548264\n",
      "Batch loss: 0.013198457658290863\n",
      "Batch loss: 0.06141640245914459\n",
      "Batch loss: 0.014838424511253834\n",
      "Batch loss: 0.015462590381503105\n",
      "Batch loss: 0.006061419378966093\n",
      "Batch loss: 0.019870908930897713\n",
      "Batch loss: 0.017257165163755417\n",
      "Batch loss: 0.04256482422351837\n",
      "Batch loss: 0.011775149032473564\n",
      "Batch loss: 0.03055942989885807\n",
      "Batch loss: 0.020146144554018974\n",
      "Batch loss: 0.05611129850149155\n",
      "Batch loss: 0.011571233160793781\n",
      "Batch loss: 0.09608601033687592\n",
      "Batch loss: 0.06673354655504227\n",
      "Batch loss: 0.062168773263692856\n",
      "Batch loss: 0.02230069413781166\n",
      "Batch loss: 0.01430906355381012\n",
      "Batch loss: 0.07511592656373978\n",
      "Batch loss: 0.010712259449064732\n",
      "Batch loss: 0.01924363523721695\n",
      "Batch loss: 0.003322240198031068\n",
      "Batch loss: 0.025351507589221\n",
      "Batch loss: 0.09483352303504944\n",
      "Batch loss: 0.038327958434820175\n",
      "Batch loss: 0.10014757513999939\n",
      "Batch loss: 0.021770961582660675\n",
      "Batch loss: 0.02158184163272381\n",
      "Batch loss: 0.04154001548886299\n",
      "Batch loss: 0.021226346492767334\n",
      "Batch loss: 0.038848526775836945\n",
      "Batch loss: 0.03491989150643349\n",
      "Batch loss: 0.047158945351839066\n",
      "Batch loss: 0.035709064453840256\n",
      "Batch loss: 0.054316747933626175\n",
      "Batch loss: 0.030630851164460182\n",
      "Batch loss: 0.029459724202752113\n",
      "Batch loss: 0.015123658813536167\n",
      "Batch loss: 0.09913124144077301\n",
      "Batch loss: 0.05658078193664551\n",
      "Batch loss: 0.015998197719454765\n",
      "Batch loss: 0.03871866688132286\n",
      "Batch loss: 0.03357665613293648\n",
      "Batch loss: 0.04450502246618271\n",
      "Batch loss: 0.024582579731941223\n",
      "Batch loss: 0.02578236162662506\n",
      "Batch loss: 0.025810865685343742\n",
      "Batch loss: 0.03722100332379341\n",
      "Batch loss: 0.019550759345293045\n",
      "Batch loss: 0.030422380194067955\n",
      "Batch loss: 0.04086408019065857\n",
      "Batch loss: 0.01796993426978588\n",
      "Batch loss: 0.0157447699457407\n",
      "Batch loss: 0.022111278027296066\n",
      "Batch loss: 0.019592108204960823\n",
      "Batch loss: 0.012916299514472485\n",
      "Batch loss: 0.045150455087423325\n",
      "Batch loss: 0.046251073479652405\n",
      "Batch loss: 0.0228902418166399\n",
      "Batch loss: 0.039584532380104065\n",
      "Batch loss: 0.04187668114900589\n",
      "Batch loss: 0.02910807728767395\n",
      "Batch loss: 0.0263837780803442\n",
      "Batch loss: 0.026559367775917053\n",
      "Batch loss: 0.014926874078810215\n",
      "Batch loss: 0.05401633307337761\n",
      "Batch loss: 0.024794749915599823\n",
      "Batch loss: 0.05042235180735588\n",
      "Batch loss: 0.01248856820166111\n",
      "Batch loss: 0.021316414698958397\n",
      "Batch loss: 0.028864910826086998\n",
      "Batch loss: 0.05664539337158203\n",
      "Batch loss: 0.008363181725144386\n",
      "Batch loss: 0.042115841060876846\n",
      "Batch loss: 0.060505907982587814\n",
      "Batch loss: 0.02180563472211361\n",
      "Batch loss: 0.0026276730932295322\n",
      "Batch loss: 0.01660170964896679\n",
      "Batch loss: 0.010902744717895985\n",
      "Batch loss: 0.015597193501889706\n",
      "Batch loss: 0.04631367325782776\n",
      "Batch loss: 0.013255286030471325\n",
      "Batch loss: 0.01952497661113739\n",
      "Batch loss: 0.03824448585510254\n",
      "Batch loss: 0.022080618888139725\n",
      "Batch loss: 0.028472384437918663\n",
      "Batch loss: 0.03643358498811722\n",
      "Batch loss: 0.040965378284454346\n",
      "Batch loss: 0.00831089448183775\n",
      "Batch loss: 0.04156383499503136\n",
      "Batch loss: 0.025033174082636833\n",
      "Batch loss: 0.008653069846332073\n",
      "Batch loss: 0.006717792246490717\n",
      "Batch loss: 0.028849441558122635\n",
      "Batch loss: 0.027130965143442154\n",
      "Batch loss: 0.014239102602005005\n",
      "Batch loss: 0.026526402682065964\n",
      "Batch loss: 0.016413310542702675\n",
      "Batch loss: 0.035299889743328094\n",
      "Batch loss: 0.010655985213816166\n",
      "Batch loss: 0.010255392640829086\n",
      "Batch loss: 0.03031095862388611\n",
      "Batch loss: 0.03360789269208908\n",
      "Batch loss: 0.01562829501926899\n",
      "Batch loss: 0.038533613085746765\n",
      "Batch loss: 0.021702365949749947\n",
      "Batch loss: 0.07897871732711792\n",
      "Batch loss: 0.06835811585187912\n",
      "Batch loss: 0.01355503499507904\n",
      "Batch loss: 0.009061003103852272\n",
      "Batch loss: 0.0657840371131897\n",
      "Batch loss: 0.09344062209129333\n",
      "Batch loss: 0.028585223481059074\n",
      "Batch loss: 0.07283565402030945\n",
      "Batch loss: 0.015208023600280285\n",
      "Batch loss: 0.03509163111448288\n",
      "Batch loss: 0.02626141533255577\n",
      "Batch loss: 0.10583171248435974\n",
      "Batch loss: 0.030802661553025246\n",
      "Batch loss: 0.05359593406319618\n",
      "Batch loss: 0.012840538285672665\n",
      "Batch loss: 0.02935326099395752\n",
      "Batch loss: 0.061949145048856735\n",
      "Batch loss: 0.024668319150805473\n",
      "Batch loss: 0.02100488170981407\n",
      "Batch loss: 0.026932336390018463\n",
      "Batch loss: 0.06169296056032181\n",
      "Batch loss: 0.033609241247177124\n",
      "Batch loss: 0.05215940997004509\n",
      "Batch loss: 0.01772787608206272\n",
      "Batch loss: 0.03759179264307022\n",
      "Batch loss: 0.05168214812874794\n",
      "Batch loss: 0.01732339709997177\n",
      "Batch loss: 0.03543318435549736\n",
      "Batch loss: 0.036072440445423126\n",
      "Batch loss: 0.009302455931901932\n",
      "Batch loss: 0.05747297778725624\n",
      "Batch loss: 0.019108129665255547\n",
      "Batch loss: 0.04035648703575134\n",
      "Batch loss: 0.03675243258476257\n",
      "Batch loss: 0.03479509428143501\n",
      "Batch loss: 0.010243179276585579\n",
      "Batch loss: 0.026400433853268623\n",
      "Batch loss: 0.023044394329190254\n",
      "Batch loss: 0.007741671055555344\n",
      "Batch loss: 0.016687635332345963\n",
      "Batch loss: 0.030809685587882996\n",
      "Batch loss: 0.016636110842227936\n",
      "Batch loss: 0.051225870847702026\n",
      "Batch loss: 0.035751666873693466\n",
      "Batch loss: 0.04810543730854988\n",
      "Batch loss: 0.008364282548427582\n",
      "Batch loss: 0.02839381992816925\n",
      "Batch loss: 0.02288827858865261\n",
      "Batch loss: 0.03948167711496353\n",
      "Batch loss: 0.015539276413619518\n",
      "Batch loss: 0.00988056231290102\n",
      "Batch loss: 0.05898081138730049\n",
      "Batch loss: 0.018365465104579926\n",
      "Batch loss: 0.016425687819719315\n",
      "Batch loss: 0.026921015232801437\n",
      "Batch loss: 0.02640973962843418\n",
      "Batch loss: 0.0671611949801445\n",
      "Batch loss: 0.016814352944493294\n",
      "Batch loss: 0.014681247062981129\n",
      "Batch loss: 0.004250053316354752\n",
      "Batch loss: 0.04845442250370979\n",
      "Batch loss: 0.01757781207561493\n",
      "Batch loss: 0.014221965335309505\n",
      "Batch loss: 0.03395352140069008\n",
      "Batch loss: 0.02771201729774475\n",
      "Batch loss: 0.02841450646519661\n",
      "Batch loss: 0.010128099471330643\n",
      "Batch loss: 0.025957098230719566\n",
      "Batch loss: 0.1097610592842102\n",
      "Batch loss: 0.04275205358862877\n",
      "Batch loss: 0.017077045515179634\n",
      "Batch loss: 0.0786271020770073\n",
      "Batch loss: 0.0073868827894330025\n",
      "Batch loss: 0.015572606585919857\n",
      "Batch loss: 0.012651602737605572\n",
      "Batch loss: 0.043157145380973816\n",
      "Batch loss: 0.06362640857696533\n",
      "Batch loss: 0.012938035652041435\n",
      "Batch loss: 0.01268076803535223\n",
      "Batch loss: 0.010576392523944378\n",
      "Batch loss: 0.05196426436305046\n",
      "Batch loss: 0.02312093786895275\n",
      "Batch loss: 0.030610034242272377\n",
      "Batch loss: 0.004866871517151594\n",
      "Batch loss: 0.026220843195915222\n",
      "Batch loss: 0.013323090970516205\n",
      "Batch loss: 0.037890125066041946\n",
      "Batch loss: 0.05264439806342125\n",
      "Batch loss: 0.012703033164143562\n",
      "Batch loss: 0.03385904058814049\n",
      "Batch loss: 0.031240476295351982\n",
      "Batch loss: 0.03911564126610756\n",
      "Batch loss: 0.02492300048470497\n",
      "Batch loss: 0.04203401133418083\n",
      "Batch loss: 0.0238684993237257\n",
      "Batch loss: 0.052454959601163864\n",
      "Batch loss: 0.05827365443110466\n",
      "Batch loss: 0.0335615910589695\n",
      "Batch loss: 0.03001215308904648\n",
      "Epoch [11/100], Loss: 0.0364\n",
      "Validation Loss: 0.4132, Accuracy: 93.86%\n",
      "Batch loss: 0.07198365032672882\n",
      "Batch loss: 0.02059084177017212\n",
      "Batch loss: 0.07208140194416046\n",
      "Batch loss: 0.02976997382938862\n",
      "Batch loss: 0.006749421823769808\n",
      "Batch loss: 0.07116428762674332\n",
      "Batch loss: 0.0976562649011612\n",
      "Batch loss: 0.017179427668452263\n",
      "Batch loss: 0.02084335871040821\n",
      "Batch loss: 0.007065929006785154\n",
      "Batch loss: 0.05442371219396591\n",
      "Batch loss: 0.06576960533857346\n",
      "Batch loss: 0.01666872575879097\n",
      "Batch loss: 0.0741572305560112\n",
      "Batch loss: 0.06560971587896347\n",
      "Batch loss: 0.04267144575715065\n",
      "Batch loss: 0.056627389043569565\n",
      "Batch loss: 0.02976808324456215\n",
      "Batch loss: 0.010817773640155792\n",
      "Batch loss: 0.053236156702041626\n",
      "Batch loss: 0.019754277542233467\n",
      "Batch loss: 0.037221863865852356\n",
      "Batch loss: 0.08003031462430954\n",
      "Batch loss: 0.055568017065525055\n",
      "Batch loss: 0.002457232680171728\n",
      "Batch loss: 0.03999800235033035\n",
      "Batch loss: 0.015574918128550053\n",
      "Batch loss: 0.06903406977653503\n",
      "Batch loss: 0.040021803230047226\n",
      "Batch loss: 0.02899528294801712\n",
      "Batch loss: 0.0582607164978981\n",
      "Batch loss: 0.02360050193965435\n",
      "Batch loss: 0.02834341675043106\n",
      "Batch loss: 0.022612351924180984\n",
      "Batch loss: 0.05496792867779732\n",
      "Batch loss: 0.039058320224285126\n",
      "Batch loss: 0.014626151882112026\n",
      "Batch loss: 0.06131136789917946\n",
      "Batch loss: 0.042682547122240067\n",
      "Batch loss: 0.008015021681785583\n",
      "Batch loss: 0.02284805104136467\n",
      "Batch loss: 0.012038376182317734\n",
      "Batch loss: 0.0430450402200222\n",
      "Batch loss: 0.03597402945160866\n",
      "Batch loss: 0.00990142859518528\n",
      "Batch loss: 0.03295736014842987\n",
      "Batch loss: 0.06599755585193634\n",
      "Batch loss: 0.06927026808261871\n",
      "Batch loss: 0.043603017926216125\n",
      "Batch loss: 0.01884395256638527\n",
      "Batch loss: 0.04118180647492409\n",
      "Batch loss: 0.05527728050947189\n",
      "Batch loss: 0.015486088581383228\n",
      "Batch loss: 0.011450172401964664\n",
      "Batch loss: 0.05252677574753761\n",
      "Batch loss: 0.06880757957696915\n",
      "Batch loss: 0.03231872245669365\n",
      "Batch loss: 0.01851160265505314\n",
      "Batch loss: 0.016973072662949562\n",
      "Batch loss: 0.03433004021644592\n",
      "Batch loss: 0.028561992570757866\n",
      "Batch loss: 0.05376275256276131\n",
      "Batch loss: 0.027389267459511757\n",
      "Batch loss: 0.053660474717617035\n",
      "Batch loss: 0.05306832492351532\n",
      "Batch loss: 0.040449533611536026\n",
      "Batch loss: 0.04326573759317398\n",
      "Batch loss: 0.03850945085287094\n",
      "Batch loss: 0.021180476993322372\n",
      "Batch loss: 0.018501222133636475\n",
      "Batch loss: 0.021899865940213203\n",
      "Batch loss: 0.020595233887434006\n",
      "Batch loss: 0.01202377025038004\n",
      "Batch loss: 0.0268117543309927\n",
      "Batch loss: 0.02774592489004135\n",
      "Batch loss: 0.03788761422038078\n",
      "Batch loss: 0.06024633347988129\n",
      "Batch loss: 0.01894642785191536\n",
      "Batch loss: 0.01700134389102459\n",
      "Batch loss: 0.0062389555387198925\n",
      "Batch loss: 0.028207190334796906\n",
      "Batch loss: 0.021404186263680458\n",
      "Batch loss: 0.07730422914028168\n",
      "Batch loss: 0.05492181330919266\n",
      "Batch loss: 0.03292844071984291\n",
      "Batch loss: 0.07874241471290588\n",
      "Batch loss: 0.009027949534356594\n",
      "Batch loss: 0.028835363686084747\n",
      "Batch loss: 0.022820526733994484\n",
      "Batch loss: 0.01393877062946558\n",
      "Batch loss: 0.05439954996109009\n",
      "Batch loss: 0.01886654458940029\n",
      "Batch loss: 0.0557134747505188\n",
      "Batch loss: 0.07997998595237732\n",
      "Batch loss: 0.1099524050951004\n",
      "Batch loss: 0.015651755034923553\n",
      "Batch loss: 0.009676499292254448\n",
      "Batch loss: 0.023717019706964493\n",
      "Batch loss: 0.08521655946969986\n",
      "Batch loss: 0.04949776455760002\n",
      "Batch loss: 0.029713962227106094\n",
      "Batch loss: 0.019678641110658646\n",
      "Batch loss: 0.01974526047706604\n",
      "Batch loss: 0.03919587284326553\n",
      "Batch loss: 0.015053088776767254\n",
      "Batch loss: 0.011388156563043594\n",
      "Batch loss: 0.04556957259774208\n",
      "Batch loss: 0.06539953500032425\n",
      "Batch loss: 0.07971282303333282\n",
      "Batch loss: 0.019509099423885345\n",
      "Batch loss: 0.04487757384777069\n",
      "Batch loss: 0.004070035647600889\n",
      "Batch loss: 0.027345027774572372\n",
      "Batch loss: 0.0390649251639843\n",
      "Batch loss: 0.03008268214762211\n",
      "Batch loss: 0.08921834081411362\n",
      "Batch loss: 0.010259214788675308\n",
      "Batch loss: 0.008117740042507648\n",
      "Batch loss: 0.02499420754611492\n",
      "Batch loss: 0.008758882991969585\n",
      "Batch loss: 0.10637665539979935\n",
      "Batch loss: 0.009828941896557808\n",
      "Batch loss: 0.028055327013134956\n",
      "Batch loss: 0.03632482886314392\n",
      "Batch loss: 0.02995835803449154\n",
      "Batch loss: 0.01797226257622242\n",
      "Batch loss: 0.030979374423623085\n",
      "Batch loss: 0.015931107103824615\n",
      "Batch loss: 0.027069618925452232\n",
      "Batch loss: 0.07684022933244705\n",
      "Batch loss: 0.02938755229115486\n",
      "Batch loss: 0.027363596484065056\n",
      "Batch loss: 0.029669305309653282\n",
      "Batch loss: 0.047109901905059814\n",
      "Batch loss: 0.03267652913928032\n",
      "Batch loss: 0.0053290147334337234\n",
      "Batch loss: 0.005475124344229698\n",
      "Batch loss: 0.028926508501172066\n",
      "Batch loss: 0.013829120434820652\n",
      "Batch loss: 0.004422009456902742\n",
      "Batch loss: 0.022923292592167854\n",
      "Batch loss: 0.016661236062645912\n",
      "Batch loss: 0.06389094144105911\n",
      "Batch loss: 0.032317690551280975\n",
      "Batch loss: 0.015240967273712158\n",
      "Batch loss: 0.018010947853326797\n",
      "Batch loss: 0.03856698051095009\n",
      "Batch loss: 0.03151348605751991\n",
      "Batch loss: 0.018757695332169533\n",
      "Batch loss: 0.006163535173982382\n",
      "Batch loss: 0.07395878434181213\n",
      "Batch loss: 0.03132878988981247\n",
      "Batch loss: 0.022311462089419365\n",
      "Batch loss: 0.05665115267038345\n",
      "Batch loss: 0.01206306740641594\n",
      "Batch loss: 0.012089666910469532\n",
      "Batch loss: 0.07872321456670761\n",
      "Batch loss: 0.05474070459604263\n",
      "Batch loss: 0.04365518316626549\n",
      "Batch loss: 0.045943472534418106\n",
      "Batch loss: 0.04220554977655411\n",
      "Batch loss: 0.06277082115411758\n",
      "Batch loss: 0.02079991064965725\n",
      "Batch loss: 0.014445926994085312\n",
      "Batch loss: 0.026453252881765366\n",
      "Batch loss: 0.017967965453863144\n",
      "Batch loss: 0.025761069729924202\n",
      "Batch loss: 0.026379665359854698\n",
      "Batch loss: 0.036421094089746475\n",
      "Batch loss: 0.014524891972541809\n",
      "Batch loss: 0.045009009540081024\n",
      "Batch loss: 0.05610301345586777\n",
      "Batch loss: 0.012812023051083088\n",
      "Batch loss: 0.05762147530913353\n",
      "Batch loss: 0.02451506443321705\n",
      "Batch loss: 0.030121352523565292\n",
      "Batch loss: 0.01426367275416851\n",
      "Batch loss: 0.022518116980791092\n",
      "Batch loss: 0.05600464344024658\n",
      "Batch loss: 0.04600878059864044\n",
      "Batch loss: 0.05483085289597511\n",
      "Batch loss: 0.010822901502251625\n",
      "Batch loss: 0.04094405472278595\n",
      "Batch loss: 0.02734813094139099\n",
      "Batch loss: 0.03500402346253395\n",
      "Batch loss: 0.03259293735027313\n",
      "Batch loss: 0.06408262252807617\n",
      "Batch loss: 0.03784167394042015\n",
      "Batch loss: 0.017272761091589928\n",
      "Batch loss: 0.020724253728985786\n",
      "Batch loss: 0.01374826580286026\n",
      "Batch loss: 0.05488584563136101\n",
      "Batch loss: 0.01690755784511566\n",
      "Batch loss: 0.011989042162895203\n",
      "Batch loss: 0.0064793601632118225\n",
      "Batch loss: 0.01711585558950901\n",
      "Batch loss: 0.006418935023248196\n",
      "Batch loss: 0.013456388376653194\n",
      "Batch loss: 0.03625565767288208\n",
      "Batch loss: 0.026602493599057198\n",
      "Batch loss: 0.07561162114143372\n",
      "Batch loss: 0.007525550667196512\n",
      "Batch loss: 0.003152247751131654\n",
      "Batch loss: 0.015989581122994423\n",
      "Batch loss: 0.023372508585453033\n",
      "Batch loss: 0.07982847094535828\n",
      "Batch loss: 0.030532527714967728\n",
      "Batch loss: 0.007088573183864355\n",
      "Batch loss: 0.06052320823073387\n",
      "Batch loss: 0.02073465660214424\n",
      "Batch loss: 0.015188705176115036\n",
      "Batch loss: 0.011609058827161789\n",
      "Batch loss: 0.040672317147254944\n",
      "Batch loss: 0.042523808777332306\n",
      "Batch loss: 0.04733363166451454\n",
      "Batch loss: 0.013604093343019485\n",
      "Batch loss: 0.06932947784662247\n",
      "Batch loss: 0.025234831497073174\n",
      "Batch loss: 0.017481500282883644\n",
      "Batch loss: 0.01989620178937912\n",
      "Batch loss: 0.009011943824589252\n",
      "Batch loss: 0.012783759273588657\n",
      "Batch loss: 0.045856621116399765\n",
      "Batch loss: 0.03424587473273277\n",
      "Batch loss: 0.004936625249683857\n",
      "Batch loss: 0.040433309972286224\n",
      "Batch loss: 0.008271756581962109\n",
      "Batch loss: 0.024389425292611122\n",
      "Batch loss: 0.025672761723399162\n",
      "Batch loss: 0.04643242433667183\n",
      "Batch loss: 0.025465745478868484\n",
      "Batch loss: 0.00398383941501379\n",
      "Batch loss: 0.024049194529652596\n",
      "Batch loss: 0.004342793021351099\n",
      "Batch loss: 0.010157685726881027\n",
      "Batch loss: 0.012984488159418106\n",
      "Batch loss: 0.04465118795633316\n",
      "Batch loss: 0.04968993365764618\n",
      "Batch loss: 0.06407091021537781\n",
      "Batch loss: 0.05145825818181038\n",
      "Batch loss: 0.06211299076676369\n",
      "Batch loss: 0.012174287810921669\n",
      "Batch loss: 0.05552399903535843\n",
      "Batch loss: 0.0120473001152277\n",
      "Batch loss: 0.047718800604343414\n",
      "Batch loss: 0.012139597907662392\n",
      "Batch loss: 0.034600745886564255\n",
      "Batch loss: 0.019371652975678444\n",
      "Batch loss: 0.011999065987765789\n",
      "Batch loss: 0.01520776841789484\n",
      "Batch loss: 0.02575058676302433\n",
      "Batch loss: 0.037688832730054855\n",
      "Batch loss: 0.026785407215356827\n",
      "Batch loss: 0.14905951917171478\n",
      "Batch loss: 0.09035969525575638\n",
      "Batch loss: 0.0340656042098999\n",
      "Batch loss: 0.04290022701025009\n",
      "Batch loss: 0.005975249223411083\n",
      "Batch loss: 0.01846967451274395\n",
      "Batch loss: 0.04690587520599365\n",
      "Batch loss: 0.054128922522068024\n",
      "Batch loss: 0.024347659200429916\n",
      "Batch loss: 0.07300201803445816\n",
      "Batch loss: 0.022942878305912018\n",
      "Batch loss: 0.0019243471324443817\n",
      "Batch loss: 0.00651171850040555\n",
      "Batch loss: 0.08607062697410583\n",
      "Batch loss: 0.03643910959362984\n",
      "Batch loss: 0.019757356494665146\n",
      "Batch loss: 0.010135587304830551\n",
      "Batch loss: 0.028290128335356712\n",
      "Batch loss: 0.024789586663246155\n",
      "Batch loss: 0.01990620233118534\n",
      "Batch loss: 0.012040603905916214\n",
      "Batch loss: 0.026531245559453964\n",
      "Batch loss: 0.01140798069536686\n",
      "Batch loss: 0.017359890043735504\n",
      "Batch loss: 0.01851140335202217\n",
      "Batch loss: 0.03518766537308693\n",
      "Batch loss: 0.05665208771824837\n",
      "Batch loss: 0.026996592059731483\n",
      "Batch loss: 0.02021034248173237\n",
      "Batch loss: 0.030015045776963234\n",
      "Batch loss: 0.0481116883456707\n",
      "Batch loss: 0.07624669373035431\n",
      "Batch loss: 0.031114486977458\n",
      "Batch loss: 0.01115372683852911\n",
      "Batch loss: 0.0056133633479475975\n",
      "Batch loss: 0.011870351620018482\n",
      "Batch loss: 0.009139048866927624\n",
      "Batch loss: 0.013039880432188511\n",
      "Batch loss: 0.0059993383474648\n",
      "Batch loss: 0.031610798090696335\n",
      "Batch loss: 0.026823297142982483\n",
      "Batch loss: 0.009139311499893665\n",
      "Batch loss: 0.02909710444509983\n",
      "Batch loss: 0.02534106746315956\n",
      "Batch loss: 0.016608230769634247\n",
      "Batch loss: 0.039367467164993286\n",
      "Batch loss: 0.06772409379482269\n",
      "Batch loss: 0.03454601392149925\n",
      "Batch loss: 0.02347605489194393\n",
      "Batch loss: 0.023330213502049446\n",
      "Batch loss: 0.015505500137805939\n",
      "Batch loss: 0.09928031265735626\n",
      "Batch loss: 0.03169867768883705\n",
      "Batch loss: 0.010631638579070568\n",
      "Batch loss: 0.012326503172516823\n",
      "Batch loss: 0.07226494699716568\n",
      "Batch loss: 0.051611121743917465\n",
      "Batch loss: 0.015337185002863407\n",
      "Batch loss: 0.01911439746618271\n",
      "Batch loss: 0.028382128104567528\n",
      "Batch loss: 0.01674618571996689\n",
      "Batch loss: 0.04414428398013115\n",
      "Batch loss: 0.006780862342566252\n",
      "Batch loss: 0.023776253685355186\n",
      "Batch loss: 0.019442643970251083\n",
      "Batch loss: 0.01025977823883295\n",
      "Batch loss: 0.05713875964283943\n",
      "Batch loss: 0.03835529834032059\n",
      "Batch loss: 0.012499019503593445\n",
      "Batch loss: 0.021912796422839165\n",
      "Batch loss: 0.06802225857973099\n",
      "Batch loss: 0.007609224878251553\n",
      "Batch loss: 0.086155466735363\n",
      "Batch loss: 0.010007917881011963\n",
      "Batch loss: 0.05851621925830841\n",
      "Batch loss: 0.027296867221593857\n",
      "Batch loss: 0.014925197698175907\n",
      "Batch loss: 0.0511077344417572\n",
      "Batch loss: 0.022735675796866417\n",
      "Batch loss: 0.021149294450879097\n",
      "Batch loss: 0.027059774845838547\n",
      "Batch loss: 0.02695593796670437\n",
      "Batch loss: 0.016626616939902306\n",
      "Batch loss: 0.01865810714662075\n",
      "Batch loss: 0.008378446102142334\n",
      "Batch loss: 0.02541319839656353\n",
      "Batch loss: 0.011734845116734505\n",
      "Batch loss: 0.03143228590488434\n",
      "Batch loss: 0.01429114118218422\n",
      "Batch loss: 0.027965227141976357\n",
      "Batch loss: 0.07482369989156723\n",
      "Batch loss: 0.03270751237869263\n",
      "Batch loss: 0.009540162049233913\n",
      "Batch loss: 0.008659669198095798\n",
      "Batch loss: 0.05126164108514786\n",
      "Batch loss: 0.007346066180616617\n",
      "Batch loss: 0.021382223814725876\n",
      "Batch loss: 0.016643354669213295\n",
      "Batch loss: 0.024163521826267242\n",
      "Batch loss: 0.03259336203336716\n",
      "Batch loss: 0.006252370774745941\n",
      "Batch loss: 0.018442217260599136\n",
      "Batch loss: 0.01786991022527218\n",
      "Batch loss: 0.0022608251310884953\n",
      "Batch loss: 0.02045721374452114\n",
      "Batch loss: 0.04689249396324158\n",
      "Batch loss: 0.025922957807779312\n",
      "Batch loss: 0.07842055708169937\n",
      "Batch loss: 0.0658254474401474\n",
      "Batch loss: 0.011229409836232662\n",
      "Batch loss: 0.02463732287287712\n",
      "Batch loss: 0.04416613653302193\n",
      "Batch loss: 0.04173755273222923\n",
      "Batch loss: 0.020410703495144844\n",
      "Batch loss: 0.021945666521787643\n",
      "Batch loss: 0.0665259137749672\n",
      "Batch loss: 0.01615096814930439\n",
      "Batch loss: 0.005938338115811348\n",
      "Batch loss: 0.051341284066438675\n",
      "Batch loss: 0.05599808692932129\n",
      "Batch loss: 0.03299564868211746\n",
      "Batch loss: 0.03798255696892738\n",
      "Batch loss: 0.010928243398666382\n",
      "Batch loss: 0.03345438465476036\n",
      "Batch loss: 0.005418421234935522\n",
      "Batch loss: 0.05726829916238785\n",
      "Batch loss: 0.05909796804189682\n",
      "Batch loss: 0.030209410935640335\n",
      "Batch loss: 0.025216612964868546\n",
      "Batch loss: 0.026881521567702293\n",
      "Batch loss: 0.03445189818739891\n",
      "Batch loss: 0.002867180621251464\n",
      "Batch loss: 0.010016145184636116\n",
      "Batch loss: 0.025612451136112213\n",
      "Batch loss: 0.03464926406741142\n",
      "Batch loss: 0.02275499515235424\n",
      "Batch loss: 0.045123085379600525\n",
      "Batch loss: 0.041939638555049896\n",
      "Batch loss: 0.010476543568074703\n",
      "Batch loss: 0.08365271240472794\n",
      "Batch loss: 0.026738645508885384\n",
      "Batch loss: 0.036037832498550415\n",
      "Batch loss: 0.02935529313981533\n",
      "Batch loss: 0.04563363641500473\n",
      "Batch loss: 0.028579093515872955\n",
      "Batch loss: 0.005905518773943186\n",
      "Batch loss: 0.025651678442955017\n",
      "Batch loss: 0.01808195374906063\n",
      "Batch loss: 0.013849012553691864\n",
      "Batch loss: 0.04063894599676132\n",
      "Batch loss: 0.02239011786878109\n",
      "Batch loss: 0.00572285708039999\n",
      "Batch loss: 0.02197592705488205\n",
      "Batch loss: 0.03367322310805321\n",
      "Batch loss: 0.029487038031220436\n",
      "Batch loss: 0.022773750126361847\n",
      "Batch loss: 0.022509867325425148\n",
      "Batch loss: 0.02708219364285469\n",
      "Batch loss: 0.05813537910580635\n",
      "Batch loss: 0.018863627687096596\n",
      "Batch loss: 0.03318139165639877\n",
      "Batch loss: 0.07526160776615143\n",
      "Batch loss: 0.056729912757873535\n",
      "Batch loss: 0.008630264550447464\n",
      "Epoch [12/100], Loss: 0.0325\n",
      "Validation Loss: 0.4666, Accuracy: 91.10%\n",
      "Batch loss: 0.021519280970096588\n",
      "Batch loss: 0.01884031482040882\n",
      "Batch loss: 0.02804090455174446\n",
      "Batch loss: 0.08083899319171906\n",
      "Batch loss: 0.011951664462685585\n",
      "Batch loss: 0.019999317824840546\n",
      "Batch loss: 0.01750081777572632\n",
      "Batch loss: 0.008897362276911736\n",
      "Batch loss: 0.1322784423828125\n",
      "Batch loss: 0.032991599291563034\n",
      "Batch loss: 0.005217864643782377\n",
      "Batch loss: 0.05936235934495926\n",
      "Batch loss: 0.00466653797775507\n",
      "Batch loss: 0.029663626104593277\n",
      "Batch loss: 0.014694973826408386\n",
      "Batch loss: 0.05259743332862854\n",
      "Batch loss: 0.025954561308026314\n",
      "Batch loss: 0.047730278223752975\n",
      "Batch loss: 0.005995505955070257\n",
      "Batch loss: 0.03483607620000839\n",
      "Batch loss: 0.009390246123075485\n",
      "Batch loss: 0.11202926188707352\n",
      "Batch loss: 0.016245683655142784\n",
      "Batch loss: 0.09999675303697586\n",
      "Batch loss: 0.029640603810548782\n",
      "Batch loss: 0.003529768902808428\n",
      "Batch loss: 0.027036070823669434\n",
      "Batch loss: 0.06234949082136154\n",
      "Batch loss: 0.029304487630724907\n",
      "Batch loss: 0.06828443706035614\n",
      "Batch loss: 0.018311914056539536\n",
      "Batch loss: 0.012001587077975273\n",
      "Batch loss: 0.056186743080616\n",
      "Batch loss: 0.0297422893345356\n",
      "Batch loss: 0.00447203079238534\n",
      "Batch loss: 0.01712283305823803\n",
      "Batch loss: 0.001019954914227128\n",
      "Batch loss: 0.031702782958745956\n",
      "Batch loss: 0.04231603816151619\n",
      "Batch loss: 0.022176947444677353\n",
      "Batch loss: 0.07690738886594772\n",
      "Batch loss: 0.04256246238946915\n",
      "Batch loss: 0.04902743920683861\n",
      "Batch loss: 0.06801362335681915\n",
      "Batch loss: 0.07117685675621033\n",
      "Batch loss: 0.020354876294732094\n",
      "Batch loss: 0.054001349955797195\n",
      "Batch loss: 0.044640082865953445\n",
      "Batch loss: 0.008869758807122707\n",
      "Batch loss: 0.014046803116798401\n",
      "Batch loss: 0.10023319721221924\n",
      "Batch loss: 0.04007627069950104\n",
      "Batch loss: 0.023971782997250557\n",
      "Batch loss: 0.06772298365831375\n",
      "Batch loss: 0.03727225586771965\n",
      "Batch loss: 0.05259919911623001\n",
      "Batch loss: 0.029146138578653336\n",
      "Batch loss: 0.06584601104259491\n",
      "Batch loss: 0.010182534344494343\n",
      "Batch loss: 0.02908039279282093\n",
      "Batch loss: 0.018138453364372253\n",
      "Batch loss: 0.018624477088451385\n",
      "Batch loss: 0.022702975198626518\n",
      "Batch loss: 0.04114222526550293\n",
      "Batch loss: 0.020242158323526382\n",
      "Batch loss: 0.04003084823489189\n",
      "Batch loss: 0.027295682579278946\n",
      "Batch loss: 0.04110425338149071\n",
      "Batch loss: 0.03275070711970329\n",
      "Batch loss: 0.027983879670500755\n",
      "Batch loss: 0.05917869508266449\n",
      "Batch loss: 0.021207580342888832\n",
      "Batch loss: 0.03648531809449196\n",
      "Batch loss: 0.03169761598110199\n",
      "Batch loss: 0.008994251489639282\n",
      "Batch loss: 0.00917833112180233\n",
      "Batch loss: 0.03066413663327694\n",
      "Batch loss: 0.016971461474895477\n",
      "Batch loss: 0.05366899073123932\n",
      "Batch loss: 0.034591175615787506\n",
      "Batch loss: 0.025129538029432297\n",
      "Batch loss: 0.04039129987359047\n",
      "Batch loss: 0.016186099499464035\n",
      "Batch loss: 0.12807460129261017\n",
      "Batch loss: 0.08704885840415955\n",
      "Batch loss: 0.05571739375591278\n",
      "Batch loss: 0.010677178390324116\n",
      "Batch loss: 0.01391573715955019\n",
      "Batch loss: 0.01722085103392601\n",
      "Batch loss: 0.005836367141455412\n",
      "Batch loss: 0.019932832568883896\n",
      "Batch loss: 0.019052600488066673\n",
      "Batch loss: 0.0805383175611496\n",
      "Batch loss: 0.027947697788476944\n",
      "Batch loss: 0.034964755177497864\n",
      "Batch loss: 0.012052204459905624\n",
      "Batch loss: 0.03583937883377075\n",
      "Batch loss: 0.06064005568623543\n",
      "Batch loss: 0.03246508538722992\n",
      "Batch loss: 0.023719029501080513\n",
      "Batch loss: 0.01760520040988922\n",
      "Batch loss: 0.024836689233779907\n",
      "Batch loss: 0.017740629613399506\n",
      "Batch loss: 0.0087363850325346\n",
      "Batch loss: 0.013548176735639572\n",
      "Batch loss: 0.023967862129211426\n",
      "Batch loss: 0.03505272418260574\n",
      "Batch loss: 0.015900367870926857\n",
      "Batch loss: 0.009051195345818996\n",
      "Batch loss: 0.004366288427263498\n",
      "Batch loss: 0.017457906156778336\n",
      "Batch loss: 0.017208324745297432\n",
      "Batch loss: 0.04300534725189209\n",
      "Batch loss: 0.01979774609208107\n",
      "Batch loss: 0.02017437480390072\n",
      "Batch loss: 0.021310167387127876\n",
      "Batch loss: 0.046697504818439484\n",
      "Batch loss: 0.02549104019999504\n",
      "Batch loss: 0.021614598110318184\n",
      "Batch loss: 0.039842866361141205\n",
      "Batch loss: 0.02010555937886238\n",
      "Batch loss: 0.046529754996299744\n",
      "Batch loss: 0.016596494242548943\n",
      "Batch loss: 0.005249679554253817\n",
      "Batch loss: 0.030925745144486427\n",
      "Batch loss: 0.08312489092350006\n",
      "Batch loss: 0.005229969508945942\n",
      "Batch loss: 0.014700083062052727\n",
      "Batch loss: 0.017174938693642616\n",
      "Batch loss: 0.03644614666700363\n",
      "Batch loss: 0.038774218410253525\n",
      "Batch loss: 0.031063832342624664\n",
      "Batch loss: 0.044659972190856934\n",
      "Batch loss: 0.03964107483625412\n",
      "Batch loss: 0.040059443563222885\n",
      "Batch loss: 0.003816094482317567\n",
      "Batch loss: 0.019568780437111855\n",
      "Batch loss: 0.02015804685652256\n",
      "Batch loss: 0.005977398715913296\n",
      "Batch loss: 0.004930684808641672\n",
      "Batch loss: 0.03701454773545265\n",
      "Batch loss: 0.012201442383229733\n",
      "Batch loss: 0.026152893900871277\n",
      "Batch loss: 0.06068786233663559\n",
      "Batch loss: 0.00824754312634468\n",
      "Batch loss: 0.014009930193424225\n",
      "Batch loss: 0.02267754077911377\n",
      "Batch loss: 0.012362604029476643\n",
      "Batch loss: 0.07391038537025452\n",
      "Batch loss: 0.037158090621232986\n",
      "Batch loss: 0.07447685301303864\n",
      "Batch loss: 0.023371899500489235\n",
      "Batch loss: 0.030399959534406662\n",
      "Batch loss: 0.010635240003466606\n",
      "Batch loss: 0.014133927412331104\n",
      "Batch loss: 0.007716949563473463\n",
      "Batch loss: 0.08120293915271759\n",
      "Batch loss: 0.022016435861587524\n",
      "Batch loss: 0.011014490388333797\n",
      "Batch loss: 0.020025551319122314\n",
      "Batch loss: 0.008948026224970818\n",
      "Batch loss: 0.012428343296051025\n",
      "Batch loss: 0.020711757242679596\n",
      "Batch loss: 0.054077617824077606\n",
      "Batch loss: 0.029125120490789413\n",
      "Batch loss: 0.034639954566955566\n",
      "Batch loss: 0.002376230200752616\n",
      "Batch loss: 0.03400033339858055\n",
      "Batch loss: 0.01745200715959072\n",
      "Batch loss: 0.05429886281490326\n",
      "Batch loss: 0.030527429655194283\n",
      "Batch loss: 0.01877976953983307\n",
      "Batch loss: 0.020062342286109924\n",
      "Batch loss: 0.018481018021702766\n",
      "Batch loss: 0.015046711079776287\n",
      "Batch loss: 0.06406939774751663\n",
      "Batch loss: 0.02364179491996765\n",
      "Batch loss: 0.0015663274098187685\n",
      "Batch loss: 0.022510625422000885\n",
      "Batch loss: 0.010468034073710442\n",
      "Batch loss: 0.02632160112261772\n",
      "Batch loss: 0.015417138114571571\n",
      "Batch loss: 0.01839532144367695\n",
      "Batch loss: 0.026778774335980415\n",
      "Batch loss: 0.03296652436256409\n",
      "Batch loss: 0.02206282876431942\n",
      "Batch loss: 0.04127196967601776\n",
      "Batch loss: 0.011413868516683578\n",
      "Batch loss: 0.0533946193754673\n",
      "Batch loss: 0.01359714288264513\n",
      "Batch loss: 0.039753351360559464\n",
      "Batch loss: 0.08068428188562393\n",
      "Batch loss: 0.054756049066782\n",
      "Batch loss: 0.04804079979658127\n",
      "Batch loss: 0.01124885305762291\n",
      "Batch loss: 0.007358837872743607\n",
      "Batch loss: 0.01708250679075718\n",
      "Batch loss: 0.020091189071536064\n",
      "Batch loss: 0.022424383088946342\n",
      "Batch loss: 0.0284554623067379\n",
      "Batch loss: 0.008793961256742477\n",
      "Batch loss: 0.03062323108315468\n",
      "Batch loss: 0.05047310143709183\n",
      "Batch loss: 0.008884907700121403\n",
      "Batch loss: 0.02009933814406395\n",
      "Batch loss: 0.019713301211595535\n",
      "Batch loss: 0.060313671827316284\n",
      "Batch loss: 0.00740832882001996\n",
      "Batch loss: 0.01153081189841032\n",
      "Batch loss: 0.041715286672115326\n",
      "Batch loss: 0.019984934478998184\n",
      "Batch loss: 0.05295657739043236\n",
      "Batch loss: 0.007250717841088772\n",
      "Batch loss: 0.05021598935127258\n",
      "Batch loss: 0.0204133540391922\n",
      "Batch loss: 0.01467432826757431\n",
      "Batch loss: 0.010457992553710938\n",
      "Batch loss: 0.005909140687435865\n",
      "Batch loss: 0.006605250295251608\n",
      "Batch loss: 0.03858266770839691\n",
      "Batch loss: 0.01956380344927311\n",
      "Batch loss: 0.040782053023576736\n",
      "Batch loss: 0.026789233088493347\n",
      "Batch loss: 0.015268806368112564\n",
      "Batch loss: 0.017169412225484848\n",
      "Batch loss: 0.022403839975595474\n",
      "Batch loss: 0.024865565821528435\n",
      "Batch loss: 0.033329110592603683\n",
      "Batch loss: 0.0820840448141098\n",
      "Batch loss: 0.01205360610038042\n",
      "Batch loss: 0.010575675405561924\n",
      "Batch loss: 0.03162350878119469\n",
      "Batch loss: 0.026964198797941208\n",
      "Batch loss: 0.00827983021736145\n",
      "Batch loss: 0.005890817381441593\n",
      "Batch loss: 0.05374538525938988\n",
      "Batch loss: 0.021992027759552002\n",
      "Batch loss: 0.08410166203975677\n",
      "Batch loss: 0.004923239815980196\n",
      "Batch loss: 0.01771094650030136\n",
      "Batch loss: 0.03190695494413376\n",
      "Batch loss: 0.014457214623689651\n",
      "Batch loss: 0.008902939036488533\n",
      "Batch loss: 0.06891077756881714\n",
      "Batch loss: 0.009047462604939938\n",
      "Batch loss: 0.036497630178928375\n",
      "Batch loss: 0.03786582127213478\n",
      "Batch loss: 0.027603695169091225\n",
      "Batch loss: 0.013567397370934486\n",
      "Batch loss: 0.07452695816755295\n",
      "Batch loss: 0.009849848225712776\n",
      "Batch loss: 0.06263929605484009\n",
      "Batch loss: 0.016100339591503143\n",
      "Batch loss: 0.020168105140328407\n",
      "Batch loss: 0.027688253670930862\n",
      "Batch loss: 0.043977607041597366\n",
      "Batch loss: 0.081196129322052\n",
      "Batch loss: 0.09607591480016708\n",
      "Batch loss: 0.01537284441292286\n",
      "Batch loss: 0.08173780143260956\n",
      "Batch loss: 0.009429830126464367\n",
      "Batch loss: 0.016742147505283356\n",
      "Batch loss: 0.020939961075782776\n",
      "Batch loss: 0.026791134849190712\n",
      "Batch loss: 0.09601039439439774\n",
      "Batch loss: 0.009754044935107231\n",
      "Batch loss: 0.02887727878987789\n",
      "Batch loss: 0.051520586013793945\n",
      "Batch loss: 0.01837335154414177\n",
      "Batch loss: 0.03978618606925011\n",
      "Batch loss: 0.07755449414253235\n",
      "Batch loss: 0.010934635996818542\n",
      "Batch loss: 0.024688268080353737\n",
      "Batch loss: 0.01115661021322012\n",
      "Batch loss: 0.025850476697087288\n",
      "Batch loss: 0.007436686661094427\n",
      "Batch loss: 0.03968070074915886\n",
      "Batch loss: 0.011411013081669807\n",
      "Batch loss: 0.02349764108657837\n",
      "Batch loss: 0.04665796086192131\n",
      "Batch loss: 0.028335366398096085\n",
      "Batch loss: 0.0239644106477499\n",
      "Batch loss: 0.01174888014793396\n",
      "Batch loss: 0.05223538354039192\n",
      "Batch loss: 0.020938538014888763\n",
      "Batch loss: 0.03654586896300316\n",
      "Batch loss: 0.01466280035674572\n",
      "Batch loss: 0.08608299493789673\n",
      "Batch loss: 0.030316054821014404\n",
      "Batch loss: 0.004451451823115349\n",
      "Batch loss: 0.017040470615029335\n",
      "Batch loss: 0.0199431199580431\n",
      "Batch loss: 0.01792796142399311\n",
      "Batch loss: 0.019048236310482025\n",
      "Batch loss: 0.02906634472310543\n",
      "Batch loss: 0.11219200491905212\n",
      "Batch loss: 0.007214539218693972\n",
      "Batch loss: 0.05616144463419914\n",
      "Batch loss: 0.05164243280887604\n",
      "Batch loss: 0.03567386046051979\n",
      "Batch loss: 0.029904726892709732\n",
      "Batch loss: 0.009264860302209854\n",
      "Batch loss: 0.06711024045944214\n",
      "Batch loss: 0.012821769341826439\n",
      "Batch loss: 0.006499642040580511\n",
      "Batch loss: 0.012938475236296654\n",
      "Batch loss: 0.005800547543913126\n",
      "Batch loss: 0.015124166384339333\n",
      "Batch loss: 0.012285462580621243\n",
      "Batch loss: 0.01574583724141121\n",
      "Batch loss: 0.03284895420074463\n",
      "Batch loss: 0.014305244199931622\n",
      "Batch loss: 0.03451010584831238\n",
      "Batch loss: 0.02473345585167408\n",
      "Batch loss: 0.03553509712219238\n",
      "Batch loss: 0.05325961485505104\n",
      "Batch loss: 0.009744317270815372\n",
      "Batch loss: 0.025958700105547905\n",
      "Batch loss: 0.024906324222683907\n",
      "Batch loss: 0.02636580914258957\n",
      "Batch loss: 0.016417810693383217\n",
      "Batch loss: 0.014349834062159061\n",
      "Batch loss: 0.006488727871328592\n",
      "Batch loss: 0.045243266969919205\n",
      "Batch loss: 0.021029174327850342\n",
      "Batch loss: 0.026036012917757034\n",
      "Batch loss: 0.042657263576984406\n",
      "Batch loss: 0.02185811661183834\n",
      "Batch loss: 0.01210688054561615\n",
      "Batch loss: 0.008748911321163177\n",
      "Batch loss: 0.01620904542505741\n",
      "Batch loss: 0.010990929789841175\n",
      "Batch loss: 0.01669558696448803\n",
      "Batch loss: 0.059110403060913086\n",
      "Batch loss: 0.03217661380767822\n",
      "Batch loss: 0.04362301155924797\n",
      "Batch loss: 0.010388102382421494\n",
      "Batch loss: 0.010804081335663795\n",
      "Batch loss: 0.04709517955780029\n",
      "Batch loss: 0.024376019835472107\n",
      "Batch loss: 0.05383242666721344\n",
      "Batch loss: 0.021457673981785774\n",
      "Batch loss: 0.02378939464688301\n",
      "Batch loss: 0.012955183163285255\n",
      "Batch loss: 0.012483401224017143\n",
      "Batch loss: 0.01592917926609516\n",
      "Batch loss: 0.028278402984142303\n",
      "Batch loss: 0.028718383982777596\n",
      "Batch loss: 0.023068660870194435\n",
      "Batch loss: 0.0024703473318368196\n",
      "Batch loss: 0.008708387613296509\n",
      "Batch loss: 0.008958129212260246\n",
      "Batch loss: 0.020071350038051605\n",
      "Batch loss: 0.02498578280210495\n",
      "Batch loss: 0.028517022728919983\n",
      "Batch loss: 0.008120870217680931\n",
      "Batch loss: 0.028323758393526077\n",
      "Batch loss: 0.008693451061844826\n",
      "Batch loss: 0.025824280455708504\n",
      "Batch loss: 0.07238615304231644\n",
      "Batch loss: 0.025067757815122604\n",
      "Batch loss: 0.07437484711408615\n",
      "Batch loss: 0.0634431317448616\n",
      "Batch loss: 0.05790381133556366\n",
      "Batch loss: 0.021829940378665924\n",
      "Batch loss: 0.007116095162928104\n",
      "Batch loss: 0.03318934142589569\n",
      "Batch loss: 0.043974619358778\n",
      "Batch loss: 0.018077099695801735\n",
      "Batch loss: 0.036096155643463135\n",
      "Batch loss: 0.026831911876797676\n",
      "Batch loss: 0.0057060811668634415\n",
      "Batch loss: 0.012829132378101349\n",
      "Batch loss: 0.003921571187674999\n",
      "Batch loss: 0.025424722582101822\n",
      "Batch loss: 0.0335785336792469\n",
      "Batch loss: 0.022962503135204315\n",
      "Batch loss: 0.04931790754199028\n",
      "Batch loss: 0.05732308328151703\n",
      "Batch loss: 0.040561046451330185\n",
      "Batch loss: 0.027775004506111145\n",
      "Batch loss: 0.026521604508161545\n",
      "Batch loss: 0.03594562038779259\n",
      "Batch loss: 0.018643410876393318\n",
      "Batch loss: 0.031655050814151764\n",
      "Batch loss: 0.021105322986841202\n",
      "Batch loss: 0.048487428575754166\n",
      "Batch loss: 0.04885965213179588\n",
      "Batch loss: 0.01763724535703659\n",
      "Batch loss: 0.007595226634293795\n",
      "Batch loss: 0.01677570305764675\n",
      "Batch loss: 0.006655388977378607\n",
      "Batch loss: 0.013202086091041565\n",
      "Batch loss: 0.025127286091446877\n",
      "Batch loss: 0.02343890443444252\n",
      "Batch loss: 0.07485757023096085\n",
      "Batch loss: 0.0430653840303421\n",
      "Batch loss: 0.005643153563141823\n",
      "Batch loss: 0.014050411991775036\n",
      "Batch loss: 0.000896758574526757\n",
      "Batch loss: 0.010204658843576908\n",
      "Batch loss: 0.009694622829556465\n",
      "Batch loss: 0.010977940633893013\n",
      "Batch loss: 0.01722593419253826\n",
      "Batch loss: 0.004566558636724949\n",
      "Batch loss: 0.03284645453095436\n",
      "Batch loss: 0.03643770143389702\n",
      "Batch loss: 0.017234424129128456\n",
      "Batch loss: 0.032747700810432434\n",
      "Batch loss: 0.02265569195151329\n",
      "Batch loss: 0.006451128050684929\n",
      "Batch loss: 0.017192985862493515\n",
      "Batch loss: 0.011030850000679493\n",
      "Batch loss: 0.03346986696124077\n",
      "Batch loss: 0.008284821175038815\n",
      "Batch loss: 0.050566378980875015\n",
      "Batch loss: 0.03529110923409462\n",
      "Epoch [13/100], Loss: 0.0294\n",
      "Validation Loss: 0.7758, Accuracy: 90.52%\n",
      "Batch loss: 0.02740384452044964\n",
      "Batch loss: 0.010819255374372005\n",
      "Batch loss: 0.03516792133450508\n",
      "Batch loss: 0.012316988781094551\n",
      "Batch loss: 0.04869541898369789\n",
      "Batch loss: 0.012075133621692657\n",
      "Batch loss: 0.015569592826068401\n",
      "Batch loss: 0.004204113036394119\n",
      "Batch loss: 0.06308360397815704\n",
      "Batch loss: 0.007746563758701086\n",
      "Batch loss: 0.034140367060899734\n",
      "Batch loss: 0.012100095860660076\n",
      "Batch loss: 0.019248338416218758\n",
      "Batch loss: 0.01322290301322937\n",
      "Batch loss: 0.01720503345131874\n",
      "Batch loss: 0.05461769923567772\n",
      "Batch loss: 0.04204972833395004\n",
      "Batch loss: 0.018472502008080482\n",
      "Batch loss: 0.006287227384746075\n",
      "Batch loss: 0.014975828118622303\n",
      "Batch loss: 0.005363202188163996\n",
      "Batch loss: 0.06107500195503235\n",
      "Batch loss: 0.03483569249510765\n",
      "Batch loss: 0.015274018049240112\n",
      "Batch loss: 0.0023244651965796947\n",
      "Batch loss: 0.002996385795995593\n",
      "Batch loss: 0.030582964420318604\n",
      "Batch loss: 0.04425919055938721\n",
      "Batch loss: 0.02137609012424946\n",
      "Batch loss: 0.06001393496990204\n",
      "Batch loss: 0.045410819351673126\n",
      "Batch loss: 0.050109248608350754\n",
      "Batch loss: 0.03635687381029129\n",
      "Batch loss: 0.009953469038009644\n",
      "Batch loss: 0.012953970581293106\n",
      "Batch loss: 0.05563654750585556\n",
      "Batch loss: 0.0009270831360481679\n",
      "Batch loss: 0.011791021563112736\n",
      "Batch loss: 0.04855811223387718\n",
      "Batch loss: 0.02012319304049015\n",
      "Batch loss: 0.039915457367897034\n",
      "Batch loss: 0.03426132723689079\n",
      "Batch loss: 0.023266565054655075\n",
      "Batch loss: 0.034448035061359406\n",
      "Batch loss: 0.0461733303964138\n",
      "Batch loss: 0.02996029332280159\n",
      "Batch loss: 0.03238110989332199\n",
      "Batch loss: 0.05974821746349335\n",
      "Batch loss: 0.03150520101189613\n",
      "Batch loss: 0.04557982459664345\n",
      "Batch loss: 0.01746394857764244\n",
      "Batch loss: 0.009239655919373035\n",
      "Batch loss: 0.009996882639825344\n",
      "Batch loss: 0.009121515788137913\n",
      "Batch loss: 0.0024515234399586916\n",
      "Batch loss: 0.023122303187847137\n",
      "Batch loss: 0.007863293401896954\n",
      "Batch loss: 0.020899225026369095\n",
      "Batch loss: 0.009835315868258476\n",
      "Batch loss: 0.045466382056474686\n",
      "Batch loss: 0.012439579702913761\n",
      "Batch loss: 0.014424072578549385\n",
      "Batch loss: 0.03419684246182442\n",
      "Batch loss: 0.01797492988407612\n",
      "Batch loss: 0.011459553614258766\n",
      "Batch loss: 0.03846798464655876\n",
      "Batch loss: 0.047726694494485855\n",
      "Batch loss: 0.03243391215801239\n",
      "Batch loss: 0.05394826829433441\n",
      "Batch loss: 0.016644654795527458\n",
      "Batch loss: 0.010735301300883293\n",
      "Batch loss: 0.007844742387533188\n",
      "Batch loss: 0.000525228213518858\n",
      "Batch loss: 0.03219881281256676\n",
      "Batch loss: 0.006173805333673954\n",
      "Batch loss: 0.03561027720570564\n",
      "Batch loss: 0.07368765771389008\n",
      "Batch loss: 0.030883725732564926\n",
      "Batch loss: 0.03389478474855423\n",
      "Batch loss: 0.04294879361987114\n",
      "Batch loss: 0.004665283020585775\n",
      "Batch loss: 0.05796403810381889\n",
      "Batch loss: 0.01213801372796297\n",
      "Batch loss: 0.11406990885734558\n",
      "Batch loss: 0.033064283430576324\n",
      "Batch loss: 0.04530644416809082\n",
      "Batch loss: 0.023881852626800537\n",
      "Batch loss: 0.05278337001800537\n",
      "Batch loss: 0.010822517797350883\n",
      "Batch loss: 0.005233684554696083\n",
      "Batch loss: 0.017034610733389854\n",
      "Batch loss: 0.013715209439396858\n",
      "Batch loss: 0.00835026241838932\n",
      "Batch loss: 0.018520811572670937\n",
      "Batch loss: 0.0071221282705664635\n",
      "Batch loss: 0.00269068730995059\n",
      "Batch loss: 0.006972549017518759\n",
      "Batch loss: 0.02600395865738392\n",
      "Batch loss: 0.1065908670425415\n",
      "Batch loss: 0.03434121981263161\n",
      "Batch loss: 0.017727483063936234\n",
      "Batch loss: 0.01242670975625515\n",
      "Batch loss: 0.024542223662137985\n",
      "Batch loss: 0.022905079647898674\n",
      "Batch loss: 0.045207321643829346\n",
      "Batch loss: 0.0062124477699398994\n",
      "Batch loss: 0.054496124386787415\n",
      "Batch loss: 0.038737595081329346\n",
      "Batch loss: 0.018640803173184395\n",
      "Batch loss: 0.018875323235988617\n",
      "Batch loss: 0.0422004871070385\n",
      "Batch loss: 0.024980036541819572\n",
      "Batch loss: 0.007471195422112942\n",
      "Batch loss: 0.0058674574829638\n",
      "Batch loss: 0.015270926989614964\n",
      "Batch loss: 0.03059672936797142\n",
      "Batch loss: 0.025516441091895103\n",
      "Batch loss: 0.009144994430243969\n",
      "Batch loss: 0.010120744816958904\n",
      "Batch loss: 0.0255451537668705\n",
      "Batch loss: 0.003970548510551453\n",
      "Batch loss: 0.016695549711585045\n",
      "Batch loss: 0.086573027074337\n",
      "Batch loss: 0.014881635084748268\n",
      "Batch loss: 0.04409758374094963\n",
      "Batch loss: 0.06942278891801834\n",
      "Batch loss: 0.0473957285284996\n",
      "Batch loss: 0.02415543608367443\n",
      "Batch loss: 0.005533108487725258\n",
      "Batch loss: 0.022511163726449013\n",
      "Batch loss: 0.016942692920565605\n",
      "Batch loss: 0.01976007968187332\n",
      "Batch loss: 0.05050184205174446\n",
      "Batch loss: 0.017950007691979408\n",
      "Batch loss: 0.026619140058755875\n",
      "Batch loss: 0.0017361600184813142\n",
      "Batch loss: 0.0017463284311816096\n",
      "Batch loss: 0.03061550110578537\n",
      "Batch loss: 0.04089770466089249\n",
      "Batch loss: 0.07282888889312744\n",
      "Batch loss: 0.003830270143225789\n",
      "Batch loss: 0.00982134509831667\n",
      "Batch loss: 0.015219979919493198\n",
      "Batch loss: 0.0454714260995388\n",
      "Batch loss: 0.012374281883239746\n",
      "Batch loss: 0.006802359130233526\n",
      "Batch loss: 0.03592396154999733\n",
      "Batch loss: 0.02778107300400734\n",
      "Batch loss: 0.051715459674596786\n",
      "Batch loss: 0.0034693768247962\n",
      "Batch loss: 0.025831837207078934\n",
      "Batch loss: 0.007453688886016607\n",
      "Batch loss: 0.025971783325076103\n",
      "Batch loss: 0.01385826338082552\n",
      "Batch loss: 0.039943404495716095\n",
      "Batch loss: 0.011398525908589363\n",
      "Batch loss: 0.016419751569628716\n",
      "Batch loss: 0.014531097374856472\n",
      "Batch loss: 0.015700221061706543\n",
      "Batch loss: 0.056195005774497986\n",
      "Batch loss: 0.004315309692174196\n",
      "Batch loss: 0.02727428264915943\n",
      "Batch loss: 0.022307787090539932\n",
      "Batch loss: 0.022091850638389587\n",
      "Batch loss: 0.007224850356578827\n",
      "Batch loss: 0.021619705483317375\n",
      "Batch loss: 0.0025383990723639727\n",
      "Batch loss: 0.013421617448329926\n",
      "Batch loss: 0.013275706209242344\n",
      "Batch loss: 0.04647507518529892\n",
      "Batch loss: 0.020094890147447586\n",
      "Batch loss: 0.053592219948768616\n",
      "Batch loss: 0.01567348651587963\n",
      "Batch loss: 0.06408568471670151\n",
      "Batch loss: 0.016059795394539833\n",
      "Batch loss: 0.009027228690683842\n",
      "Batch loss: 0.030757449567317963\n",
      "Batch loss: 0.006565944757312536\n",
      "Batch loss: 0.01845601573586464\n",
      "Batch loss: 0.014392497949302197\n",
      "Batch loss: 0.05008694529533386\n",
      "Batch loss: 0.014755930751562119\n",
      "Batch loss: 0.01735835149884224\n",
      "Batch loss: 0.05843621492385864\n",
      "Batch loss: 0.01698947697877884\n",
      "Batch loss: 0.03496097773313522\n",
      "Batch loss: 0.003385725896805525\n",
      "Batch loss: 0.02827337570488453\n",
      "Batch loss: 0.02443055249750614\n",
      "Batch loss: 0.012662488035857677\n",
      "Batch loss: 0.0023369533009827137\n",
      "Batch loss: 0.020828479900956154\n",
      "Batch loss: 0.04714169353246689\n",
      "Batch loss: 0.0054476214572787285\n",
      "Batch loss: 0.04103902727365494\n",
      "Batch loss: 0.017231348901987076\n",
      "Batch loss: 0.017705079168081284\n",
      "Batch loss: 0.059480153024196625\n",
      "Batch loss: 0.017464254051446915\n",
      "Batch loss: 0.04305815324187279\n",
      "Batch loss: 0.004650707822293043\n",
      "Batch loss: 0.020349562168121338\n",
      "Batch loss: 0.0033785130362957716\n",
      "Batch loss: 0.008008637465536594\n",
      "Batch loss: 0.012627680785953999\n",
      "Batch loss: 0.04176805168390274\n",
      "Batch loss: 0.02054017409682274\n",
      "Batch loss: 0.009692955762147903\n",
      "Batch loss: 0.003085862612351775\n",
      "Batch loss: 0.09907947480678558\n",
      "Batch loss: 0.018974928185343742\n",
      "Batch loss: 0.05396229773759842\n",
      "Batch loss: 0.00844294112175703\n",
      "Batch loss: 0.028634414076805115\n",
      "Batch loss: 0.044013142585754395\n",
      "Batch loss: 0.007682842202484608\n",
      "Batch loss: 0.04503168538212776\n",
      "Batch loss: 0.006497314665466547\n",
      "Batch loss: 0.005739165935665369\n",
      "Batch loss: 0.012837890535593033\n",
      "Batch loss: 0.025748319923877716\n",
      "Batch loss: 0.005971046630293131\n",
      "Batch loss: 0.004136599600315094\n",
      "Batch loss: 0.05122314393520355\n",
      "Batch loss: 0.005419556517153978\n",
      "Batch loss: 0.012922809459269047\n",
      "Batch loss: 0.010657183825969696\n",
      "Batch loss: 0.06623347848653793\n",
      "Batch loss: 0.01643400453031063\n",
      "Batch loss: 0.05107180029153824\n",
      "Batch loss: 0.031192516908049583\n",
      "Batch loss: 0.011539796367287636\n",
      "Batch loss: 0.04013875871896744\n",
      "Batch loss: 0.014766466803848743\n",
      "Batch loss: 0.003263474442064762\n",
      "Batch loss: 0.008309160359203815\n",
      "Batch loss: 0.07047076523303986\n",
      "Batch loss: 0.05874849483370781\n",
      "Batch loss: 0.0030674657318741083\n",
      "Batch loss: 0.012022641487419605\n",
      "Batch loss: 0.05421588942408562\n",
      "Batch loss: 0.030662711709737778\n",
      "Batch loss: 0.028582870960235596\n",
      "Batch loss: 0.013006982393562794\n",
      "Batch loss: 0.01801684871315956\n",
      "Batch loss: 0.02036658488214016\n",
      "Batch loss: 0.02625524438917637\n",
      "Batch loss: 0.010223940014839172\n",
      "Batch loss: 0.03141915425658226\n",
      "Batch loss: 0.007374575827270746\n",
      "Batch loss: 0.06750430911779404\n",
      "Batch loss: 0.04552709683775902\n",
      "Batch loss: 0.04752340167760849\n",
      "Batch loss: 0.02889612875878811\n",
      "Batch loss: 0.03033396229147911\n",
      "Batch loss: 0.04855160415172577\n",
      "Batch loss: 0.04476296901702881\n",
      "Batch loss: 0.04478354752063751\n",
      "Batch loss: 0.013167345896363258\n",
      "Batch loss: 0.021686645224690437\n",
      "Batch loss: 0.01031730230897665\n",
      "Batch loss: 0.033780619502067566\n",
      "Batch loss: 0.021466344594955444\n",
      "Batch loss: 0.03504972532391548\n",
      "Batch loss: 0.026922646909952164\n",
      "Batch loss: 0.0023124616127461195\n",
      "Batch loss: 0.07469519227743149\n",
      "Batch loss: 0.03887230157852173\n",
      "Batch loss: 0.025761403143405914\n",
      "Batch loss: 0.018068773671984673\n",
      "Batch loss: 0.05309637263417244\n",
      "Batch loss: 0.030677005648612976\n",
      "Batch loss: 0.03824419900774956\n",
      "Batch loss: 0.007206677924841642\n",
      "Batch loss: 0.013890150003135204\n",
      "Batch loss: 0.04098959639668465\n",
      "Batch loss: 0.014056107960641384\n",
      "Batch loss: 0.03174882382154465\n",
      "Batch loss: 0.01416383683681488\n",
      "Batch loss: 0.03768718242645264\n",
      "Batch loss: 0.029611296951770782\n",
      "Batch loss: 0.02720830775797367\n",
      "Batch loss: 0.04689469560980797\n",
      "Batch loss: 0.06687375158071518\n",
      "Batch loss: 0.04260914400219917\n",
      "Batch loss: 0.017679953947663307\n",
      "Batch loss: 0.014165001921355724\n",
      "Batch loss: 0.010309025645256042\n",
      "Batch loss: 0.006126664578914642\n",
      "Batch loss: 0.00946081057190895\n",
      "Batch loss: 0.010371172800660133\n",
      "Batch loss: 0.054995447397232056\n",
      "Batch loss: 0.01537738461047411\n",
      "Batch loss: 0.006081957370042801\n",
      "Batch loss: 0.007457297295331955\n",
      "Batch loss: 0.015476051717996597\n",
      "Batch loss: 0.03706274926662445\n",
      "Batch loss: 0.04649357497692108\n",
      "Batch loss: 0.06211904063820839\n",
      "Batch loss: 0.031494125723838806\n",
      "Batch loss: 0.014596992172300816\n",
      "Batch loss: 0.007142946124076843\n",
      "Batch loss: 0.036295440047979355\n",
      "Batch loss: 0.01001083105802536\n",
      "Batch loss: 0.05973999202251434\n",
      "Batch loss: 0.020453952252864838\n",
      "Batch loss: 0.022229542955756187\n",
      "Batch loss: 0.039170004427433014\n",
      "Batch loss: 0.04332546889781952\n",
      "Batch loss: 0.013885087333619595\n",
      "Batch loss: 0.014849396422505379\n",
      "Batch loss: 0.019608963280916214\n",
      "Batch loss: 0.016577893868088722\n",
      "Batch loss: 0.01356210932135582\n",
      "Batch loss: 0.0031557297334074974\n",
      "Batch loss: 0.05796818062663078\n",
      "Batch loss: 0.025657759979367256\n",
      "Batch loss: 0.01854335144162178\n",
      "Batch loss: 0.013017803430557251\n",
      "Batch loss: 0.06674391776323318\n",
      "Batch loss: 0.031793367117643356\n",
      "Batch loss: 0.05244163051247597\n",
      "Batch loss: 0.020403536036610603\n",
      "Batch loss: 0.08501498401165009\n",
      "Batch loss: 0.003659462323412299\n",
      "Batch loss: 0.022888321429491043\n",
      "Batch loss: 0.014914575964212418\n",
      "Batch loss: 0.08504875004291534\n",
      "Batch loss: 0.02149960771203041\n",
      "Batch loss: 0.04970068112015724\n",
      "Batch loss: 0.023020850494503975\n",
      "Batch loss: 0.019604166969656944\n",
      "Batch loss: 0.028232498094439507\n",
      "Batch loss: 0.03182931989431381\n",
      "Batch loss: 0.02704070322215557\n",
      "Batch loss: 0.018164880573749542\n",
      "Batch loss: 0.0006815562956035137\n",
      "Batch loss: 0.014363647438585758\n",
      "Batch loss: 0.04842997342348099\n",
      "Batch loss: 0.01484091766178608\n",
      "Batch loss: 0.08279134333133698\n",
      "Batch loss: 0.004468933679163456\n",
      "Batch loss: 0.020876988768577576\n",
      "Batch loss: 0.014857235364615917\n",
      "Batch loss: 0.0502098947763443\n",
      "Batch loss: 0.057212695479393005\n",
      "Batch loss: 0.07690597325563431\n",
      "Batch loss: 0.03974020481109619\n",
      "Batch loss: 0.022939488291740417\n",
      "Batch loss: 0.00825528334826231\n",
      "Batch loss: 0.02767629362642765\n",
      "Batch loss: 0.006090967915952206\n",
      "Batch loss: 0.026608210057020187\n",
      "Batch loss: 0.027639299631118774\n",
      "Batch loss: 0.018886957317590714\n",
      "Batch loss: 0.04605501517653465\n",
      "Batch loss: 0.02599797025322914\n",
      "Batch loss: 0.0333193875849247\n",
      "Batch loss: 0.01915680803358555\n",
      "Batch loss: 0.07040385156869888\n",
      "Batch loss: 0.027098968625068665\n",
      "Batch loss: 0.0704432800412178\n",
      "Batch loss: 0.013836530968546867\n",
      "Batch loss: 0.022469837218523026\n",
      "Batch loss: 0.02615746483206749\n",
      "Batch loss: 0.018966058269143105\n",
      "Batch loss: 0.029881445690989494\n",
      "Batch loss: 0.022230470553040504\n",
      "Batch loss: 0.028464417904615402\n",
      "Batch loss: 0.0688701942563057\n",
      "Batch loss: 0.01911994442343712\n",
      "Batch loss: 0.00627834303304553\n",
      "Batch loss: 0.026110230013728142\n",
      "Batch loss: 0.05140438303351402\n",
      "Batch loss: 0.07279419898986816\n",
      "Batch loss: 0.013059337623417377\n",
      "Batch loss: 0.02772611565887928\n",
      "Batch loss: 0.0036136116832494736\n",
      "Batch loss: 0.02914787456393242\n",
      "Batch loss: 0.010025515221059322\n",
      "Batch loss: 0.074134461581707\n",
      "Batch loss: 0.040549326688051224\n",
      "Batch loss: 0.03111187554895878\n",
      "Batch loss: 0.06658973544836044\n",
      "Batch loss: 0.005089329089969397\n",
      "Batch loss: 0.00700095109641552\n",
      "Batch loss: 0.024837268516421318\n",
      "Batch loss: 0.020117120817303658\n",
      "Batch loss: 0.017344389110803604\n",
      "Batch loss: 0.00992337241768837\n",
      "Batch loss: 0.009661265648901463\n",
      "Batch loss: 0.04665238782763481\n",
      "Batch loss: 0.02598988451063633\n",
      "Batch loss: 0.01367841474711895\n",
      "Batch loss: 0.03400154039263725\n",
      "Batch loss: 0.0413033589720726\n",
      "Batch loss: 0.030880063772201538\n",
      "Batch loss: 0.02442934736609459\n",
      "Batch loss: 0.019562048837542534\n",
      "Batch loss: 0.007882343605160713\n",
      "Batch loss: 0.019778527319431305\n",
      "Batch loss: 0.00398172065615654\n",
      "Batch loss: 0.008108782581984997\n",
      "Batch loss: 0.017489628866314888\n",
      "Batch loss: 0.0020960690453648567\n",
      "Batch loss: 0.022668464109301567\n",
      "Batch loss: 0.014702823013067245\n",
      "Batch loss: 0.015515731647610664\n",
      "Batch loss: 0.02455125004053116\n",
      "Batch loss: 0.0125753004103899\n",
      "Batch loss: 0.017729001119732857\n",
      "Batch loss: 0.05055772885680199\n",
      "Batch loss: 0.004561677109450102\n",
      "Batch loss: 0.01557586807757616\n",
      "Batch loss: 0.04343574866652489\n",
      "Batch loss: 0.027835825458168983\n",
      "Batch loss: 0.008515344001352787\n",
      "Epoch [14/100], Loss: 0.0268\n",
      "Validation Loss: 0.5783, Accuracy: 92.29%\n",
      "No improvement for 10 epoches. Early stopping.\n",
      "Start training Gaussian_Blur model.\n",
      "Batch loss: 1.283625841140747\n",
      "Batch loss: 0.6791748404502869\n",
      "Batch loss: 0.8188716173171997\n",
      "Batch loss: 0.7177332639694214\n",
      "Batch loss: 0.6460540294647217\n",
      "Batch loss: 0.5870654582977295\n",
      "Batch loss: 0.6337396502494812\n",
      "Batch loss: 0.6700502634048462\n",
      "Batch loss: 0.7509468793869019\n",
      "Batch loss: 0.4959615170955658\n",
      "Batch loss: 0.612702488899231\n",
      "Batch loss: 0.6034500598907471\n",
      "Batch loss: 0.47166508436203003\n",
      "Batch loss: 0.7432578802108765\n",
      "Batch loss: 0.5507185459136963\n",
      "Batch loss: 0.43462979793548584\n",
      "Batch loss: 0.33339667320251465\n",
      "Batch loss: 0.39274489879608154\n",
      "Batch loss: 0.32747548818588257\n",
      "Batch loss: 0.5298691391944885\n",
      "Batch loss: 0.4693557918071747\n",
      "Batch loss: 0.31178018450737\n",
      "Batch loss: 0.31216686964035034\n",
      "Batch loss: 0.3794460892677307\n",
      "Batch loss: 0.3210628032684326\n",
      "Batch loss: 0.2102956622838974\n",
      "Batch loss: 0.47460034489631653\n",
      "Batch loss: 0.40633106231689453\n",
      "Batch loss: 0.2617361545562744\n",
      "Batch loss: 0.28353139758110046\n",
      "Batch loss: 0.4549909234046936\n",
      "Batch loss: 0.33665862679481506\n",
      "Batch loss: 0.2731805741786957\n",
      "Batch loss: 0.17213833332061768\n",
      "Batch loss: 0.2684326171875\n",
      "Batch loss: 0.33795323967933655\n",
      "Batch loss: 0.39560723304748535\n",
      "Batch loss: 0.2016240358352661\n",
      "Batch loss: 0.32249218225479126\n",
      "Batch loss: 0.20358753204345703\n",
      "Batch loss: 0.29018059372901917\n",
      "Batch loss: 0.3416801989078522\n",
      "Batch loss: 0.3393900692462921\n",
      "Batch loss: 0.27769505977630615\n",
      "Batch loss: 0.3254125416278839\n",
      "Batch loss: 0.22567905485630035\n",
      "Batch loss: 0.28471505641937256\n",
      "Batch loss: 0.22443096339702606\n",
      "Batch loss: 0.19528253376483917\n",
      "Batch loss: 0.24663886427879333\n",
      "Batch loss: 0.29576748609542847\n",
      "Batch loss: 0.3356732130050659\n",
      "Batch loss: 0.2640141248703003\n",
      "Batch loss: 0.22170734405517578\n",
      "Batch loss: 0.3090081512928009\n",
      "Batch loss: 0.3296445906162262\n",
      "Batch loss: 0.22049619257450104\n",
      "Batch loss: 0.32436859607696533\n",
      "Batch loss: 0.24849234521389008\n",
      "Batch loss: 0.2558586597442627\n",
      "Batch loss: 0.23622342944145203\n",
      "Batch loss: 0.19596035778522491\n",
      "Batch loss: 0.23447242379188538\n",
      "Batch loss: 0.234932541847229\n",
      "Batch loss: 0.23173560202121735\n",
      "Batch loss: 0.28994351625442505\n",
      "Batch loss: 0.30395233631134033\n",
      "Batch loss: 0.38496044278144836\n",
      "Batch loss: 0.289455771446228\n",
      "Batch loss: 0.2885262370109558\n",
      "Batch loss: 0.23671528697013855\n",
      "Batch loss: 0.3433823883533478\n",
      "Batch loss: 0.22351157665252686\n",
      "Batch loss: 0.2543644607067108\n",
      "Batch loss: 0.24115319550037384\n",
      "Batch loss: 0.266488254070282\n",
      "Batch loss: 0.2955649495124817\n",
      "Batch loss: 0.2748032510280609\n",
      "Batch loss: 0.18254956603050232\n",
      "Batch loss: 0.3022322952747345\n",
      "Batch loss: 0.20724378526210785\n",
      "Batch loss: 0.2507382929325104\n",
      "Batch loss: 0.3169211447238922\n",
      "Batch loss: 0.3638041019439697\n",
      "Batch loss: 0.262757271528244\n",
      "Batch loss: 0.20079508423805237\n",
      "Batch loss: 0.2014559507369995\n",
      "Batch loss: 0.2863104045391083\n",
      "Batch loss: 0.35599568486213684\n",
      "Batch loss: 0.24799083173274994\n",
      "Batch loss: 0.3541225790977478\n",
      "Batch loss: 0.218780055642128\n",
      "Batch loss: 0.29714062809944153\n",
      "Batch loss: 0.22885608673095703\n",
      "Batch loss: 0.2789880335330963\n",
      "Batch loss: 0.22626455128192902\n",
      "Batch loss: 0.18419641256332397\n",
      "Batch loss: 0.34245580434799194\n",
      "Batch loss: 0.2673981487751007\n",
      "Batch loss: 0.2994583249092102\n",
      "Batch loss: 0.2134670466184616\n",
      "Batch loss: 0.15469510853290558\n",
      "Batch loss: 0.2346808910369873\n",
      "Batch loss: 0.24970142543315887\n",
      "Batch loss: 0.22329185903072357\n",
      "Batch loss: 0.29947006702423096\n",
      "Batch loss: 0.33099186420440674\n",
      "Batch loss: 0.213908851146698\n",
      "Batch loss: 0.17327560484409332\n",
      "Batch loss: 0.39797407388687134\n",
      "Batch loss: 0.25129228830337524\n",
      "Batch loss: 0.17247232794761658\n",
      "Batch loss: 0.24583999812602997\n",
      "Batch loss: 0.1591644138097763\n",
      "Batch loss: 0.1832968145608902\n",
      "Batch loss: 0.25409477949142456\n",
      "Batch loss: 0.23140400648117065\n",
      "Batch loss: 0.1533525586128235\n",
      "Batch loss: 0.2125006467103958\n",
      "Batch loss: 0.23635950684547424\n",
      "Batch loss: 0.20658260583877563\n",
      "Batch loss: 0.2421097457408905\n",
      "Batch loss: 0.2732577919960022\n",
      "Batch loss: 0.12849360704421997\n",
      "Batch loss: 0.32707417011260986\n",
      "Batch loss: 0.3659707307815552\n",
      "Batch loss: 0.26745161414146423\n",
      "Batch loss: 0.27680402994155884\n",
      "Batch loss: 0.2267075479030609\n",
      "Batch loss: 0.25233516097068787\n",
      "Batch loss: 0.23878413438796997\n",
      "Batch loss: 0.26588550209999084\n",
      "Batch loss: 0.21420204639434814\n",
      "Batch loss: 0.3641807436943054\n",
      "Batch loss: 0.4025021195411682\n",
      "Batch loss: 0.27181756496429443\n",
      "Batch loss: 0.2830444276332855\n",
      "Batch loss: 0.2511782944202423\n",
      "Batch loss: 0.39326637983322144\n",
      "Batch loss: 0.22463615238666534\n",
      "Batch loss: 0.21891973912715912\n",
      "Batch loss: 0.20740285515785217\n",
      "Batch loss: 0.27077603340148926\n",
      "Batch loss: 0.28814786672592163\n",
      "Batch loss: 0.2031417340040207\n",
      "Batch loss: 0.29614758491516113\n",
      "Batch loss: 0.2155018448829651\n",
      "Batch loss: 0.21911554038524628\n",
      "Batch loss: 0.2741377055644989\n",
      "Batch loss: 0.1496376395225525\n",
      "Batch loss: 0.19640374183654785\n",
      "Batch loss: 0.2407936304807663\n",
      "Batch loss: 0.30332261323928833\n",
      "Batch loss: 0.30614927411079407\n",
      "Batch loss: 0.26904356479644775\n",
      "Batch loss: 0.3033982515335083\n",
      "Batch loss: 0.25234508514404297\n",
      "Batch loss: 0.2249404639005661\n",
      "Batch loss: 0.23951883614063263\n",
      "Batch loss: 0.23731400072574615\n",
      "Batch loss: 0.16821376979351044\n",
      "Batch loss: 0.2549697458744049\n",
      "Batch loss: 0.2060617208480835\n",
      "Batch loss: 0.2371223419904709\n",
      "Batch loss: 0.28417742252349854\n",
      "Batch loss: 0.22147227823734283\n",
      "Batch loss: 0.18130181729793549\n",
      "Batch loss: 0.2124250829219818\n",
      "Batch loss: 0.21667981147766113\n",
      "Batch loss: 0.23576559126377106\n",
      "Batch loss: 0.2720435857772827\n",
      "Batch loss: 0.28375619649887085\n",
      "Batch loss: 0.18954844772815704\n",
      "Batch loss: 0.19930841028690338\n",
      "Batch loss: 0.1709064394235611\n",
      "Batch loss: 0.23446539044380188\n",
      "Batch loss: 0.29543536901474\n",
      "Batch loss: 0.1864227056503296\n",
      "Batch loss: 0.22310851514339447\n",
      "Batch loss: 0.20537954568862915\n",
      "Batch loss: 0.26469114422798157\n",
      "Batch loss: 0.17564208805561066\n",
      "Batch loss: 0.096711166203022\n",
      "Batch loss: 0.28685450553894043\n",
      "Batch loss: 0.25236833095550537\n",
      "Batch loss: 0.16839608550071716\n",
      "Batch loss: 0.2412576824426651\n",
      "Batch loss: 0.25540927052497864\n",
      "Batch loss: 0.1910986304283142\n",
      "Batch loss: 0.2121153473854065\n",
      "Batch loss: 0.27888578176498413\n",
      "Batch loss: 0.1783457249403\n",
      "Batch loss: 0.2838915288448334\n",
      "Batch loss: 0.21068471670150757\n",
      "Batch loss: 0.17940615117549896\n",
      "Batch loss: 0.20932327210903168\n",
      "Batch loss: 0.14237180352210999\n",
      "Batch loss: 0.26345282793045044\n",
      "Batch loss: 0.19408820569515228\n",
      "Batch loss: 0.18754075467586517\n",
      "Batch loss: 0.2748357057571411\n",
      "Batch loss: 0.2782600224018097\n",
      "Batch loss: 0.15334568917751312\n",
      "Batch loss: 0.12035569548606873\n",
      "Batch loss: 0.2336481511592865\n",
      "Batch loss: 0.27806535363197327\n",
      "Batch loss: 0.3185710608959198\n",
      "Batch loss: 0.28421902656555176\n",
      "Batch loss: 0.15350764989852905\n",
      "Batch loss: 0.18281888961791992\n",
      "Batch loss: 0.2119700163602829\n",
      "Batch loss: 0.25167912244796753\n",
      "Batch loss: 0.23204727470874786\n",
      "Batch loss: 0.28157204389572144\n",
      "Batch loss: 0.22455161809921265\n",
      "Batch loss: 0.15047861635684967\n",
      "Batch loss: 0.27325841784477234\n",
      "Batch loss: 0.20942191779613495\n",
      "Batch loss: 0.16184981167316437\n",
      "Batch loss: 0.22824831306934357\n",
      "Batch loss: 0.13183371722698212\n",
      "Batch loss: 0.17546987533569336\n",
      "Batch loss: 0.16137611865997314\n",
      "Batch loss: 0.39964136481285095\n",
      "Batch loss: 0.23225751519203186\n",
      "Batch loss: 0.2959885597229004\n",
      "Batch loss: 0.16187231242656708\n",
      "Batch loss: 0.3044317364692688\n",
      "Batch loss: 0.3223989009857178\n",
      "Batch loss: 0.241313636302948\n",
      "Batch loss: 0.3512257933616638\n",
      "Batch loss: 0.3008095622062683\n",
      "Batch loss: 0.18873126804828644\n",
      "Batch loss: 0.18733109533786774\n",
      "Batch loss: 0.19443699717521667\n",
      "Batch loss: 0.3119792640209198\n",
      "Batch loss: 0.21519973874092102\n",
      "Batch loss: 0.23014314472675323\n",
      "Batch loss: 0.17119179666042328\n",
      "Batch loss: 0.2036675214767456\n",
      "Batch loss: 0.25028079748153687\n",
      "Batch loss: 0.25009071826934814\n",
      "Batch loss: 0.15681332349777222\n",
      "Batch loss: 0.2081950455904007\n",
      "Batch loss: 0.17754116654396057\n",
      "Batch loss: 0.17322085797786713\n",
      "Batch loss: 0.1881757229566574\n",
      "Batch loss: 0.22307731211185455\n",
      "Batch loss: 0.20615600049495697\n",
      "Batch loss: 0.17527520656585693\n",
      "Batch loss: 0.2757970690727234\n",
      "Batch loss: 0.2331143170595169\n",
      "Batch loss: 0.35946524143218994\n",
      "Batch loss: 0.3009452819824219\n",
      "Batch loss: 0.21700431406497955\n",
      "Batch loss: 0.324160635471344\n",
      "Batch loss: 0.25993093848228455\n",
      "Batch loss: 0.23713654279708862\n",
      "Batch loss: 0.3016989827156067\n",
      "Batch loss: 0.18425437808036804\n",
      "Batch loss: 0.19280782341957092\n",
      "Batch loss: 0.2068375200033188\n",
      "Batch loss: 0.27962636947631836\n",
      "Batch loss: 0.22069604694843292\n",
      "Batch loss: 0.25609931349754333\n",
      "Batch loss: 0.11714040488004684\n",
      "Batch loss: 0.26956021785736084\n",
      "Batch loss: 0.16510923206806183\n",
      "Batch loss: 0.22200827300548553\n",
      "Batch loss: 0.23090733587741852\n",
      "Batch loss: 0.2210340052843094\n",
      "Batch loss: 0.2629907429218292\n",
      "Batch loss: 0.16149818897247314\n",
      "Batch loss: 0.18377943336963654\n",
      "Batch loss: 0.24309055507183075\n",
      "Batch loss: 0.19246424734592438\n",
      "Batch loss: 0.15538038313388824\n",
      "Batch loss: 0.2563885450363159\n",
      "Batch loss: 0.1708676666021347\n",
      "Batch loss: 0.26389726996421814\n",
      "Batch loss: 0.23556101322174072\n",
      "Batch loss: 0.13084080815315247\n",
      "Batch loss: 0.2933136522769928\n",
      "Batch loss: 0.42661193013191223\n",
      "Batch loss: 0.2214927077293396\n",
      "Batch loss: 0.26056477427482605\n",
      "Batch loss: 0.21140950918197632\n",
      "Batch loss: 0.1526937335729599\n",
      "Batch loss: 0.1968216598033905\n",
      "Batch loss: 0.14214850962162018\n",
      "Batch loss: 0.21041525900363922\n",
      "Batch loss: 0.23651903867721558\n",
      "Batch loss: 0.2118963599205017\n",
      "Batch loss: 0.20415034890174866\n",
      "Batch loss: 0.23513121902942657\n",
      "Batch loss: 0.24995563924312592\n",
      "Batch loss: 0.24091032147407532\n",
      "Batch loss: 0.15715192258358002\n",
      "Batch loss: 0.24582920968532562\n",
      "Batch loss: 0.21014529466629028\n",
      "Batch loss: 0.26958632469177246\n",
      "Batch loss: 0.1849932074546814\n",
      "Batch loss: 0.2205296903848648\n",
      "Batch loss: 0.1899910271167755\n",
      "Batch loss: 0.2619077265262604\n",
      "Batch loss: 0.21534273028373718\n",
      "Batch loss: 0.11803486943244934\n",
      "Batch loss: 0.1678636521100998\n",
      "Batch loss: 0.171746164560318\n",
      "Batch loss: 0.25226908922195435\n",
      "Batch loss: 0.2564272880554199\n",
      "Batch loss: 0.14915218949317932\n",
      "Batch loss: 0.16911667585372925\n",
      "Batch loss: 0.26347628235816956\n",
      "Batch loss: 0.2729654014110565\n",
      "Batch loss: 0.2688716650009155\n",
      "Batch loss: 0.15969890356063843\n",
      "Batch loss: 0.1860567033290863\n",
      "Batch loss: 0.2602151334285736\n",
      "Batch loss: 0.19787003099918365\n",
      "Batch loss: 0.2452012598514557\n",
      "Batch loss: 0.1390945017337799\n",
      "Batch loss: 0.17811959981918335\n",
      "Batch loss: 0.2831825613975525\n",
      "Batch loss: 0.19943323731422424\n",
      "Batch loss: 0.19150255620479584\n",
      "Batch loss: 0.22798779606819153\n",
      "Batch loss: 0.29892486333847046\n",
      "Batch loss: 0.1953565776348114\n",
      "Batch loss: 0.2850821912288666\n",
      "Batch loss: 0.19066724181175232\n",
      "Batch loss: 0.3478652536869049\n",
      "Batch loss: 0.13146458566188812\n",
      "Batch loss: 0.18639899790287018\n",
      "Batch loss: 0.2046661078929901\n",
      "Batch loss: 0.16929562389850616\n",
      "Batch loss: 0.12227185815572739\n",
      "Batch loss: 0.17278428375720978\n",
      "Batch loss: 0.208340585231781\n",
      "Batch loss: 0.13960760831832886\n",
      "Batch loss: 0.16266722977161407\n",
      "Batch loss: 0.13368558883666992\n",
      "Batch loss: 0.14971566200256348\n",
      "Batch loss: 0.24259503185749054\n",
      "Batch loss: 0.13248908519744873\n",
      "Batch loss: 0.20627303421497345\n",
      "Batch loss: 0.27494344115257263\n",
      "Batch loss: 0.35835814476013184\n",
      "Batch loss: 0.14525124430656433\n",
      "Batch loss: 0.1495884358882904\n",
      "Batch loss: 0.18703699111938477\n",
      "Batch loss: 0.20003804564476013\n",
      "Batch loss: 0.10806335508823395\n",
      "Batch loss: 0.19729933142662048\n",
      "Batch loss: 0.1879371553659439\n",
      "Batch loss: 0.188683420419693\n",
      "Batch loss: 0.11663062125444412\n",
      "Batch loss: 0.26666519045829773\n",
      "Batch loss: 0.27468547224998474\n",
      "Batch loss: 0.2491457462310791\n",
      "Batch loss: 0.2097870409488678\n",
      "Batch loss: 0.2065776139497757\n",
      "Batch loss: 0.20160406827926636\n",
      "Batch loss: 0.2455361783504486\n",
      "Batch loss: 0.22563812136650085\n",
      "Batch loss: 0.1480666548013687\n",
      "Batch loss: 0.2350350320339203\n",
      "Batch loss: 0.16089971363544464\n",
      "Batch loss: 0.16076330840587616\n",
      "Batch loss: 0.2780221104621887\n",
      "Batch loss: 0.1455916464328766\n",
      "Batch loss: 0.28434836864471436\n",
      "Batch loss: 0.20262987911701202\n",
      "Batch loss: 0.1759079545736313\n",
      "Batch loss: 0.20996639132499695\n",
      "Batch loss: 0.14684413373470306\n",
      "Batch loss: 0.2498917579650879\n",
      "Batch loss: 0.1415279656648636\n",
      "Batch loss: 0.28201600909233093\n",
      "Batch loss: 0.277991384267807\n",
      "Batch loss: 0.3986576497554779\n",
      "Batch loss: 0.16440534591674805\n",
      "Batch loss: 0.19311371445655823\n",
      "Batch loss: 0.24475102126598358\n",
      "Batch loss: 0.10958383232355118\n",
      "Batch loss: 0.22664418816566467\n",
      "Batch loss: 0.2763499319553375\n",
      "Batch loss: 0.14498715102672577\n",
      "Batch loss: 0.20510993897914886\n",
      "Batch loss: 0.17833401262760162\n",
      "Batch loss: 0.20071761310100555\n",
      "Batch loss: 0.21899260580539703\n",
      "Batch loss: 0.11690045148134232\n",
      "Batch loss: 0.16427919268608093\n",
      "Batch loss: 0.290990948677063\n",
      "Batch loss: 0.2432820200920105\n",
      "Batch loss: 0.07104133069515228\n",
      "Batch loss: 0.20834504067897797\n",
      "Batch loss: 0.1642603874206543\n",
      "Batch loss: 0.16165858507156372\n",
      "Batch loss: 0.17601309716701508\n",
      "Batch loss: 0.15501801669597626\n",
      "Batch loss: 0.31760239601135254\n",
      "Batch loss: 0.17582769691944122\n",
      "Batch loss: 0.12601907551288605\n",
      "Batch loss: 0.3234389126300812\n",
      "Batch loss: 0.21972258388996124\n",
      "Batch loss: 0.17195452749729156\n",
      "Batch loss: 0.17107751965522766\n",
      "Batch loss: 0.3201645016670227\n",
      "Batch loss: 0.18155895173549652\n",
      "Batch loss: 0.1857399344444275\n",
      "Batch loss: 0.13529205322265625\n",
      "Batch loss: 0.1858147531747818\n",
      "Batch loss: 0.2499467432498932\n",
      "Batch loss: 0.18011824786663055\n",
      "Batch loss: 0.16017501056194305\n",
      "Epoch [1/100], Loss: 0.2525\n",
      "Validation Loss: 0.7528, Accuracy: 73.87%\n",
      "Batch loss: 0.2516491413116455\n",
      "Batch loss: 0.1911839097738266\n",
      "Batch loss: 0.22975966334342957\n",
      "Batch loss: 0.2261027693748474\n",
      "Batch loss: 0.2255088984966278\n",
      "Batch loss: 0.19837261736392975\n",
      "Batch loss: 0.46648505330085754\n",
      "Batch loss: 0.2182040512561798\n",
      "Batch loss: 0.2934197187423706\n",
      "Batch loss: 0.15159741044044495\n",
      "Batch loss: 0.20447123050689697\n",
      "Batch loss: 0.2840205132961273\n",
      "Batch loss: 0.17961162328720093\n",
      "Batch loss: 0.27851197123527527\n",
      "Batch loss: 0.2586725354194641\n",
      "Batch loss: 0.20381058752536774\n",
      "Batch loss: 0.18812163174152374\n",
      "Batch loss: 0.2446710616350174\n",
      "Batch loss: 0.13370728492736816\n",
      "Batch loss: 0.19733405113220215\n",
      "Batch loss: 0.16239307820796967\n",
      "Batch loss: 0.15465404093265533\n",
      "Batch loss: 0.18412812054157257\n",
      "Batch loss: 0.16121543943881989\n",
      "Batch loss: 0.14957942068576813\n",
      "Batch loss: 0.14661476016044617\n",
      "Batch loss: 0.18806298077106476\n",
      "Batch loss: 0.2266272008419037\n",
      "Batch loss: 0.1391455978155136\n",
      "Batch loss: 0.15261569619178772\n",
      "Batch loss: 0.3034665286540985\n",
      "Batch loss: 0.17254483699798584\n",
      "Batch loss: 0.16507568955421448\n",
      "Batch loss: 0.12274948507547379\n",
      "Batch loss: 0.1131974384188652\n",
      "Batch loss: 0.2745731472969055\n",
      "Batch loss: 0.2282906323671341\n",
      "Batch loss: 0.15354929864406586\n",
      "Batch loss: 0.22974541783332825\n",
      "Batch loss: 0.12465228140354156\n",
      "Batch loss: 0.1969558149576187\n",
      "Batch loss: 0.15051209926605225\n",
      "Batch loss: 0.22402989864349365\n",
      "Batch loss: 0.1643180251121521\n",
      "Batch loss: 0.2504599988460541\n",
      "Batch loss: 0.15116707980632782\n",
      "Batch loss: 0.1701263189315796\n",
      "Batch loss: 0.15866252779960632\n",
      "Batch loss: 0.09128166735172272\n",
      "Batch loss: 0.14040254056453705\n",
      "Batch loss: 0.20620128512382507\n",
      "Batch loss: 0.20910795032978058\n",
      "Batch loss: 0.12861549854278564\n",
      "Batch loss: 0.14977191388607025\n",
      "Batch loss: 0.2158992886543274\n",
      "Batch loss: 0.2013830989599228\n",
      "Batch loss: 0.2012552171945572\n",
      "Batch loss: 0.1684400737285614\n",
      "Batch loss: 0.15075337886810303\n",
      "Batch loss: 0.16578786075115204\n",
      "Batch loss: 0.19001314043998718\n",
      "Batch loss: 0.1609007567167282\n",
      "Batch loss: 0.22377026081085205\n",
      "Batch loss: 0.21123293042182922\n",
      "Batch loss: 0.11701224744319916\n",
      "Batch loss: 0.2084379941225052\n",
      "Batch loss: 0.18244071304798126\n",
      "Batch loss: 0.2557971179485321\n",
      "Batch loss: 0.1577221006155014\n",
      "Batch loss: 0.15399426221847534\n",
      "Batch loss: 0.19385923445224762\n",
      "Batch loss: 0.21975119411945343\n",
      "Batch loss: 0.1327609419822693\n",
      "Batch loss: 0.14877989888191223\n",
      "Batch loss: 0.15060818195343018\n",
      "Batch loss: 0.19268709421157837\n",
      "Batch loss: 0.18950319290161133\n",
      "Batch loss: 0.20827019214630127\n",
      "Batch loss: 0.15016546845436096\n",
      "Batch loss: 0.19704808294773102\n",
      "Batch loss: 0.10948606580495834\n",
      "Batch loss: 0.09427081793546677\n",
      "Batch loss: 0.21997356414794922\n",
      "Batch loss: 0.3217916786670685\n",
      "Batch loss: 0.2105170637369156\n",
      "Batch loss: 0.13641229271888733\n",
      "Batch loss: 0.08528006076812744\n",
      "Batch loss: 0.15372101962566376\n",
      "Batch loss: 0.14451532065868378\n",
      "Batch loss: 0.14041411876678467\n",
      "Batch loss: 0.2789476811885834\n",
      "Batch loss: 0.09894353896379471\n",
      "Batch loss: 0.2641350030899048\n",
      "Batch loss: 0.17927181720733643\n",
      "Batch loss: 0.19060248136520386\n",
      "Batch loss: 0.14621669054031372\n",
      "Batch loss: 0.12569963932037354\n",
      "Batch loss: 0.2604046165943146\n",
      "Batch loss: 0.17200890183448792\n",
      "Batch loss: 0.230218768119812\n",
      "Batch loss: 0.15131127834320068\n",
      "Batch loss: 0.09454303979873657\n",
      "Batch loss: 0.2013298124074936\n",
      "Batch loss: 0.19923211634159088\n",
      "Batch loss: 0.18349048495292664\n",
      "Batch loss: 0.1330615133047104\n",
      "Batch loss: 0.17057929933071136\n",
      "Batch loss: 0.2065618634223938\n",
      "Batch loss: 0.1769840568304062\n",
      "Batch loss: 0.2728123068809509\n",
      "Batch loss: 0.22050045430660248\n",
      "Batch loss: 0.08345887809991837\n",
      "Batch loss: 0.15507137775421143\n",
      "Batch loss: 0.11798464506864548\n",
      "Batch loss: 0.12689730525016785\n",
      "Batch loss: 0.17741794884204865\n",
      "Batch loss: 0.151087686419487\n",
      "Batch loss: 0.07307782769203186\n",
      "Batch loss: 0.15377628803253174\n",
      "Batch loss: 0.15004125237464905\n",
      "Batch loss: 0.191362664103508\n",
      "Batch loss: 0.15702079236507416\n",
      "Batch loss: 0.2120453119277954\n",
      "Batch loss: 0.11841325461864471\n",
      "Batch loss: 0.22306768596172333\n",
      "Batch loss: 0.23278386890888214\n",
      "Batch loss: 0.15754272043704987\n",
      "Batch loss: 0.18422064185142517\n",
      "Batch loss: 0.1282770335674286\n",
      "Batch loss: 0.22170348465442657\n",
      "Batch loss: 0.179130420088768\n",
      "Batch loss: 0.2527673840522766\n",
      "Batch loss: 0.1756776124238968\n",
      "Batch loss: 0.20203864574432373\n",
      "Batch loss: 0.23461122810840607\n",
      "Batch loss: 0.08874049782752991\n",
      "Batch loss: 0.1730586737394333\n",
      "Batch loss: 0.22227415442466736\n",
      "Batch loss: 0.20201367139816284\n",
      "Batch loss: 0.07216772437095642\n",
      "Batch loss: 0.1419639140367508\n",
      "Batch loss: 0.137970969080925\n",
      "Batch loss: 0.20303721725940704\n",
      "Batch loss: 0.2643757462501526\n",
      "Batch loss: 0.19760499894618988\n",
      "Batch loss: 0.20000013709068298\n",
      "Batch loss: 0.23824715614318848\n",
      "Batch loss: 0.17927111685276031\n",
      "Batch loss: 0.16017776727676392\n",
      "Batch loss: 0.11516471207141876\n",
      "Batch loss: 0.16037492454051971\n",
      "Batch loss: 0.17661504447460175\n",
      "Batch loss: 0.22422073781490326\n",
      "Batch loss: 0.27601689100265503\n",
      "Batch loss: 0.22892752289772034\n",
      "Batch loss: 0.22857549786567688\n",
      "Batch loss: 0.19465360045433044\n",
      "Batch loss: 0.12861867249011993\n",
      "Batch loss: 0.17841415107250214\n",
      "Batch loss: 0.20708338916301727\n",
      "Batch loss: 0.08726013451814651\n",
      "Batch loss: 0.1775023490190506\n",
      "Batch loss: 0.1482616811990738\n",
      "Batch loss: 0.17331328988075256\n",
      "Batch loss: 0.20294596254825592\n",
      "Batch loss: 0.19303300976753235\n",
      "Batch loss: 0.14981669187545776\n",
      "Batch loss: 0.18192589282989502\n",
      "Batch loss: 0.13784195482730865\n",
      "Batch loss: 0.15940645337104797\n",
      "Batch loss: 0.19325882196426392\n",
      "Batch loss: 0.24527573585510254\n",
      "Batch loss: 0.10180383920669556\n",
      "Batch loss: 0.13415756821632385\n",
      "Batch loss: 0.1285596787929535\n",
      "Batch loss: 0.24659602344036102\n",
      "Batch loss: 0.22624506056308746\n",
      "Batch loss: 0.13514761626720428\n",
      "Batch loss: 0.21852391958236694\n",
      "Batch loss: 0.16911838948726654\n",
      "Batch loss: 0.19933149218559265\n",
      "Batch loss: 0.10018336027860641\n",
      "Batch loss: 0.07745873183012009\n",
      "Batch loss: 0.3044402599334717\n",
      "Batch loss: 0.18587026000022888\n",
      "Batch loss: 0.14252081513404846\n",
      "Batch loss: 0.2362879365682602\n",
      "Batch loss: 0.1683402955532074\n",
      "Batch loss: 0.17962415516376495\n",
      "Batch loss: 0.15088742971420288\n",
      "Batch loss: 0.17414817214012146\n",
      "Batch loss: 0.16653180122375488\n",
      "Batch loss: 0.20522215962409973\n",
      "Batch loss: 0.22662390768527985\n",
      "Batch loss: 0.12803004682064056\n",
      "Batch loss: 0.14699424803256989\n",
      "Batch loss: 0.11801772564649582\n",
      "Batch loss: 0.23278005421161652\n",
      "Batch loss: 0.17401903867721558\n",
      "Batch loss: 0.1351931095123291\n",
      "Batch loss: 0.25942423939704895\n",
      "Batch loss: 0.21215248107910156\n",
      "Batch loss: 0.15752939879894257\n",
      "Batch loss: 0.11166603863239288\n",
      "Batch loss: 0.2400515377521515\n",
      "Batch loss: 0.26603108644485474\n",
      "Batch loss: 0.3131661117076874\n",
      "Batch loss: 0.17106935381889343\n",
      "Batch loss: 0.09608510881662369\n",
      "Batch loss: 0.11058028787374496\n",
      "Batch loss: 0.1382199376821518\n",
      "Batch loss: 0.17392989993095398\n",
      "Batch loss: 0.1734708845615387\n",
      "Batch loss: 0.26946982741355896\n",
      "Batch loss: 0.22352354228496552\n",
      "Batch loss: 0.135900616645813\n",
      "Batch loss: 0.18334060907363892\n",
      "Batch loss: 0.17903321981430054\n",
      "Batch loss: 0.12182056158781052\n",
      "Batch loss: 0.17427535355091095\n",
      "Batch loss: 0.14199934899806976\n",
      "Batch loss: 0.17469769716262817\n",
      "Batch loss: 0.13933341205120087\n",
      "Batch loss: 0.3328419327735901\n",
      "Batch loss: 0.1388426423072815\n",
      "Batch loss: 0.27332475781440735\n",
      "Batch loss: 0.15092071890830994\n",
      "Batch loss: 0.1947154849767685\n",
      "Batch loss: 0.28619107604026794\n",
      "Batch loss: 0.15132689476013184\n",
      "Batch loss: 0.21184171736240387\n",
      "Batch loss: 0.1701156497001648\n",
      "Batch loss: 0.13730497658252716\n",
      "Batch loss: 0.13403579592704773\n",
      "Batch loss: 0.1275467723608017\n",
      "Batch loss: 0.2011805772781372\n",
      "Batch loss: 0.1283978670835495\n",
      "Batch loss: 0.19563063979148865\n",
      "Batch loss: 0.17286358773708344\n",
      "Batch loss: 0.1499946564435959\n",
      "Batch loss: 0.1958736777305603\n",
      "Batch loss: 0.17691822350025177\n",
      "Batch loss: 0.12473650276660919\n",
      "Batch loss: 0.162212535738945\n",
      "Batch loss: 0.14939984679222107\n",
      "Batch loss: 0.15820303559303284\n",
      "Batch loss: 0.13486705720424652\n",
      "Batch loss: 0.21407172083854675\n",
      "Batch loss: 0.11875411123037338\n",
      "Batch loss: 0.13205701112747192\n",
      "Batch loss: 0.20056097209453583\n",
      "Batch loss: 0.21693843603134155\n",
      "Batch loss: 0.3447902500629425\n",
      "Batch loss: 0.2993828058242798\n",
      "Batch loss: 0.1688031703233719\n",
      "Batch loss: 0.2734861671924591\n",
      "Batch loss: 0.20924334228038788\n",
      "Batch loss: 0.20171299576759338\n",
      "Batch loss: 0.29865139722824097\n",
      "Batch loss: 0.16762278974056244\n",
      "Batch loss: 0.1739458441734314\n",
      "Batch loss: 0.14492543041706085\n",
      "Batch loss: 0.2194371223449707\n",
      "Batch loss: 0.1612863838672638\n",
      "Batch loss: 0.17396865785121918\n",
      "Batch loss: 0.10193756222724915\n",
      "Batch loss: 0.2571372091770172\n",
      "Batch loss: 0.21337828040122986\n",
      "Batch loss: 0.15499292314052582\n",
      "Batch loss: 0.1380014270544052\n",
      "Batch loss: 0.1796313226222992\n",
      "Batch loss: 0.17519165575504303\n",
      "Batch loss: 0.112336665391922\n",
      "Batch loss: 0.16685649752616882\n",
      "Batch loss: 0.2290208339691162\n",
      "Batch loss: 0.17903532087802887\n",
      "Batch loss: 0.1253259927034378\n",
      "Batch loss: 0.17318588495254517\n",
      "Batch loss: 0.15543073415756226\n",
      "Batch loss: 0.2095852941274643\n",
      "Batch loss: 0.18132244050502777\n",
      "Batch loss: 0.09712399542331696\n",
      "Batch loss: 0.20542475581169128\n",
      "Batch loss: 0.37552911043167114\n",
      "Batch loss: 0.18621309101581573\n",
      "Batch loss: 0.25696346163749695\n",
      "Batch loss: 0.17122429609298706\n",
      "Batch loss: 0.09737121313810349\n",
      "Batch loss: 0.14564505219459534\n",
      "Batch loss: 0.0896655023097992\n",
      "Batch loss: 0.1461363583803177\n",
      "Batch loss: 0.16827107965946198\n",
      "Batch loss: 0.12094852328300476\n",
      "Batch loss: 0.10691846907138824\n",
      "Batch loss: 0.18885555863380432\n",
      "Batch loss: 0.21540223062038422\n",
      "Batch loss: 0.17373383045196533\n",
      "Batch loss: 0.13539059460163116\n",
      "Batch loss: 0.16739557683467865\n",
      "Batch loss: 0.12652575969696045\n",
      "Batch loss: 0.1845853179693222\n",
      "Batch loss: 0.12155269831418991\n",
      "Batch loss: 0.16403163969516754\n",
      "Batch loss: 0.13147805631160736\n",
      "Batch loss: 0.22164233028888702\n",
      "Batch loss: 0.1452886462211609\n",
      "Batch loss: 0.0703478753566742\n",
      "Batch loss: 0.13865350186824799\n",
      "Batch loss: 0.0864168331027031\n",
      "Batch loss: 0.19035391509532928\n",
      "Batch loss: 0.17754852771759033\n",
      "Batch loss: 0.10902245342731476\n",
      "Batch loss: 0.1358163207769394\n",
      "Batch loss: 0.16195568442344666\n",
      "Batch loss: 0.24452431499958038\n",
      "Batch loss: 0.21244952082633972\n",
      "Batch loss: 0.14875759184360504\n",
      "Batch loss: 0.13979552686214447\n",
      "Batch loss: 0.20246942341327667\n",
      "Batch loss: 0.13811887800693512\n",
      "Batch loss: 0.16896308958530426\n",
      "Batch loss: 0.11435230076313019\n",
      "Batch loss: 0.13235852122306824\n",
      "Batch loss: 0.20392243564128876\n",
      "Batch loss: 0.15791438519954681\n",
      "Batch loss: 0.21947826445102692\n",
      "Batch loss: 0.1662474274635315\n",
      "Batch loss: 0.2605114281177521\n",
      "Batch loss: 0.16725650429725647\n",
      "Batch loss: 0.2343294620513916\n",
      "Batch loss: 0.13546748459339142\n",
      "Batch loss: 0.27636605501174927\n",
      "Batch loss: 0.09736599773168564\n",
      "Batch loss: 0.1418597549200058\n",
      "Batch loss: 0.16058897972106934\n",
      "Batch loss: 0.18490932881832123\n",
      "Batch loss: 0.059990547597408295\n",
      "Batch loss: 0.13378143310546875\n",
      "Batch loss: 0.15181124210357666\n",
      "Batch loss: 0.0999709889292717\n",
      "Batch loss: 0.1331283003091812\n",
      "Batch loss: 0.1373824179172516\n",
      "Batch loss: 0.13381296396255493\n",
      "Batch loss: 0.21203897893428802\n",
      "Batch loss: 0.1456241011619568\n",
      "Batch loss: 0.1695222109556198\n",
      "Batch loss: 0.2058338224887848\n",
      "Batch loss: 0.2913506329059601\n",
      "Batch loss: 0.1251000314950943\n",
      "Batch loss: 0.10795953869819641\n",
      "Batch loss: 0.15744321048259735\n",
      "Batch loss: 0.1670139580965042\n",
      "Batch loss: 0.08760404586791992\n",
      "Batch loss: 0.10846955329179764\n",
      "Batch loss: 0.15383748710155487\n",
      "Batch loss: 0.19403763115406036\n",
      "Batch loss: 0.09963278472423553\n",
      "Batch loss: 0.25427690148353577\n",
      "Batch loss: 0.25748589634895325\n",
      "Batch loss: 0.214285746216774\n",
      "Batch loss: 0.14987444877624512\n",
      "Batch loss: 0.21282067894935608\n",
      "Batch loss: 0.15025697648525238\n",
      "Batch loss: 0.2791008949279785\n",
      "Batch loss: 0.20221322774887085\n",
      "Batch loss: 0.13488182425498962\n",
      "Batch loss: 0.23073062300682068\n",
      "Batch loss: 0.15842154622077942\n",
      "Batch loss: 0.14391253888607025\n",
      "Batch loss: 0.274511456489563\n",
      "Batch loss: 0.12425864487886429\n",
      "Batch loss: 0.21507421135902405\n",
      "Batch loss: 0.1662183701992035\n",
      "Batch loss: 0.13741500675678253\n",
      "Batch loss: 0.14816886186599731\n",
      "Batch loss: 0.13938896358013153\n",
      "Batch loss: 0.21786417067050934\n",
      "Batch loss: 0.12225434929132462\n",
      "Batch loss: 0.24259798228740692\n",
      "Batch loss: 0.25326550006866455\n",
      "Batch loss: 0.32516202330589294\n",
      "Batch loss: 0.13324302434921265\n",
      "Batch loss: 0.1459103673696518\n",
      "Batch loss: 0.23109214007854462\n",
      "Batch loss: 0.10254012793302536\n",
      "Batch loss: 0.1684742420911789\n",
      "Batch loss: 0.3129499852657318\n",
      "Batch loss: 0.13213478028774261\n",
      "Batch loss: 0.1709626317024231\n",
      "Batch loss: 0.16411812603473663\n",
      "Batch loss: 0.13487401604652405\n",
      "Batch loss: 0.13662776350975037\n",
      "Batch loss: 0.11312878131866455\n",
      "Batch loss: 0.1223587840795517\n",
      "Batch loss: 0.24425621330738068\n",
      "Batch loss: 0.1934846192598343\n",
      "Batch loss: 0.07088080793619156\n",
      "Batch loss: 0.1333579421043396\n",
      "Batch loss: 0.08585544675588608\n",
      "Batch loss: 0.14552517235279083\n",
      "Batch loss: 0.13734669983386993\n",
      "Batch loss: 0.1660032868385315\n",
      "Batch loss: 0.2680888772010803\n",
      "Batch loss: 0.1313650906085968\n",
      "Batch loss: 0.0758250281214714\n",
      "Batch loss: 0.30472734570503235\n",
      "Batch loss: 0.15540647506713867\n",
      "Batch loss: 0.10924945026636124\n",
      "Batch loss: 0.12838630378246307\n",
      "Batch loss: 0.24876227974891663\n",
      "Batch loss: 0.1464741826057434\n",
      "Batch loss: 0.12374358624219894\n",
      "Batch loss: 0.0827847570180893\n",
      "Batch loss: 0.12085174024105072\n",
      "Batch loss: 0.1998172402381897\n",
      "Batch loss: 0.11395762860774994\n",
      "Batch loss: 0.11141207069158554\n",
      "Epoch [2/100], Loss: 0.1770\n",
      "Validation Loss: 0.2420, Accuracy: 91.78%\n",
      "Batch loss: 0.20923401415348053\n",
      "Batch loss: 0.17181269824504852\n",
      "Batch loss: 0.1713266521692276\n",
      "Batch loss: 0.21213491261005402\n",
      "Batch loss: 0.19968025386333466\n",
      "Batch loss: 0.19755002856254578\n",
      "Batch loss: 0.3927353620529175\n",
      "Batch loss: 0.18217384815216064\n",
      "Batch loss: 0.2619682252407074\n",
      "Batch loss: 0.10176318883895874\n",
      "Batch loss: 0.1627821922302246\n",
      "Batch loss: 0.22504495084285736\n",
      "Batch loss: 0.1272672414779663\n",
      "Batch loss: 0.25643405318260193\n",
      "Batch loss: 0.23129892349243164\n",
      "Batch loss: 0.19004741311073303\n",
      "Batch loss: 0.15285225212574005\n",
      "Batch loss: 0.19943736493587494\n",
      "Batch loss: 0.07592905312776566\n",
      "Batch loss: 0.1736956089735031\n",
      "Batch loss: 0.1379193216562271\n",
      "Batch loss: 0.12738186120986938\n",
      "Batch loss: 0.13053444027900696\n",
      "Batch loss: 0.15290671586990356\n",
      "Batch loss: 0.1387382596731186\n",
      "Batch loss: 0.12107644975185394\n",
      "Batch loss: 0.18043801188468933\n",
      "Batch loss: 0.13396167755126953\n",
      "Batch loss: 0.1032634899020195\n",
      "Batch loss: 0.12393981963396072\n",
      "Batch loss: 0.23497986793518066\n",
      "Batch loss: 0.14120885729789734\n",
      "Batch loss: 0.14317095279693604\n",
      "Batch loss: 0.10774746537208557\n",
      "Batch loss: 0.13526760041713715\n",
      "Batch loss: 0.27980828285217285\n",
      "Batch loss: 0.1556319296360016\n",
      "Batch loss: 0.15730370581150055\n",
      "Batch loss: 0.1976848542690277\n",
      "Batch loss: 0.08083353191614151\n",
      "Batch loss: 0.15949717164039612\n",
      "Batch loss: 0.10723426192998886\n",
      "Batch loss: 0.16097430884838104\n",
      "Batch loss: 0.12103784084320068\n",
      "Batch loss: 0.1594502180814743\n",
      "Batch loss: 0.13514426350593567\n",
      "Batch loss: 0.14659279584884644\n",
      "Batch loss: 0.11822223663330078\n",
      "Batch loss: 0.11036500334739685\n",
      "Batch loss: 0.14568978548049927\n",
      "Batch loss: 0.18020927906036377\n",
      "Batch loss: 0.1356450766324997\n",
      "Batch loss: 0.10039813071489334\n",
      "Batch loss: 0.1191210001707077\n",
      "Batch loss: 0.12032534927129745\n",
      "Batch loss: 0.18585355579853058\n",
      "Batch loss: 0.11276350915431976\n",
      "Batch loss: 0.13043764233589172\n",
      "Batch loss: 0.12211807072162628\n",
      "Batch loss: 0.14577654004096985\n",
      "Batch loss: 0.15059803426265717\n",
      "Batch loss: 0.13945244252681732\n",
      "Batch loss: 0.19289591908454895\n",
      "Batch loss: 0.1902841031551361\n",
      "Batch loss: 0.06677507609128952\n",
      "Batch loss: 0.17812390625476837\n",
      "Batch loss: 0.15174272656440735\n",
      "Batch loss: 0.2703673541545868\n",
      "Batch loss: 0.18969787657260895\n",
      "Batch loss: 0.15056243538856506\n",
      "Batch loss: 0.17507930099964142\n",
      "Batch loss: 0.18397442996501923\n",
      "Batch loss: 0.07374025881290436\n",
      "Batch loss: 0.11778350919485092\n",
      "Batch loss: 0.1281881183385849\n",
      "Batch loss: 0.22067444026470184\n",
      "Batch loss: 0.14355742931365967\n",
      "Batch loss: 0.21386843919754028\n",
      "Batch loss: 0.14743667840957642\n",
      "Batch loss: 0.18352802097797394\n",
      "Batch loss: 0.08226409554481506\n",
      "Batch loss: 0.09145282208919525\n",
      "Batch loss: 0.19356903433799744\n",
      "Batch loss: 0.24387934803962708\n",
      "Batch loss: 0.17337065935134888\n",
      "Batch loss: 0.14712868630886078\n",
      "Batch loss: 0.06248309090733528\n",
      "Batch loss: 0.14782562851905823\n",
      "Batch loss: 0.18184639513492584\n",
      "Batch loss: 0.11681745946407318\n",
      "Batch loss: 0.1906048208475113\n",
      "Batch loss: 0.09398455917835236\n",
      "Batch loss: 0.22929051518440247\n",
      "Batch loss: 0.1742672324180603\n",
      "Batch loss: 0.1664227396249771\n",
      "Batch loss: 0.15144386887550354\n",
      "Batch loss: 0.11317067593336105\n",
      "Batch loss: 0.19599966704845428\n",
      "Batch loss: 0.1565420925617218\n",
      "Batch loss: 0.17301583290100098\n",
      "Batch loss: 0.14489135146141052\n",
      "Batch loss: 0.08498785644769669\n",
      "Batch loss: 0.12810681760311127\n",
      "Batch loss: 0.16145022213459015\n",
      "Batch loss: 0.14269466698169708\n",
      "Batch loss: 0.15790614485740662\n",
      "Batch loss: 0.16137482225894928\n",
      "Batch loss: 0.1862456351518631\n",
      "Batch loss: 0.13570111989974976\n",
      "Batch loss: 0.2170669138431549\n",
      "Batch loss: 0.20155487954616547\n",
      "Batch loss: 0.12607094645500183\n",
      "Batch loss: 0.07962547987699509\n",
      "Batch loss: 0.10011100023984909\n",
      "Batch loss: 0.11425919830799103\n",
      "Batch loss: 0.1920466125011444\n",
      "Batch loss: 0.13739091157913208\n",
      "Batch loss: 0.05134734883904457\n",
      "Batch loss: 0.08943796157836914\n",
      "Batch loss: 0.12759174406528473\n",
      "Batch loss: 0.09286975860595703\n",
      "Batch loss: 0.11199124902486801\n",
      "Batch loss: 0.1957838088274002\n",
      "Batch loss: 0.11096475273370743\n",
      "Batch loss: 0.19593138992786407\n",
      "Batch loss: 0.16414481401443481\n",
      "Batch loss: 0.12324606627225876\n",
      "Batch loss: 0.15792222321033478\n",
      "Batch loss: 0.12079732120037079\n",
      "Batch loss: 0.16255488991737366\n",
      "Batch loss: 0.15184563398361206\n",
      "Batch loss: 0.17863015830516815\n",
      "Batch loss: 0.145528644323349\n",
      "Batch loss: 0.2199990153312683\n",
      "Batch loss: 0.17401772737503052\n",
      "Batch loss: 0.10110169649124146\n",
      "Batch loss: 0.07924772053956985\n",
      "Batch loss: 0.1674930602312088\n",
      "Batch loss: 0.22301626205444336\n",
      "Batch loss: 0.042921487241983414\n",
      "Batch loss: 0.10147097706794739\n",
      "Batch loss: 0.08125519007444382\n",
      "Batch loss: 0.18734139204025269\n",
      "Batch loss: 0.15721647441387177\n",
      "Batch loss: 0.18377245962619781\n",
      "Batch loss: 0.1594405174255371\n",
      "Batch loss: 0.16483338177204132\n",
      "Batch loss: 0.12687982618808746\n",
      "Batch loss: 0.14433301985263824\n",
      "Batch loss: 0.10741137713193893\n",
      "Batch loss: 0.1288115233182907\n",
      "Batch loss: 0.16585929691791534\n",
      "Batch loss: 0.22144824266433716\n",
      "Batch loss: 0.18343298137187958\n",
      "Batch loss: 0.1700393259525299\n",
      "Batch loss: 0.17613059282302856\n",
      "Batch loss: 0.18975140154361725\n",
      "Batch loss: 0.1360119879245758\n",
      "Batch loss: 0.1462155282497406\n",
      "Batch loss: 0.18041028082370758\n",
      "Batch loss: 0.09040488302707672\n",
      "Batch loss: 0.1543254554271698\n",
      "Batch loss: 0.17826084792613983\n",
      "Batch loss: 0.14287368953227997\n",
      "Batch loss: 0.12321142852306366\n",
      "Batch loss: 0.17694801092147827\n",
      "Batch loss: 0.1372338831424713\n",
      "Batch loss: 0.1286279261112213\n",
      "Batch loss: 0.1308564394712448\n",
      "Batch loss: 0.12228676676750183\n",
      "Batch loss: 0.17643919587135315\n",
      "Batch loss: 0.22623592615127563\n",
      "Batch loss: 0.08287595212459564\n",
      "Batch loss: 0.13659322261810303\n",
      "Batch loss: 0.11305098235607147\n",
      "Batch loss: 0.16627147793769836\n",
      "Batch loss: 0.23384207487106323\n",
      "Batch loss: 0.09389803558588028\n",
      "Batch loss: 0.1737353354692459\n",
      "Batch loss: 0.13489070534706116\n",
      "Batch loss: 0.19051267206668854\n",
      "Batch loss: 0.10799809545278549\n",
      "Batch loss: 0.07829099893569946\n",
      "Batch loss: 0.273761123418808\n",
      "Batch loss: 0.1821672022342682\n",
      "Batch loss: 0.14767515659332275\n",
      "Batch loss: 0.18757811188697815\n",
      "Batch loss: 0.16577531397342682\n",
      "Batch loss: 0.13701416552066803\n",
      "Batch loss: 0.1117868423461914\n",
      "Batch loss: 0.1493222862482071\n",
      "Batch loss: 0.12151198834180832\n",
      "Batch loss: 0.17320701479911804\n",
      "Batch loss: 0.14019615948200226\n",
      "Batch loss: 0.13704058527946472\n",
      "Batch loss: 0.13250114023685455\n",
      "Batch loss: 0.0989232137799263\n",
      "Batch loss: 0.13695412874221802\n",
      "Batch loss: 0.15205831825733185\n",
      "Batch loss: 0.10609827935695648\n",
      "Batch loss: 0.19865652918815613\n",
      "Batch loss: 0.2130347490310669\n",
      "Batch loss: 0.11510571092367172\n",
      "Batch loss: 0.0967792347073555\n",
      "Batch loss: 0.20740646123886108\n",
      "Batch loss: 0.242160364985466\n",
      "Batch loss: 0.2905740439891815\n",
      "Batch loss: 0.1704578995704651\n",
      "Batch loss: 0.1111687570810318\n",
      "Batch loss: 0.11932402849197388\n",
      "Batch loss: 0.10720323771238327\n",
      "Batch loss: 0.13340488076210022\n",
      "Batch loss: 0.16337065398693085\n",
      "Batch loss: 0.18534041941165924\n",
      "Batch loss: 0.150494784116745\n",
      "Batch loss: 0.1032920852303505\n",
      "Batch loss: 0.16172829270362854\n",
      "Batch loss: 0.11502929031848907\n",
      "Batch loss: 0.0624086819589138\n",
      "Batch loss: 0.1436450034379959\n",
      "Batch loss: 0.11210442334413528\n",
      "Batch loss: 0.11833551526069641\n",
      "Batch loss: 0.0839429423213005\n",
      "Batch loss: 0.2695434093475342\n",
      "Batch loss: 0.10496414452791214\n",
      "Batch loss: 0.21723297238349915\n",
      "Batch loss: 0.21343758702278137\n",
      "Batch loss: 0.16384181380271912\n",
      "Batch loss: 0.2933619022369385\n",
      "Batch loss: 0.14755018055438995\n",
      "Batch loss: 0.23206134140491486\n",
      "Batch loss: 0.12804649770259857\n",
      "Batch loss: 0.1267394870519638\n",
      "Batch loss: 0.16302432119846344\n",
      "Batch loss: 0.1037951335310936\n",
      "Batch loss: 0.19386184215545654\n",
      "Batch loss: 0.1464964747428894\n",
      "Batch loss: 0.19630233943462372\n",
      "Batch loss: 0.14032630622386932\n",
      "Batch loss: 0.15220798552036285\n",
      "Batch loss: 0.2083633989095688\n",
      "Batch loss: 0.1512080430984497\n",
      "Batch loss: 0.09663461893796921\n",
      "Batch loss: 0.12427614629268646\n",
      "Batch loss: 0.14587916433811188\n",
      "Batch loss: 0.13852810859680176\n",
      "Batch loss: 0.18749892711639404\n",
      "Batch loss: 0.23914605379104614\n",
      "Batch loss: 0.13947926461696625\n",
      "Batch loss: 0.1332247108221054\n",
      "Batch loss: 0.1669483482837677\n",
      "Batch loss: 0.1806868612766266\n",
      "Batch loss: 0.32973334193229675\n",
      "Batch loss: 0.2531031370162964\n",
      "Batch loss: 0.15889541804790497\n",
      "Batch loss: 0.2366316169500351\n",
      "Batch loss: 0.18245171010494232\n",
      "Batch loss: 0.1924121081829071\n",
      "Batch loss: 0.18223218619823456\n",
      "Batch loss: 0.14961564540863037\n",
      "Batch loss: 0.17270773649215698\n",
      "Batch loss: 0.17372822761535645\n",
      "Batch loss: 0.1580103635787964\n",
      "Batch loss: 0.11112980544567108\n",
      "Batch loss: 0.15827523171901703\n",
      "Batch loss: 0.1103157326579094\n",
      "Batch loss: 0.19878718256950378\n",
      "Batch loss: 0.12978754937648773\n",
      "Batch loss: 0.1427340805530548\n",
      "Batch loss: 0.1267516314983368\n",
      "Batch loss: 0.1574241667985916\n",
      "Batch loss: 0.1228349357843399\n",
      "Batch loss: 0.12220724672079086\n",
      "Batch loss: 0.12475559115409851\n",
      "Batch loss: 0.20227296650409698\n",
      "Batch loss: 0.11956856399774551\n",
      "Batch loss: 0.11517643183469772\n",
      "Batch loss: 0.1800345927476883\n",
      "Batch loss: 0.11053508520126343\n",
      "Batch loss: 0.18016962707042694\n",
      "Batch loss: 0.10873793065547943\n",
      "Batch loss: 0.07650116086006165\n",
      "Batch loss: 0.2250065952539444\n",
      "Batch loss: 0.31876444816589355\n",
      "Batch loss: 0.20218142867088318\n",
      "Batch loss: 0.1615874469280243\n",
      "Batch loss: 0.1466878205537796\n",
      "Batch loss: 0.07161922007799149\n",
      "Batch loss: 0.09227264672517776\n",
      "Batch loss: 0.0983307808637619\n",
      "Batch loss: 0.12011315673589706\n",
      "Batch loss: 0.132261723279953\n",
      "Batch loss: 0.15819886326789856\n",
      "Batch loss: 0.15241968631744385\n",
      "Batch loss: 0.16329587996006012\n",
      "Batch loss: 0.23554614186286926\n",
      "Batch loss: 0.1900445818901062\n",
      "Batch loss: 0.12464901804924011\n",
      "Batch loss: 0.16727080941200256\n",
      "Batch loss: 0.10933886468410492\n",
      "Batch loss: 0.1592330038547516\n",
      "Batch loss: 0.1143866553902626\n",
      "Batch loss: 0.13625390827655792\n",
      "Batch loss: 0.11444824934005737\n",
      "Batch loss: 0.19711783528327942\n",
      "Batch loss: 0.09915395081043243\n",
      "Batch loss: 0.06744670122861862\n",
      "Batch loss: 0.10386518388986588\n",
      "Batch loss: 0.1066620945930481\n",
      "Batch loss: 0.2166978120803833\n",
      "Batch loss: 0.14829294383525848\n",
      "Batch loss: 0.11469671875238419\n",
      "Batch loss: 0.13229818642139435\n",
      "Batch loss: 0.1666574478149414\n",
      "Batch loss: 0.17918643355369568\n",
      "Batch loss: 0.17189937829971313\n",
      "Batch loss: 0.15585878491401672\n",
      "Batch loss: 0.12483148276805878\n",
      "Batch loss: 0.13559818267822266\n",
      "Batch loss: 0.13140743970870972\n",
      "Batch loss: 0.11862162500619888\n",
      "Batch loss: 0.12884119153022766\n",
      "Batch loss: 0.11618298292160034\n",
      "Batch loss: 0.15851491689682007\n",
      "Batch loss: 0.11426813155412674\n",
      "Batch loss: 0.17149032652378082\n",
      "Batch loss: 0.141969233751297\n",
      "Batch loss: 0.2604808509349823\n",
      "Batch loss: 0.14049723744392395\n",
      "Batch loss: 0.17534039914608002\n",
      "Batch loss: 0.14781561493873596\n",
      "Batch loss: 0.20132438838481903\n",
      "Batch loss: 0.07822056114673615\n",
      "Batch loss: 0.11551062762737274\n",
      "Batch loss: 0.19336555898189545\n",
      "Batch loss: 0.13981109857559204\n",
      "Batch loss: 0.04285474494099617\n",
      "Batch loss: 0.08390376716852188\n",
      "Batch loss: 0.17580950260162354\n",
      "Batch loss: 0.08268899470567703\n",
      "Batch loss: 0.18176263570785522\n",
      "Batch loss: 0.10760997980833054\n",
      "Batch loss: 0.11675179749727249\n",
      "Batch loss: 0.16612403094768524\n",
      "Batch loss: 0.09251422435045242\n",
      "Batch loss: 0.17072026431560516\n",
      "Batch loss: 0.1568712741136551\n",
      "Batch loss: 0.18953609466552734\n",
      "Batch loss: 0.1576576977968216\n",
      "Batch loss: 0.12928622961044312\n",
      "Batch loss: 0.13109290599822998\n",
      "Batch loss: 0.17927753925323486\n",
      "Batch loss: 0.05805586278438568\n",
      "Batch loss: 0.09562671929597855\n",
      "Batch loss: 0.13020680844783783\n",
      "Batch loss: 0.1601930409669876\n",
      "Batch loss: 0.0639408528804779\n",
      "Batch loss: 0.2620093822479248\n",
      "Batch loss: 0.2346057891845703\n",
      "Batch loss: 0.16901445388793945\n",
      "Batch loss: 0.20027856528759003\n",
      "Batch loss: 0.11529027670621872\n",
      "Batch loss: 0.13261005282402039\n",
      "Batch loss: 0.2147081196308136\n",
      "Batch loss: 0.18623551726341248\n",
      "Batch loss: 0.14208559691905975\n",
      "Batch loss: 0.17521169781684875\n",
      "Batch loss: 0.1554344892501831\n",
      "Batch loss: 0.08826277405023575\n",
      "Batch loss: 0.20479533076286316\n",
      "Batch loss: 0.12064889818429947\n",
      "Batch loss: 0.17987783253192902\n",
      "Batch loss: 0.14615589380264282\n",
      "Batch loss: 0.10418897122144699\n",
      "Batch loss: 0.18869997560977936\n",
      "Batch loss: 0.12625817954540253\n",
      "Batch loss: 0.16529715061187744\n",
      "Batch loss: 0.07975373417139053\n",
      "Batch loss: 0.1815217286348343\n",
      "Batch loss: 0.2061876803636551\n",
      "Batch loss: 0.33246004581451416\n",
      "Batch loss: 0.1147594004869461\n",
      "Batch loss: 0.1182364672422409\n",
      "Batch loss: 0.1472836434841156\n",
      "Batch loss: 0.07593011856079102\n",
      "Batch loss: 0.12463190406560898\n",
      "Batch loss: 0.1735437661409378\n",
      "Batch loss: 0.1378851979970932\n",
      "Batch loss: 0.1485259234905243\n",
      "Batch loss: 0.16970598697662354\n",
      "Batch loss: 0.10311167687177658\n",
      "Batch loss: 0.17069514095783234\n",
      "Batch loss: 0.0755862221121788\n",
      "Batch loss: 0.11714023351669312\n",
      "Batch loss: 0.22523881494998932\n",
      "Batch loss: 0.20302385091781616\n",
      "Batch loss: 0.0955134704709053\n",
      "Batch loss: 0.1403781771659851\n",
      "Batch loss: 0.11872119456529617\n",
      "Batch loss: 0.11676131933927536\n",
      "Batch loss: 0.11008858680725098\n",
      "Batch loss: 0.11443394422531128\n",
      "Batch loss: 0.3113615810871124\n",
      "Batch loss: 0.13216310739517212\n",
      "Batch loss: 0.11109932512044907\n",
      "Batch loss: 0.24361436069011688\n",
      "Batch loss: 0.18293936550617218\n",
      "Batch loss: 0.11147864162921906\n",
      "Batch loss: 0.10655336081981659\n",
      "Batch loss: 0.28845837712287903\n",
      "Batch loss: 0.14095772802829742\n",
      "Batch loss: 0.15156115591526031\n",
      "Batch loss: 0.1166747510433197\n",
      "Batch loss: 0.09709787368774414\n",
      "Batch loss: 0.1470191329717636\n",
      "Batch loss: 0.1315680891275406\n",
      "Batch loss: 0.06675312668085098\n",
      "Epoch [3/100], Loss: 0.1526\n",
      "Validation Loss: 0.4396, Accuracy: 80.22%\n",
      "Batch loss: 0.21968767046928406\n",
      "Batch loss: 0.14282220602035522\n",
      "Batch loss: 0.19927430152893066\n",
      "Batch loss: 0.17532770335674286\n",
      "Batch loss: 0.165559783577919\n",
      "Batch loss: 0.16064681112766266\n",
      "Batch loss: 0.29168105125427246\n",
      "Batch loss: 0.18394993245601654\n",
      "Batch loss: 0.2042270302772522\n",
      "Batch loss: 0.08176231384277344\n",
      "Batch loss: 0.16099652647972107\n",
      "Batch loss: 0.15269723534584045\n",
      "Batch loss: 0.1257612258195877\n",
      "Batch loss: 0.19958576560020447\n",
      "Batch loss: 0.20452004671096802\n",
      "Batch loss: 0.13937580585479736\n",
      "Batch loss: 0.12972916662693024\n",
      "Batch loss: 0.17829902470111847\n",
      "Batch loss: 0.08953194320201874\n",
      "Batch loss: 0.17992250621318817\n",
      "Batch loss: 0.1419498771429062\n",
      "Batch loss: 0.12023124098777771\n",
      "Batch loss: 0.10491830110549927\n",
      "Batch loss: 0.10565894842147827\n",
      "Batch loss: 0.122714102268219\n",
      "Batch loss: 0.12157134711742401\n",
      "Batch loss: 0.19730396568775177\n",
      "Batch loss: 0.11818204820156097\n",
      "Batch loss: 0.08347645401954651\n",
      "Batch loss: 0.12310202419757843\n",
      "Batch loss: 0.26338252425193787\n",
      "Batch loss: 0.15729175508022308\n",
      "Batch loss: 0.1471468061208725\n",
      "Batch loss: 0.09166447818279266\n",
      "Batch loss: 0.11434859037399292\n",
      "Batch loss: 0.20094385743141174\n",
      "Batch loss: 0.09993210434913635\n",
      "Batch loss: 0.11846915632486343\n",
      "Batch loss: 0.19688063859939575\n",
      "Batch loss: 0.08700300008058548\n",
      "Batch loss: 0.09015654027462006\n",
      "Batch loss: 0.11706431210041046\n",
      "Batch loss: 0.14290477335453033\n",
      "Batch loss: 0.1036064624786377\n",
      "Batch loss: 0.14721590280532837\n",
      "Batch loss: 0.1288386881351471\n",
      "Batch loss: 0.15422704815864563\n",
      "Batch loss: 0.13156819343566895\n",
      "Batch loss: 0.06422048062086105\n",
      "Batch loss: 0.10892380774021149\n",
      "Batch loss: 0.15326115489006042\n",
      "Batch loss: 0.12523166835308075\n",
      "Batch loss: 0.09518920630216599\n",
      "Batch loss: 0.12116599828004837\n",
      "Batch loss: 0.09377142786979675\n",
      "Batch loss: 0.14046405255794525\n",
      "Batch loss: 0.07206965982913971\n",
      "Batch loss: 0.11379987746477127\n",
      "Batch loss: 0.09735357016324997\n",
      "Batch loss: 0.07642778754234314\n",
      "Batch loss: 0.14345701038837433\n",
      "Batch loss: 0.11711376905441284\n",
      "Batch loss: 0.16106589138507843\n",
      "Batch loss: 0.20770466327667236\n",
      "Batch loss: 0.07016171514987946\n",
      "Batch loss: 0.1645277440547943\n",
      "Batch loss: 0.14683249592781067\n",
      "Batch loss: 0.18144208192825317\n",
      "Batch loss: 0.11828658729791641\n",
      "Batch loss: 0.13529551029205322\n",
      "Batch loss: 0.12675991654396057\n",
      "Batch loss: 0.17048607766628265\n",
      "Batch loss: 0.09963959455490112\n",
      "Batch loss: 0.07673285156488419\n",
      "Batch loss: 0.06735651195049286\n",
      "Batch loss: 0.18186195194721222\n",
      "Batch loss: 0.1502283662557602\n",
      "Batch loss: 0.1732953041791916\n",
      "Batch loss: 0.092000812292099\n",
      "Batch loss: 0.16341818869113922\n",
      "Batch loss: 0.05475137382745743\n",
      "Batch loss: 0.08909167349338531\n",
      "Batch loss: 0.18793678283691406\n",
      "Batch loss: 0.23702950775623322\n",
      "Batch loss: 0.14969834685325623\n",
      "Batch loss: 0.11790268868207932\n",
      "Batch loss: 0.06942233443260193\n",
      "Batch loss: 0.1427460014820099\n",
      "Batch loss: 0.1327880471944809\n",
      "Batch loss: 0.10851061344146729\n",
      "Batch loss: 0.17979097366333008\n",
      "Batch loss: 0.07368931174278259\n",
      "Batch loss: 0.19965755939483643\n",
      "Batch loss: 0.10214248299598694\n",
      "Batch loss: 0.11688799411058426\n",
      "Batch loss: 0.09523125737905502\n",
      "Batch loss: 0.0914887860417366\n",
      "Batch loss: 0.1668824404478073\n",
      "Batch loss: 0.1618782877922058\n",
      "Batch loss: 0.13232341408729553\n",
      "Batch loss: 0.1214996799826622\n",
      "Batch loss: 0.07378987967967987\n",
      "Batch loss: 0.1650889813899994\n",
      "Batch loss: 0.09897130727767944\n",
      "Batch loss: 0.090644970536232\n",
      "Batch loss: 0.1544722616672516\n",
      "Batch loss: 0.1459999978542328\n",
      "Batch loss: 0.15113465487957\n",
      "Batch loss: 0.11556785553693771\n",
      "Batch loss: 0.17616064846515656\n",
      "Batch loss: 0.11805658042430878\n",
      "Batch loss: 0.07021671533584595\n",
      "Batch loss: 0.052750855684280396\n",
      "Batch loss: 0.09680061042308807\n",
      "Batch loss: 0.1325116753578186\n",
      "Batch loss: 0.18244490027427673\n",
      "Batch loss: 0.09183898568153381\n",
      "Batch loss: 0.042698636651039124\n",
      "Batch loss: 0.06334956735372543\n",
      "Batch loss: 0.1226072832942009\n",
      "Batch loss: 0.10223394632339478\n",
      "Batch loss: 0.09817325323820114\n",
      "Batch loss: 0.1532898098230362\n",
      "Batch loss: 0.08660043030977249\n",
      "Batch loss: 0.12680570781230927\n",
      "Batch loss: 0.23568212985992432\n",
      "Batch loss: 0.09491929411888123\n",
      "Batch loss: 0.12328843772411346\n",
      "Batch loss: 0.10684525221586227\n",
      "Batch loss: 0.15195994079113007\n",
      "Batch loss: 0.13876453042030334\n",
      "Batch loss: 0.13372302055358887\n",
      "Batch loss: 0.15094996988773346\n",
      "Batch loss: 0.12741148471832275\n",
      "Batch loss: 0.18648089468479156\n",
      "Batch loss: 0.09839892387390137\n",
      "Batch loss: 0.07904008030891418\n",
      "Batch loss: 0.13078536093235016\n",
      "Batch loss: 0.16799961030483246\n",
      "Batch loss: 0.10007753223180771\n",
      "Batch loss: 0.07006838172674179\n",
      "Batch loss: 0.08126194775104523\n",
      "Batch loss: 0.1214369460940361\n",
      "Batch loss: 0.15278111398220062\n",
      "Batch loss: 0.1369563639163971\n",
      "Batch loss: 0.16947318613529205\n",
      "Batch loss: 0.17797423899173737\n",
      "Batch loss: 0.14156195521354675\n",
      "Batch loss: 0.12188487499952316\n",
      "Batch loss: 0.14094774425029755\n",
      "Batch loss: 0.14671313762664795\n",
      "Batch loss: 0.17168711125850677\n",
      "Batch loss: 0.21353237330913544\n",
      "Batch loss: 0.21949288249015808\n",
      "Batch loss: 0.15294012427330017\n",
      "Batch loss: 0.16025377810001373\n",
      "Batch loss: 0.16940085589885712\n",
      "Batch loss: 0.12391062080860138\n",
      "Batch loss: 0.12211434543132782\n",
      "Batch loss: 0.19514034688472748\n",
      "Batch loss: 0.06716353446245193\n",
      "Batch loss: 0.16345351934432983\n",
      "Batch loss: 0.1565748155117035\n",
      "Batch loss: 0.1460544317960739\n",
      "Batch loss: 0.08375296741724014\n",
      "Batch loss: 0.21118761599063873\n",
      "Batch loss: 0.15741223096847534\n",
      "Batch loss: 0.14642676711082458\n",
      "Batch loss: 0.09248670935630798\n",
      "Batch loss: 0.1559177041053772\n",
      "Batch loss: 0.16993844509124756\n",
      "Batch loss: 0.18924139440059662\n",
      "Batch loss: 0.10681944340467453\n",
      "Batch loss: 0.18479305505752563\n",
      "Batch loss: 0.1359206885099411\n",
      "Batch loss: 0.1658143699169159\n",
      "Batch loss: 0.20128895342350006\n",
      "Batch loss: 0.11215591430664062\n",
      "Batch loss: 0.1670844554901123\n",
      "Batch loss: 0.16242578625679016\n",
      "Batch loss: 0.14143136143684387\n",
      "Batch loss: 0.07999906688928604\n",
      "Batch loss: 0.07708512991666794\n",
      "Batch loss: 0.22093616425991058\n",
      "Batch loss: 0.09689324349164963\n",
      "Batch loss: 0.11560803651809692\n",
      "Batch loss: 0.18331687152385712\n",
      "Batch loss: 0.12349643558263779\n",
      "Batch loss: 0.09929879009723663\n",
      "Batch loss: 0.1400132179260254\n",
      "Batch loss: 0.17144885659217834\n",
      "Batch loss: 0.08504769206047058\n",
      "Batch loss: 0.12643004953861237\n",
      "Batch loss: 0.1719505935907364\n",
      "Batch loss: 0.09872351586818695\n",
      "Batch loss: 0.09498605132102966\n",
      "Batch loss: 0.09614281356334686\n",
      "Batch loss: 0.15908342599868774\n",
      "Batch loss: 0.12355109304189682\n",
      "Batch loss: 0.09237964451313019\n",
      "Batch loss: 0.23532964289188385\n",
      "Batch loss: 0.1782139390707016\n",
      "Batch loss: 0.08237326145172119\n",
      "Batch loss: 0.09393800795078278\n",
      "Batch loss: 0.16294172406196594\n",
      "Batch loss: 0.17765715718269348\n",
      "Batch loss: 0.14487490057945251\n",
      "Batch loss: 0.14726190268993378\n",
      "Batch loss: 0.07354529201984406\n",
      "Batch loss: 0.14430034160614014\n",
      "Batch loss: 0.10707531124353409\n",
      "Batch loss: 0.11509622633457184\n",
      "Batch loss: 0.16310782730579376\n",
      "Batch loss: 0.1299852579832077\n",
      "Batch loss: 0.16082218289375305\n",
      "Batch loss: 0.08016761392354965\n",
      "Batch loss: 0.13425421714782715\n",
      "Batch loss: 0.146162211894989\n",
      "Batch loss: 0.08089552074670792\n",
      "Batch loss: 0.08774416148662567\n",
      "Batch loss: 0.12585587799549103\n",
      "Batch loss: 0.1205577701330185\n",
      "Batch loss: 0.054542724043130875\n",
      "Batch loss: 0.18722276389598846\n",
      "Batch loss: 0.10410581529140472\n",
      "Batch loss: 0.17926695942878723\n",
      "Batch loss: 0.08774299174547195\n",
      "Batch loss: 0.1497167944908142\n",
      "Batch loss: 0.27088281512260437\n",
      "Batch loss: 0.09971772879362106\n",
      "Batch loss: 0.2031112164258957\n",
      "Batch loss: 0.08361931890249252\n",
      "Batch loss: 0.108319491147995\n",
      "Batch loss: 0.10869251191616058\n",
      "Batch loss: 0.08347752690315247\n",
      "Batch loss: 0.18632614612579346\n",
      "Batch loss: 0.12024695426225662\n",
      "Batch loss: 0.1997886449098587\n",
      "Batch loss: 0.16013024747371674\n",
      "Batch loss: 0.231239452958107\n",
      "Batch loss: 0.15283314883708954\n",
      "Batch loss: 0.15080301463603973\n",
      "Batch loss: 0.0920943170785904\n",
      "Batch loss: 0.13240723311901093\n",
      "Batch loss: 0.10793903470039368\n",
      "Batch loss: 0.10305412858724594\n",
      "Batch loss: 0.13332675397396088\n",
      "Batch loss: 0.17678862810134888\n",
      "Batch loss: 0.13358382880687714\n",
      "Batch loss: 0.13372832536697388\n",
      "Batch loss: 0.19528278708457947\n",
      "Batch loss: 0.18419785797595978\n",
      "Batch loss: 0.23348769545555115\n",
      "Batch loss: 0.2186669409275055\n",
      "Batch loss: 0.16288866102695465\n",
      "Batch loss: 0.29626092314720154\n",
      "Batch loss: 0.15678851306438446\n",
      "Batch loss: 0.21033309400081635\n",
      "Batch loss: 0.13577523827552795\n",
      "Batch loss: 0.11058975756168365\n",
      "Batch loss: 0.13090108335018158\n",
      "Batch loss: 0.15478238463401794\n",
      "Batch loss: 0.13983812928199768\n",
      "Batch loss: 0.13605771958827972\n",
      "Batch loss: 0.09337776899337769\n",
      "Batch loss: 0.09784924238920212\n",
      "Batch loss: 0.13429108262062073\n",
      "Batch loss: 0.13107915222644806\n",
      "Batch loss: 0.1326940655708313\n",
      "Batch loss: 0.10970806330442429\n",
      "Batch loss: 0.17000015079975128\n",
      "Batch loss: 0.11715680360794067\n",
      "Batch loss: 0.09849891066551208\n",
      "Batch loss: 0.11972010135650635\n",
      "Batch loss: 0.18240471184253693\n",
      "Batch loss: 0.10506059229373932\n",
      "Batch loss: 0.09274891018867493\n",
      "Batch loss: 0.1820000261068344\n",
      "Batch loss: 0.10822249203920364\n",
      "Batch loss: 0.15914663672447205\n",
      "Batch loss: 0.11547411233186722\n",
      "Batch loss: 0.054095104336738586\n",
      "Batch loss: 0.20892472565174103\n",
      "Batch loss: 0.2589671015739441\n",
      "Batch loss: 0.1280173361301422\n",
      "Batch loss: 0.15352480113506317\n",
      "Batch loss: 0.10149084776639938\n",
      "Batch loss: 0.07086741179227829\n",
      "Batch loss: 0.09631331264972687\n",
      "Batch loss: 0.06480620801448822\n",
      "Batch loss: 0.07745598256587982\n",
      "Batch loss: 0.09808347374200821\n",
      "Batch loss: 0.10658691823482513\n",
      "Batch loss: 0.12133707851171494\n",
      "Batch loss: 0.12358973920345306\n",
      "Batch loss: 0.2019282430410385\n",
      "Batch loss: 0.08395366370677948\n",
      "Batch loss: 0.09347411245107651\n",
      "Batch loss: 0.1737385243177414\n",
      "Batch loss: 0.10006322711706161\n",
      "Batch loss: 0.11522136628627777\n",
      "Batch loss: 0.0898384153842926\n",
      "Batch loss: 0.14599844813346863\n",
      "Batch loss: 0.06850928068161011\n",
      "Batch loss: 0.17380975186824799\n",
      "Batch loss: 0.10472077876329422\n",
      "Batch loss: 0.046933483332395554\n",
      "Batch loss: 0.0683426782488823\n",
      "Batch loss: 0.060875046998262405\n",
      "Batch loss: 0.11595846712589264\n",
      "Batch loss: 0.12988656759262085\n",
      "Batch loss: 0.09099053591489792\n",
      "Batch loss: 0.09678810834884644\n",
      "Batch loss: 0.12605541944503784\n",
      "Batch loss: 0.1133001372218132\n",
      "Batch loss: 0.19887085258960724\n",
      "Batch loss: 0.14920662343502045\n",
      "Batch loss: 0.09544333070516586\n",
      "Batch loss: 0.19715410470962524\n",
      "Batch loss: 0.12421482056379318\n",
      "Batch loss: 0.12375090271234512\n",
      "Batch loss: 0.10511496663093567\n",
      "Batch loss: 0.11028099805116653\n",
      "Batch loss: 0.20547336339950562\n",
      "Batch loss: 0.08022481203079224\n",
      "Batch loss: 0.16257710754871368\n",
      "Batch loss: 0.14944195747375488\n",
      "Batch loss: 0.23677074909210205\n",
      "Batch loss: 0.10945440828800201\n",
      "Batch loss: 0.10124010592699051\n",
      "Batch loss: 0.12170656770467758\n",
      "Batch loss: 0.17527227103710175\n",
      "Batch loss: 0.06862036883831024\n",
      "Batch loss: 0.10352537781000137\n",
      "Batch loss: 0.16217868030071259\n",
      "Batch loss: 0.14724794030189514\n",
      "Batch loss: 0.030604761093854904\n",
      "Batch loss: 0.07315689325332642\n",
      "Batch loss: 0.126600444316864\n",
      "Batch loss: 0.07811440527439117\n",
      "Batch loss: 0.12533171474933624\n",
      "Batch loss: 0.09518681466579437\n",
      "Batch loss: 0.1056709885597229\n",
      "Batch loss: 0.08904831856489182\n",
      "Batch loss: 0.07812686264514923\n",
      "Batch loss: 0.1733856350183487\n",
      "Batch loss: 0.17728891968727112\n",
      "Batch loss: 0.20768830180168152\n",
      "Batch loss: 0.09645944088697433\n",
      "Batch loss: 0.061436258256435394\n",
      "Batch loss: 0.12198542803525925\n",
      "Batch loss: 0.13660010695457458\n",
      "Batch loss: 0.062100160866975784\n",
      "Batch loss: 0.08535013347864151\n",
      "Batch loss: 0.11204198002815247\n",
      "Batch loss: 0.10371717065572739\n",
      "Batch loss: 0.098506398499012\n",
      "Batch loss: 0.21590682864189148\n",
      "Batch loss: 0.18866893649101257\n",
      "Batch loss: 0.18731789290905\n",
      "Batch loss: 0.0837266594171524\n",
      "Batch loss: 0.08817420899868011\n",
      "Batch loss: 0.09520868211984634\n",
      "Batch loss: 0.14445915818214417\n",
      "Batch loss: 0.11745326966047287\n",
      "Batch loss: 0.12233710289001465\n",
      "Batch loss: 0.1579248607158661\n",
      "Batch loss: 0.10106450319290161\n",
      "Batch loss: 0.1085955798625946\n",
      "Batch loss: 0.19302129745483398\n",
      "Batch loss: 0.07258005440235138\n",
      "Batch loss: 0.09493976831436157\n",
      "Batch loss: 0.12059338390827179\n",
      "Batch loss: 0.07531416416168213\n",
      "Batch loss: 0.12351348251104355\n",
      "Batch loss: 0.10452590882778168\n",
      "Batch loss: 0.12102469801902771\n",
      "Batch loss: 0.07863212376832962\n",
      "Batch loss: 0.1858682483434677\n",
      "Batch loss: 0.16399236023426056\n",
      "Batch loss: 0.28013941645622253\n",
      "Batch loss: 0.09412636607885361\n",
      "Batch loss: 0.1012686938047409\n",
      "Batch loss: 0.14500704407691956\n",
      "Batch loss: 0.07408042997121811\n",
      "Batch loss: 0.13746939599514008\n",
      "Batch loss: 0.13250260055065155\n",
      "Batch loss: 0.10949713736772537\n",
      "Batch loss: 0.07641252875328064\n",
      "Batch loss: 0.1336158812046051\n",
      "Batch loss: 0.09746867418289185\n",
      "Batch loss: 0.11039990186691284\n",
      "Batch loss: 0.07795310020446777\n",
      "Batch loss: 0.14041784405708313\n",
      "Batch loss: 0.14464570581912994\n",
      "Batch loss: 0.14451245963573456\n",
      "Batch loss: 0.06347119063138962\n",
      "Batch loss: 0.0879926085472107\n",
      "Batch loss: 0.08481679856777191\n",
      "Batch loss: 0.07432030886411667\n",
      "Batch loss: 0.13981662690639496\n",
      "Batch loss: 0.07470929622650146\n",
      "Batch loss: 0.26398298144340515\n",
      "Batch loss: 0.11033431440591812\n",
      "Batch loss: 0.09715178608894348\n",
      "Batch loss: 0.180500790476799\n",
      "Batch loss: 0.13301466405391693\n",
      "Batch loss: 0.11242295056581497\n",
      "Batch loss: 0.08746613562107086\n",
      "Batch loss: 0.17911221086978912\n",
      "Batch loss: 0.09577035158872604\n",
      "Batch loss: 0.10412707924842834\n",
      "Batch loss: 0.054872602224349976\n",
      "Batch loss: 0.0638100728392601\n",
      "Batch loss: 0.12107956409454346\n",
      "Batch loss: 0.089148610830307\n",
      "Batch loss: 0.07667171210050583\n",
      "Epoch [4/100], Loss: 0.1313\n",
      "Validation Loss: 0.7298, Accuracy: 88.81%\n",
      "Batch loss: 0.15876425802707672\n",
      "Batch loss: 0.1578436940908432\n",
      "Batch loss: 0.15103580057621002\n",
      "Batch loss: 0.163081556558609\n",
      "Batch loss: 0.12436170876026154\n",
      "Batch loss: 0.19013158977031708\n",
      "Batch loss: 0.3550487458705902\n",
      "Batch loss: 0.16155283153057098\n",
      "Batch loss: 0.16029523313045502\n",
      "Batch loss: 0.10383354872465134\n",
      "Batch loss: 0.1389317512512207\n",
      "Batch loss: 0.13948136568069458\n",
      "Batch loss: 0.1452762484550476\n",
      "Batch loss: 0.22854328155517578\n",
      "Batch loss: 0.15541204810142517\n",
      "Batch loss: 0.11600203812122345\n",
      "Batch loss: 0.13050666451454163\n",
      "Batch loss: 0.21864694356918335\n",
      "Batch loss: 0.06459940969944\n",
      "Batch loss: 0.12685450911521912\n",
      "Batch loss: 0.10559232532978058\n",
      "Batch loss: 0.1303672194480896\n",
      "Batch loss: 0.11932061612606049\n",
      "Batch loss: 0.14011913537979126\n",
      "Batch loss: 0.09988660365343094\n",
      "Batch loss: 0.08977527916431427\n",
      "Batch loss: 0.1756863296031952\n",
      "Batch loss: 0.1489245742559433\n",
      "Batch loss: 0.07885467261075974\n",
      "Batch loss: 0.11657792329788208\n",
      "Batch loss: 0.23373998701572418\n",
      "Batch loss: 0.12110590189695358\n",
      "Batch loss: 0.14227895438671112\n",
      "Batch loss: 0.09649375826120377\n",
      "Batch loss: 0.09518216550350189\n",
      "Batch loss: 0.17994864284992218\n",
      "Batch loss: 0.10282747447490692\n",
      "Batch loss: 0.07870841026306152\n",
      "Batch loss: 0.16306565701961517\n",
      "Batch loss: 0.056849103420972824\n",
      "Batch loss: 0.11534822732210159\n",
      "Batch loss: 0.11527789384126663\n",
      "Batch loss: 0.1285376101732254\n",
      "Batch loss: 0.09685293585062027\n",
      "Batch loss: 0.14293447136878967\n",
      "Batch loss: 0.12179887294769287\n",
      "Batch loss: 0.14485742151737213\n",
      "Batch loss: 0.11673325300216675\n",
      "Batch loss: 0.04552353918552399\n",
      "Batch loss: 0.1269065886735916\n",
      "Batch loss: 0.17607221007347107\n",
      "Batch loss: 0.11998067796230316\n",
      "Batch loss: 0.07893188297748566\n",
      "Batch loss: 0.0916857197880745\n",
      "Batch loss: 0.1011957973241806\n",
      "Batch loss: 0.11047336459159851\n",
      "Batch loss: 0.09990261495113373\n",
      "Batch loss: 0.1128171980381012\n",
      "Batch loss: 0.11200469732284546\n",
      "Batch loss: 0.12533356249332428\n",
      "Batch loss: 0.1010325625538826\n",
      "Batch loss: 0.12124540656805038\n",
      "Batch loss: 0.1639685183763504\n",
      "Batch loss: 0.15447406470775604\n",
      "Batch loss: 0.0593545027077198\n",
      "Batch loss: 0.1367662101984024\n",
      "Batch loss: 0.1379581093788147\n",
      "Batch loss: 0.16964323818683624\n",
      "Batch loss: 0.12836800515651703\n",
      "Batch loss: 0.12478162348270416\n",
      "Batch loss: 0.11743338406085968\n",
      "Batch loss: 0.1318531185388565\n",
      "Batch loss: 0.07851733267307281\n",
      "Batch loss: 0.11372498422861099\n",
      "Batch loss: 0.0601615384221077\n",
      "Batch loss: 0.10551417618989944\n",
      "Batch loss: 0.12210851907730103\n",
      "Batch loss: 0.16115887463092804\n",
      "Batch loss: 0.1071184054017067\n",
      "Batch loss: 0.12868604063987732\n",
      "Batch loss: 0.05923919007182121\n",
      "Batch loss: 0.09004302322864532\n",
      "Batch loss: 0.1333637535572052\n",
      "Batch loss: 0.19220173358917236\n",
      "Batch loss: 0.12360574305057526\n",
      "Batch loss: 0.08164870738983154\n",
      "Batch loss: 0.03798338770866394\n",
      "Batch loss: 0.15609771013259888\n",
      "Batch loss: 0.10971906036138535\n",
      "Batch loss: 0.08666236698627472\n",
      "Batch loss: 0.13311165571212769\n",
      "Batch loss: 0.05905260145664215\n",
      "Batch loss: 0.18501225113868713\n",
      "Batch loss: 0.10948635637760162\n",
      "Batch loss: 0.10493810474872589\n",
      "Batch loss: 0.09812446683645248\n",
      "Batch loss: 0.07902755588293076\n",
      "Batch loss: 0.11042225360870361\n",
      "Batch loss: 0.10723309218883514\n",
      "Batch loss: 0.13225139677524567\n",
      "Batch loss: 0.0920778289437294\n",
      "Batch loss: 0.061571668833494186\n",
      "Batch loss: 0.12383225560188293\n",
      "Batch loss: 0.1371040642261505\n",
      "Batch loss: 0.10078970342874527\n",
      "Batch loss: 0.19761072099208832\n",
      "Batch loss: 0.13616029918193817\n",
      "Batch loss: 0.10370444506406784\n",
      "Batch loss: 0.13145224750041962\n",
      "Batch loss: 0.13342368602752686\n",
      "Batch loss: 0.1110156774520874\n",
      "Batch loss: 0.0648767426609993\n",
      "Batch loss: 0.0671347826719284\n",
      "Batch loss: 0.09806576371192932\n",
      "Batch loss: 0.08976569771766663\n",
      "Batch loss: 0.14032740890979767\n",
      "Batch loss: 0.06233479455113411\n",
      "Batch loss: 0.037135932594537735\n",
      "Batch loss: 0.055498071014881134\n",
      "Batch loss: 0.1536710262298584\n",
      "Batch loss: 0.09405262023210526\n",
      "Batch loss: 0.0882435068488121\n",
      "Batch loss: 0.16877512633800507\n",
      "Batch loss: 0.11048926413059235\n",
      "Batch loss: 0.13446196913719177\n",
      "Batch loss: 0.09242664277553558\n",
      "Batch loss: 0.07234189659357071\n",
      "Batch loss: 0.11282926797866821\n",
      "Batch loss: 0.11110441386699677\n",
      "Batch loss: 0.13637124001979828\n",
      "Batch loss: 0.10873402655124664\n",
      "Batch loss: 0.14795885980129242\n",
      "Batch loss: 0.10734038054943085\n",
      "Batch loss: 0.09545190632343292\n",
      "Batch loss: 0.12066444754600525\n",
      "Batch loss: 0.04084240645170212\n",
      "Batch loss: 0.04824075475335121\n",
      "Batch loss: 0.0750611200928688\n",
      "Batch loss: 0.10955709218978882\n",
      "Batch loss: 0.05673554539680481\n",
      "Batch loss: 0.06801148504018784\n",
      "Batch loss: 0.10295933485031128\n",
      "Batch loss: 0.07883379608392715\n",
      "Batch loss: 0.09430815279483795\n",
      "Batch loss: 0.0835166871547699\n",
      "Batch loss: 0.10207710415124893\n",
      "Batch loss: 0.0867527648806572\n",
      "Batch loss: 0.07395503669977188\n",
      "Batch loss: 0.13864268362522125\n",
      "Batch loss: 0.12054982036352158\n",
      "Batch loss: 0.11204260587692261\n",
      "Batch loss: 0.11540134251117706\n",
      "Batch loss: 0.11675515025854111\n",
      "Batch loss: 0.18347135186195374\n",
      "Batch loss: 0.21460551023483276\n",
      "Batch loss: 0.1805095225572586\n",
      "Batch loss: 0.16139520704746246\n",
      "Batch loss: 0.1485780030488968\n",
      "Batch loss: 0.08213330060243607\n",
      "Batch loss: 0.12080486863851547\n",
      "Batch loss: 0.06819137185811996\n",
      "Batch loss: 0.17501957714557648\n",
      "Batch loss: 0.07659699767827988\n",
      "Batch loss: 0.13634543120861053\n",
      "Batch loss: 0.11744780838489532\n",
      "Batch loss: 0.18301165103912354\n",
      "Batch loss: 0.09744103252887726\n",
      "Batch loss: 0.12076693028211594\n",
      "Batch loss: 0.06999741494655609\n",
      "Batch loss: 0.14388109743595123\n",
      "Batch loss: 0.12383896112442017\n",
      "Batch loss: 0.19425222277641296\n",
      "Batch loss: 0.06434059888124466\n",
      "Batch loss: 0.12049524486064911\n",
      "Batch loss: 0.09736631065607071\n",
      "Batch loss: 0.15696179866790771\n",
      "Batch loss: 0.210206538438797\n",
      "Batch loss: 0.08718828856945038\n",
      "Batch loss: 0.16423821449279785\n",
      "Batch loss: 0.1086939126253128\n",
      "Batch loss: 0.14227043092250824\n",
      "Batch loss: 0.08337025344371796\n",
      "Batch loss: 0.06488563865423203\n",
      "Batch loss: 0.2037588506937027\n",
      "Batch loss: 0.14052221179008484\n",
      "Batch loss: 0.10923634469509125\n",
      "Batch loss: 0.1847575157880783\n",
      "Batch loss: 0.09791065752506256\n",
      "Batch loss: 0.08446324616670609\n",
      "Batch loss: 0.10612903535366058\n",
      "Batch loss: 0.12178383022546768\n",
      "Batch loss: 0.10454440861940384\n",
      "Batch loss: 0.1879965364933014\n",
      "Batch loss: 0.0883215069770813\n",
      "Batch loss: 0.08707533031702042\n",
      "Batch loss: 0.08334027230739594\n",
      "Batch loss: 0.06818094849586487\n",
      "Batch loss: 0.1046953797340393\n",
      "Batch loss: 0.13879743218421936\n",
      "Batch loss: 0.144047349691391\n",
      "Batch loss: 0.16235946118831635\n",
      "Batch loss: 0.24918517470359802\n",
      "Batch loss: 0.08655595779418945\n",
      "Batch loss: 0.12430162727832794\n",
      "Batch loss: 0.20816504955291748\n",
      "Batch loss: 0.17715799808502197\n",
      "Batch loss: 0.12900973856449127\n",
      "Batch loss: 0.1085849404335022\n",
      "Batch loss: 0.0676746591925621\n",
      "Batch loss: 0.09551778435707092\n",
      "Batch loss: 0.05932765454053879\n",
      "Batch loss: 0.11395003646612167\n",
      "Batch loss: 0.1060229241847992\n",
      "Batch loss: 0.14175723493099213\n",
      "Batch loss: 0.1425342708826065\n",
      "Batch loss: 0.08459487557411194\n",
      "Batch loss: 0.11171369254589081\n",
      "Batch loss: 0.08159157633781433\n",
      "Batch loss: 0.047243036329746246\n",
      "Batch loss: 0.08727303147315979\n",
      "Batch loss: 0.04885585233569145\n",
      "Batch loss: 0.108363576233387\n",
      "Batch loss: 0.0904906764626503\n",
      "Batch loss: 0.14710339903831482\n",
      "Batch loss: 0.06622303277254105\n",
      "Batch loss: 0.12656673789024353\n",
      "Batch loss: 0.07433682680130005\n",
      "Batch loss: 0.12656016647815704\n",
      "Batch loss: 0.2488299459218979\n",
      "Batch loss: 0.07956622540950775\n",
      "Batch loss: 0.1666075736284256\n",
      "Batch loss: 0.15364134311676025\n",
      "Batch loss: 0.06995397061109543\n",
      "Batch loss: 0.060731641948223114\n",
      "Batch loss: 0.0860833004117012\n",
      "Batch loss: 0.13992878794670105\n",
      "Batch loss: 0.0764877200126648\n",
      "Batch loss: 0.19125507771968842\n",
      "Batch loss: 0.05156782269477844\n",
      "Batch loss: 0.11463317275047302\n",
      "Batch loss: 0.24101869761943817\n",
      "Batch loss: 0.15519538521766663\n",
      "Batch loss: 0.07174288481473923\n",
      "Batch loss: 0.13395541906356812\n",
      "Batch loss: 0.07574934512376785\n",
      "Batch loss: 0.07526465505361557\n",
      "Batch loss: 0.08476148545742035\n",
      "Batch loss: 0.17201350629329681\n",
      "Batch loss: 0.0851510688662529\n",
      "Batch loss: 0.1203971803188324\n",
      "Batch loss: 0.19664154946804047\n",
      "Batch loss: 0.1658964902162552\n",
      "Batch loss: 0.19889235496520996\n",
      "Batch loss: 0.17428910732269287\n",
      "Batch loss: 0.08519558608531952\n",
      "Batch loss: 0.21207186579704285\n",
      "Batch loss: 0.12659770250320435\n",
      "Batch loss: 0.19238345324993134\n",
      "Batch loss: 0.0675814300775528\n",
      "Batch loss: 0.11693697422742844\n",
      "Batch loss: 0.12250987440347672\n",
      "Batch loss: 0.14946961402893066\n",
      "Batch loss: 0.09689538925886154\n",
      "Batch loss: 0.08158586174249649\n",
      "Batch loss: 0.08574267476797104\n",
      "Batch loss: 0.1396196484565735\n",
      "Batch loss: 0.15671947598457336\n",
      "Batch loss: 0.12366426736116409\n",
      "Batch loss: 0.0987926721572876\n",
      "Batch loss: 0.12272515147924423\n",
      "Batch loss: 0.14176473021507263\n",
      "Batch loss: 0.10755036026239395\n",
      "Batch loss: 0.07427623122930527\n",
      "Batch loss: 0.09356305748224258\n",
      "Batch loss: 0.20547090470790863\n",
      "Batch loss: 0.06751866638660431\n",
      "Batch loss: 0.1047251746058464\n",
      "Batch loss: 0.16627852618694305\n",
      "Batch loss: 0.10469353199005127\n",
      "Batch loss: 0.1481921523809433\n",
      "Batch loss: 0.07209687680006027\n",
      "Batch loss: 0.07077134400606155\n",
      "Batch loss: 0.1813257485628128\n",
      "Batch loss: 0.26749590039253235\n",
      "Batch loss: 0.13052111864089966\n",
      "Batch loss: 0.13763420283794403\n",
      "Batch loss: 0.06616871058940887\n",
      "Batch loss: 0.05409286543726921\n",
      "Batch loss: 0.0834096297621727\n",
      "Batch loss: 0.08107846230268478\n",
      "Batch loss: 0.09430032968521118\n",
      "Batch loss: 0.11028531193733215\n",
      "Batch loss: 0.07438166439533234\n",
      "Batch loss: 0.09017059952020645\n",
      "Batch loss: 0.1268894225358963\n",
      "Batch loss: 0.17697137594223022\n",
      "Batch loss: 0.06314259767532349\n",
      "Batch loss: 0.07327086478471756\n",
      "Batch loss: 0.14195899665355682\n",
      "Batch loss: 0.11320824176073074\n",
      "Batch loss: 0.12933097779750824\n",
      "Batch loss: 0.06027263402938843\n",
      "Batch loss: 0.1026981920003891\n",
      "Batch loss: 0.07383386045694351\n",
      "Batch loss: 0.10061643272638321\n",
      "Batch loss: 0.07771981507539749\n",
      "Batch loss: 0.04534329101443291\n",
      "Batch loss: 0.07290533185005188\n",
      "Batch loss: 0.053929369896650314\n",
      "Batch loss: 0.10146341472864151\n",
      "Batch loss: 0.13467690348625183\n",
      "Batch loss: 0.09757057577371597\n",
      "Batch loss: 0.0978056862950325\n",
      "Batch loss: 0.08530604094266891\n",
      "Batch loss: 0.09426894038915634\n",
      "Batch loss: 0.1471022516489029\n",
      "Batch loss: 0.1286204606294632\n",
      "Batch loss: 0.08661157637834549\n",
      "Batch loss: 0.14849819242954254\n",
      "Batch loss: 0.09247627854347229\n",
      "Batch loss: 0.09485114365816116\n",
      "Batch loss: 0.09433404356241226\n",
      "Batch loss: 0.07456287741661072\n",
      "Batch loss: 0.16248956322669983\n",
      "Batch loss: 0.04947349429130554\n",
      "Batch loss: 0.10674214363098145\n",
      "Batch loss: 0.128706693649292\n",
      "Batch loss: 0.21272476017475128\n",
      "Batch loss: 0.11224481463432312\n",
      "Batch loss: 0.07812961935997009\n",
      "Batch loss: 0.1349346786737442\n",
      "Batch loss: 0.1548052877187729\n",
      "Batch loss: 0.04637446627020836\n",
      "Batch loss: 0.07500278204679489\n",
      "Batch loss: 0.14396458864212036\n",
      "Batch loss: 0.09418937563896179\n",
      "Batch loss: 0.04596300423145294\n",
      "Batch loss: 0.06845953315496445\n",
      "Batch loss: 0.1540057510137558\n",
      "Batch loss: 0.07854899019002914\n",
      "Batch loss: 0.13062192499637604\n",
      "Batch loss: 0.07763200998306274\n",
      "Batch loss: 0.06429598480463028\n",
      "Batch loss: 0.11828657984733582\n",
      "Batch loss: 0.07881873846054077\n",
      "Batch loss: 0.1168174147605896\n",
      "Batch loss: 0.15287479758262634\n",
      "Batch loss: 0.1399773210287094\n",
      "Batch loss: 0.09344217926263809\n",
      "Batch loss: 0.09065950661897659\n",
      "Batch loss: 0.11440365761518478\n",
      "Batch loss: 0.10570529848337173\n",
      "Batch loss: 0.041366495192050934\n",
      "Batch loss: 0.0784742534160614\n",
      "Batch loss: 0.08993974328041077\n",
      "Batch loss: 0.08628609031438828\n",
      "Batch loss: 0.07114240527153015\n",
      "Batch loss: 0.14712867140769958\n",
      "Batch loss: 0.21848201751708984\n",
      "Batch loss: 0.12436355650424957\n",
      "Batch loss: 0.08904498815536499\n",
      "Batch loss: 0.09812577068805695\n",
      "Batch loss: 0.08411870896816254\n",
      "Batch loss: 0.09103916585445404\n",
      "Batch loss: 0.1319093257188797\n",
      "Batch loss: 0.09539201110601425\n",
      "Batch loss: 0.12779973447322845\n",
      "Batch loss: 0.08555780351161957\n",
      "Batch loss: 0.07802395522594452\n",
      "Batch loss: 0.16095563769340515\n",
      "Batch loss: 0.09558021277189255\n",
      "Batch loss: 0.09283459186553955\n",
      "Batch loss: 0.10105067491531372\n",
      "Batch loss: 0.10385362803936005\n",
      "Batch loss: 0.17439517378807068\n",
      "Batch loss: 0.06542667746543884\n",
      "Batch loss: 0.10233093053102493\n",
      "Batch loss: 0.05862221494317055\n",
      "Batch loss: 0.13207149505615234\n",
      "Batch loss: 0.18571975827217102\n",
      "Batch loss: 0.23457911610603333\n",
      "Batch loss: 0.08887197077274323\n",
      "Batch loss: 0.0797313004732132\n",
      "Batch loss: 0.10123156011104584\n",
      "Batch loss: 0.055604491382837296\n",
      "Batch loss: 0.08300547301769257\n",
      "Batch loss: 0.10870762169361115\n",
      "Batch loss: 0.14264638721942902\n",
      "Batch loss: 0.06936045736074448\n",
      "Batch loss: 0.11331233382225037\n",
      "Batch loss: 0.0921974703669548\n",
      "Batch loss: 0.10470935702323914\n",
      "Batch loss: 0.05325908958911896\n",
      "Batch loss: 0.09440670162439346\n",
      "Batch loss: 0.13327044248580933\n",
      "Batch loss: 0.08081559091806412\n",
      "Batch loss: 0.05873291939496994\n",
      "Batch loss: 0.05755797028541565\n",
      "Batch loss: 0.067795529961586\n",
      "Batch loss: 0.07163210213184357\n",
      "Batch loss: 0.08608733862638474\n",
      "Batch loss: 0.05756301060318947\n",
      "Batch loss: 0.203962042927742\n",
      "Batch loss: 0.08117993921041489\n",
      "Batch loss: 0.0632275640964508\n",
      "Batch loss: 0.21608388423919678\n",
      "Batch loss: 0.1351461261510849\n",
      "Batch loss: 0.1003083661198616\n",
      "Batch loss: 0.07102108746767044\n",
      "Batch loss: 0.13667283952236176\n",
      "Batch loss: 0.10371556878089905\n",
      "Batch loss: 0.08314286172389984\n",
      "Batch loss: 0.053715579211711884\n",
      "Batch loss: 0.07810129970312119\n",
      "Batch loss: 0.08341693878173828\n",
      "Batch loss: 0.07599132508039474\n",
      "Batch loss: 0.04736750200390816\n",
      "Epoch [5/100], Loss: 0.1146\n",
      "Validation Loss: 0.3824, Accuracy: 90.99%\n",
      "Batch loss: 0.1446787565946579\n",
      "Batch loss: 0.057349566370248795\n",
      "Batch loss: 0.10925348103046417\n",
      "Batch loss: 0.13059313595294952\n",
      "Batch loss: 0.07660692930221558\n",
      "Batch loss: 0.04551299661397934\n",
      "Batch loss: 0.1885870099067688\n",
      "Batch loss: 0.10991073399782181\n",
      "Batch loss: 0.09737137705087662\n",
      "Batch loss: 0.05985266715288162\n",
      "Batch loss: 0.11519686132669449\n",
      "Batch loss: 0.1137399971485138\n",
      "Batch loss: 0.07564982771873474\n",
      "Batch loss: 0.17002810537815094\n",
      "Batch loss: 0.1287974715232849\n",
      "Batch loss: 0.08803047239780426\n",
      "Batch loss: 0.09889183938503265\n",
      "Batch loss: 0.11206532269716263\n",
      "Batch loss: 0.07757759839296341\n",
      "Batch loss: 0.20791439712047577\n",
      "Batch loss: 0.05995330214500427\n",
      "Batch loss: 0.10966680198907852\n",
      "Batch loss: 0.09352461248636246\n",
      "Batch loss: 0.06852815300226212\n",
      "Batch loss: 0.08976994454860687\n",
      "Batch loss: 0.09657886624336243\n",
      "Batch loss: 0.095058374106884\n",
      "Batch loss: 0.08270299434661865\n",
      "Batch loss: 0.09355974197387695\n",
      "Batch loss: 0.06730348616838455\n",
      "Batch loss: 0.22544530034065247\n",
      "Batch loss: 0.11645562946796417\n",
      "Batch loss: 0.07684315741062164\n",
      "Batch loss: 0.08092362433671951\n",
      "Batch loss: 0.06204596906900406\n",
      "Batch loss: 0.15443386137485504\n",
      "Batch loss: 0.07481830567121506\n",
      "Batch loss: 0.12903057038784027\n",
      "Batch loss: 0.15582416951656342\n",
      "Batch loss: 0.0942627340555191\n",
      "Batch loss: 0.06840398907661438\n",
      "Batch loss: 0.08092375844717026\n",
      "Batch loss: 0.1267997920513153\n",
      "Batch loss: 0.0899822860956192\n",
      "Batch loss: 0.13269007205963135\n",
      "Batch loss: 0.10896016657352448\n",
      "Batch loss: 0.07413724809885025\n",
      "Batch loss: 0.07064278423786163\n",
      "Batch loss: 0.05177326872944832\n",
      "Batch loss: 0.10886790603399277\n",
      "Batch loss: 0.10257668048143387\n",
      "Batch loss: 0.12732623517513275\n",
      "Batch loss: 0.08103986829519272\n",
      "Batch loss: 0.1031685322523117\n",
      "Batch loss: 0.07492393255233765\n",
      "Batch loss: 0.11151783913373947\n",
      "Batch loss: 0.09431745111942291\n",
      "Batch loss: 0.11418964713811874\n",
      "Batch loss: 0.09873411804437637\n",
      "Batch loss: 0.15523271262645721\n",
      "Batch loss: 0.08444970101118088\n",
      "Batch loss: 0.09326466172933578\n",
      "Batch loss: 0.10808291286230087\n",
      "Batch loss: 0.11464296281337738\n",
      "Batch loss: 0.06919077038764954\n",
      "Batch loss: 0.13817553222179413\n",
      "Batch loss: 0.06905130296945572\n",
      "Batch loss: 0.17012938857078552\n",
      "Batch loss: 0.10911177843809128\n",
      "Batch loss: 0.1620682179927826\n",
      "Batch loss: 0.14888571202754974\n",
      "Batch loss: 0.12350624054670334\n",
      "Batch loss: 0.08681945502758026\n",
      "Batch loss: 0.06488767266273499\n",
      "Batch loss: 0.059085436165332794\n",
      "Batch loss: 0.10257041454315186\n",
      "Batch loss: 0.13301730155944824\n",
      "Batch loss: 0.17273762822151184\n",
      "Batch loss: 0.13111132383346558\n",
      "Batch loss: 0.1072511225938797\n",
      "Batch loss: 0.07866434007883072\n",
      "Batch loss: 0.0793980285525322\n",
      "Batch loss: 0.1294499784708023\n",
      "Batch loss: 0.15660113096237183\n",
      "Batch loss: 0.14324314892292023\n",
      "Batch loss: 0.09445128589868546\n",
      "Batch loss: 0.04423932731151581\n",
      "Batch loss: 0.12697967886924744\n",
      "Batch loss: 0.11636205017566681\n",
      "Batch loss: 0.06897348910570145\n",
      "Batch loss: 0.1417335867881775\n",
      "Batch loss: 0.10203035920858383\n",
      "Batch loss: 0.16399435698986053\n",
      "Batch loss: 0.12155227363109589\n",
      "Batch loss: 0.11784528940916061\n",
      "Batch loss: 0.07332096993923187\n",
      "Batch loss: 0.12276538461446762\n",
      "Batch loss: 0.10370434820652008\n",
      "Batch loss: 0.09269591420888901\n",
      "Batch loss: 0.08969657868146896\n",
      "Batch loss: 0.11340543627738953\n",
      "Batch loss: 0.047807302325963974\n",
      "Batch loss: 0.10851286351680756\n",
      "Batch loss: 0.11612467467784882\n",
      "Batch loss: 0.08851183950901031\n",
      "Batch loss: 0.11442974209785461\n",
      "Batch loss: 0.12069124728441238\n",
      "Batch loss: 0.10024738311767578\n",
      "Batch loss: 0.11667319387197495\n",
      "Batch loss: 0.10070127248764038\n",
      "Batch loss: 0.12616361677646637\n",
      "Batch loss: 0.08102276176214218\n",
      "Batch loss: 0.08978922665119171\n",
      "Batch loss: 0.07391676306724548\n",
      "Batch loss: 0.07432154566049576\n",
      "Batch loss: 0.15180377662181854\n",
      "Batch loss: 0.14074961841106415\n",
      "Batch loss: 0.04579688981175423\n",
      "Batch loss: 0.06701939553022385\n",
      "Batch loss: 0.10872796177864075\n",
      "Batch loss: 0.06450263410806656\n",
      "Batch loss: 0.10063565522432327\n",
      "Batch loss: 0.1138887032866478\n",
      "Batch loss: 0.10905905812978745\n",
      "Batch loss: 0.10503128170967102\n",
      "Batch loss: 0.10265924781560898\n",
      "Batch loss: 0.06906620413064957\n",
      "Batch loss: 0.08997547626495361\n",
      "Batch loss: 0.09440089017152786\n",
      "Batch loss: 0.16242791712284088\n",
      "Batch loss: 0.08671029657125473\n",
      "Batch loss: 0.0865231528878212\n",
      "Batch loss: 0.1209995225071907\n",
      "Batch loss: 0.10657034069299698\n",
      "Batch loss: 0.13423119485378265\n",
      "Batch loss: 0.025511430576443672\n",
      "Batch loss: 0.07662676274776459\n",
      "Batch loss: 0.15659166872501373\n",
      "Batch loss: 0.08595626801252365\n",
      "Batch loss: 0.04151969775557518\n",
      "Batch loss: 0.08593054115772247\n",
      "Batch loss: 0.05064886063337326\n",
      "Batch loss: 0.12645387649536133\n",
      "Batch loss: 0.14449378848075867\n",
      "Batch loss: 0.10654132813215256\n",
      "Batch loss: 0.07217470556497574\n",
      "Batch loss: 0.07862768322229385\n",
      "Batch loss: 0.09565954655408859\n",
      "Batch loss: 0.05139440670609474\n",
      "Batch loss: 0.06679149717092514\n",
      "Batch loss: 0.18759191036224365\n",
      "Batch loss: 0.08864369988441467\n",
      "Batch loss: 0.17897672951221466\n",
      "Batch loss: 0.20599088072776794\n",
      "Batch loss: 0.1258680671453476\n",
      "Batch loss: 0.134318545460701\n",
      "Batch loss: 0.1561790406703949\n",
      "Batch loss: 0.10131120681762695\n",
      "Batch loss: 0.07461784780025482\n",
      "Batch loss: 0.10574688762426376\n",
      "Batch loss: 0.045146241784095764\n",
      "Batch loss: 0.16157828271389008\n",
      "Batch loss: 0.06437689065933228\n",
      "Batch loss: 0.11705829203128815\n",
      "Batch loss: 0.08796681463718414\n",
      "Batch loss: 0.07256681472063065\n",
      "Batch loss: 0.10753205418586731\n",
      "Batch loss: 0.10292962193489075\n",
      "Batch loss: 0.08172950893640518\n",
      "Batch loss: 0.07547497004270554\n",
      "Batch loss: 0.1102437824010849\n",
      "Batch loss: 0.18419912457466125\n",
      "Batch loss: 0.0667247548699379\n",
      "Batch loss: 0.14165467023849487\n",
      "Batch loss: 0.057182397693395615\n",
      "Batch loss: 0.12790091335773468\n",
      "Batch loss: 0.15261514484882355\n",
      "Batch loss: 0.08797084540128708\n",
      "Batch loss: 0.15023916959762573\n",
      "Batch loss: 0.10650144517421722\n",
      "Batch loss: 0.08771935850381851\n",
      "Batch loss: 0.07074441015720367\n",
      "Batch loss: 0.0674576535820961\n",
      "Batch loss: 0.17917174100875854\n",
      "Batch loss: 0.13997817039489746\n",
      "Batch loss: 0.14291666448116302\n",
      "Batch loss: 0.09217771887779236\n",
      "Batch loss: 0.10821393132209778\n",
      "Batch loss: 0.09508911520242691\n",
      "Batch loss: 0.10515616834163666\n",
      "Batch loss: 0.10792333632707596\n",
      "Batch loss: 0.08553022891283035\n",
      "Batch loss: 0.07576070725917816\n",
      "Batch loss: 0.10427957028150558\n",
      "Batch loss: 0.08075535297393799\n",
      "Batch loss: 0.08154770731925964\n",
      "Batch loss: 0.05691412836313248\n",
      "Batch loss: 0.07830078154802322\n",
      "Batch loss: 0.10438253730535507\n",
      "Batch loss: 0.07639739662408829\n",
      "Batch loss: 0.20664864778518677\n",
      "Batch loss: 0.12734827399253845\n",
      "Batch loss: 0.07192778587341309\n",
      "Batch loss: 0.05226629972457886\n",
      "Batch loss: 0.1979150027036667\n",
      "Batch loss: 0.12289868295192719\n",
      "Batch loss: 0.10274595767259598\n",
      "Batch loss: 0.06883122026920319\n",
      "Batch loss: 0.06580410152673721\n",
      "Batch loss: 0.12721467018127441\n",
      "Batch loss: 0.05058383569121361\n",
      "Batch loss: 0.09619951248168945\n",
      "Batch loss: 0.08870239555835724\n",
      "Batch loss: 0.16817758977413177\n",
      "Batch loss: 0.07005991786718369\n",
      "Batch loss: 0.12405838817358017\n",
      "Batch loss: 0.10926474630832672\n",
      "Batch loss: 0.0988367348909378\n",
      "Batch loss: 0.054752301424741745\n",
      "Batch loss: 0.0659697949886322\n",
      "Batch loss: 0.06793253868818283\n",
      "Batch loss: 0.05271640047430992\n",
      "Batch loss: 0.04860107973217964\n",
      "Batch loss: 0.14181053638458252\n",
      "Batch loss: 0.05159591883420944\n",
      "Batch loss: 0.10441218316555023\n",
      "Batch loss: 0.05491593852639198\n",
      "Batch loss: 0.10322339832782745\n",
      "Batch loss: 0.16150055825710297\n",
      "Batch loss: 0.07119249552488327\n",
      "Batch loss: 0.0948600322008133\n",
      "Batch loss: 0.05387789383530617\n",
      "Batch loss: 0.08305829018354416\n",
      "Batch loss: 0.1158813014626503\n",
      "Batch loss: 0.07025131583213806\n",
      "Batch loss: 0.1965990513563156\n",
      "Batch loss: 0.09503014385700226\n",
      "Batch loss: 0.103240005671978\n",
      "Batch loss: 0.056637465953826904\n",
      "Batch loss: 0.07907149940729141\n",
      "Batch loss: 0.10939517617225647\n",
      "Batch loss: 0.09994765371084213\n",
      "Batch loss: 0.04186611250042915\n",
      "Batch loss: 0.08016227185726166\n",
      "Batch loss: 0.06570189446210861\n",
      "Batch loss: 0.06477175652980804\n",
      "Batch loss: 0.07928191870450974\n",
      "Batch loss: 0.14486277103424072\n",
      "Batch loss: 0.0657549649477005\n",
      "Batch loss: 0.05021359771490097\n",
      "Batch loss: 0.12005449086427689\n",
      "Batch loss: 0.11168256402015686\n",
      "Batch loss: 0.12724445760250092\n",
      "Batch loss: 0.09003002941608429\n",
      "Batch loss: 0.10205255448818207\n",
      "Batch loss: 0.21224187314510345\n",
      "Batch loss: 0.08669404685497284\n",
      "Batch loss: 0.1625954955816269\n",
      "Batch loss: 0.07342860102653503\n",
      "Batch loss: 0.07780373096466064\n",
      "Batch loss: 0.14523808658123016\n",
      "Batch loss: 0.09616898000240326\n",
      "Batch loss: 0.09978683292865753\n",
      "Batch loss: 0.08124181628227234\n",
      "Batch loss: 0.09341597557067871\n",
      "Batch loss: 0.08711035549640656\n",
      "Batch loss: 0.08614758402109146\n",
      "Batch loss: 0.08320213854312897\n",
      "Batch loss: 0.1167653426527977\n",
      "Batch loss: 0.054831407964229584\n",
      "Batch loss: 0.1547970473766327\n",
      "Batch loss: 0.08592333644628525\n",
      "Batch loss: 0.10797440260648727\n",
      "Batch loss: 0.07932457327842712\n",
      "Batch loss: 0.18699629604816437\n",
      "Batch loss: 0.04725503548979759\n",
      "Batch loss: 0.06376107037067413\n",
      "Batch loss: 0.14272408187389374\n",
      "Batch loss: 0.09581619501113892\n",
      "Batch loss: 0.10909061133861542\n",
      "Batch loss: 0.062200695276260376\n",
      "Batch loss: 0.10590223968029022\n",
      "Batch loss: 0.1301610916852951\n",
      "Batch loss: 0.20146840810775757\n",
      "Batch loss: 0.09850769490003586\n",
      "Batch loss: 0.08870526403188705\n",
      "Batch loss: 0.05998602136969566\n",
      "Batch loss: 0.07574957609176636\n",
      "Batch loss: 0.06948820501565933\n",
      "Batch loss: 0.08973530679941177\n",
      "Batch loss: 0.038059670478105545\n",
      "Batch loss: 0.07479377835988998\n",
      "Batch loss: 0.06254519522190094\n",
      "Batch loss: 0.08730486035346985\n",
      "Batch loss: 0.09429685026407242\n",
      "Batch loss: 0.14982779324054718\n",
      "Batch loss: 0.05261712148785591\n",
      "Batch loss: 0.08679685741662979\n",
      "Batch loss: 0.1375906616449356\n",
      "Batch loss: 0.10632932186126709\n",
      "Batch loss: 0.06008034199476242\n",
      "Batch loss: 0.0460662916302681\n",
      "Batch loss: 0.09114914387464523\n",
      "Batch loss: 0.06959059834480286\n",
      "Batch loss: 0.1565205305814743\n",
      "Batch loss: 0.07159115374088287\n",
      "Batch loss: 0.02593953348696232\n",
      "Batch loss: 0.07897333055734634\n",
      "Batch loss: 0.04112805053591728\n",
      "Batch loss: 0.09846461564302444\n",
      "Batch loss: 0.07625260204076767\n",
      "Batch loss: 0.09624863415956497\n",
      "Batch loss: 0.0599164143204689\n",
      "Batch loss: 0.1745726764202118\n",
      "Batch loss: 0.12649615108966827\n",
      "Batch loss: 0.1273358166217804\n",
      "Batch loss: 0.12134072184562683\n",
      "Batch loss: 0.0649913102388382\n",
      "Batch loss: 0.06671357899904251\n",
      "Batch loss: 0.059026360511779785\n",
      "Batch loss: 0.06985479593276978\n",
      "Batch loss: 0.05713626369833946\n",
      "Batch loss: 0.09916850179433823\n",
      "Batch loss: 0.11557081341743469\n",
      "Batch loss: 0.0848545953631401\n",
      "Batch loss: 0.17971271276474\n",
      "Batch loss: 0.07702656835317612\n",
      "Batch loss: 0.18098098039627075\n",
      "Batch loss: 0.09041374921798706\n",
      "Batch loss: 0.08452298492193222\n",
      "Batch loss: 0.1081373542547226\n",
      "Batch loss: 0.13690760731697083\n",
      "Batch loss: 0.05287229269742966\n",
      "Batch loss: 0.14860521256923676\n",
      "Batch loss: 0.1352079063653946\n",
      "Batch loss: 0.11404668539762497\n",
      "Batch loss: 0.03245450183749199\n",
      "Batch loss: 0.06164971739053726\n",
      "Batch loss: 0.09776844829320908\n",
      "Batch loss: 0.04604184627532959\n",
      "Batch loss: 0.07981392741203308\n",
      "Batch loss: 0.04227711260318756\n",
      "Batch loss: 0.055533599108457565\n",
      "Batch loss: 0.10412322729825974\n",
      "Batch loss: 0.11121951788663864\n",
      "Batch loss: 0.0847066268324852\n",
      "Batch loss: 0.12035364657640457\n",
      "Batch loss: 0.09696995466947556\n",
      "Batch loss: 0.09655573219060898\n",
      "Batch loss: 0.07040030509233475\n",
      "Batch loss: 0.08405013382434845\n",
      "Batch loss: 0.10360321402549744\n",
      "Batch loss: 0.05435076355934143\n",
      "Batch loss: 0.0812145322561264\n",
      "Batch loss: 0.1303711235523224\n",
      "Batch loss: 0.06453639268875122\n",
      "Batch loss: 0.0578850582242012\n",
      "Batch loss: 0.1516633927822113\n",
      "Batch loss: 0.1564646065235138\n",
      "Batch loss: 0.15589214861392975\n",
      "Batch loss: 0.09651690721511841\n",
      "Batch loss: 0.171986386179924\n",
      "Batch loss: 0.1296233981847763\n",
      "Batch loss: 0.11419015377759933\n",
      "Batch loss: 0.15547269582748413\n",
      "Batch loss: 0.0688834860920906\n",
      "Batch loss: 0.1466766744852066\n",
      "Batch loss: 0.08066698163747787\n",
      "Batch loss: 0.08302503824234009\n",
      "Batch loss: 0.13246941566467285\n",
      "Batch loss: 0.06478986144065857\n",
      "Batch loss: 0.09030172973871231\n",
      "Batch loss: 0.10787759721279144\n",
      "Batch loss: 0.06259430944919586\n",
      "Batch loss: 0.0603954941034317\n",
      "Batch loss: 0.1235717311501503\n",
      "Batch loss: 0.07111816108226776\n",
      "Batch loss: 0.0804142877459526\n",
      "Batch loss: 0.13375794887542725\n",
      "Batch loss: 0.15797999501228333\n",
      "Batch loss: 0.18537095189094543\n",
      "Batch loss: 0.065387062728405\n",
      "Batch loss: 0.071473628282547\n",
      "Batch loss: 0.10309792309999466\n",
      "Batch loss: 0.0643080398440361\n",
      "Batch loss: 0.09265722334384918\n",
      "Batch loss: 0.09796344488859177\n",
      "Batch loss: 0.09608694165945053\n",
      "Batch loss: 0.2083214521408081\n",
      "Batch loss: 0.10562613606452942\n",
      "Batch loss: 0.09715502709150314\n",
      "Batch loss: 0.10252276062965393\n",
      "Batch loss: 0.07869971543550491\n",
      "Batch loss: 0.07514947652816772\n",
      "Batch loss: 0.1491248607635498\n",
      "Batch loss: 0.1454710066318512\n",
      "Batch loss: 0.029702937230467796\n",
      "Batch loss: 0.07831927388906479\n",
      "Batch loss: 0.08346647024154663\n",
      "Batch loss: 0.077482670545578\n",
      "Batch loss: 0.07612653821706772\n",
      "Batch loss: 0.04799725487828255\n",
      "Batch loss: 0.1846933811903\n",
      "Batch loss: 0.08465751260519028\n",
      "Batch loss: 0.054805975407361984\n",
      "Batch loss: 0.15097711980342865\n",
      "Batch loss: 0.11686929315328598\n",
      "Batch loss: 0.12587107717990875\n",
      "Batch loss: 0.09138327091932297\n",
      "Batch loss: 0.1050463393330574\n",
      "Batch loss: 0.0784640684723854\n",
      "Batch loss: 0.07577486336231232\n",
      "Batch loss: 0.0794818103313446\n",
      "Batch loss: 0.0553981214761734\n",
      "Batch loss: 0.06981991231441498\n",
      "Batch loss: 0.07974427938461304\n",
      "Batch loss: 0.03834571689367294\n",
      "Epoch [6/100], Loss: 0.1002\n",
      "Validation Loss: 0.7317, Accuracy: 84.61%\n",
      "Batch loss: 0.08988162130117416\n",
      "Batch loss: 0.04657818377017975\n",
      "Batch loss: 0.10749930143356323\n",
      "Batch loss: 0.08629679679870605\n",
      "Batch loss: 0.09854887425899506\n",
      "Batch loss: 0.08479700982570648\n",
      "Batch loss: 0.23250405490398407\n",
      "Batch loss: 0.07922137528657913\n",
      "Batch loss: 0.07362554967403412\n",
      "Batch loss: 0.027783498167991638\n",
      "Batch loss: 0.04980798065662384\n",
      "Batch loss: 0.1238732561469078\n",
      "Batch loss: 0.07143294811248779\n",
      "Batch loss: 0.06340513378381729\n",
      "Batch loss: 0.078264020383358\n",
      "Batch loss: 0.08613386005163193\n",
      "Batch loss: 0.10132364928722382\n",
      "Batch loss: 0.08651512861251831\n",
      "Batch loss: 0.027231814339756966\n",
      "Batch loss: 0.11278754472732544\n",
      "Batch loss: 0.03540223091840744\n",
      "Batch loss: 0.05907740816473961\n",
      "Batch loss: 0.06596162915229797\n",
      "Batch loss: 0.04319189488887787\n",
      "Batch loss: 0.09350673854351044\n",
      "Batch loss: 0.06387612223625183\n",
      "Batch loss: 0.07853143662214279\n",
      "Batch loss: 0.10464805364608765\n",
      "Batch loss: 0.0727536603808403\n",
      "Batch loss: 0.05336838588118553\n",
      "Batch loss: 0.18760548532009125\n",
      "Batch loss: 0.09874275326728821\n",
      "Batch loss: 0.1068192720413208\n",
      "Batch loss: 0.06799238920211792\n",
      "Batch loss: 0.0757136195898056\n",
      "Batch loss: 0.12137043476104736\n",
      "Batch loss: 0.04678306356072426\n",
      "Batch loss: 0.15486450493335724\n",
      "Batch loss: 0.09422868490219116\n",
      "Batch loss: 0.042227569967508316\n",
      "Batch loss: 0.07875647395849228\n",
      "Batch loss: 0.09408656507730484\n",
      "Batch loss: 0.09795603901147842\n",
      "Batch loss: 0.09387796372175217\n",
      "Batch loss: 0.09408796578645706\n",
      "Batch loss: 0.11123114824295044\n",
      "Batch loss: 0.07442394644021988\n",
      "Batch loss: 0.04881308227777481\n",
      "Batch loss: 0.054616909474134445\n",
      "Batch loss: 0.1460961401462555\n",
      "Batch loss: 0.10850805789232254\n",
      "Batch loss: 0.10609078407287598\n",
      "Batch loss: 0.04814140126109123\n",
      "Batch loss: 0.059381235390901566\n",
      "Batch loss: 0.07748658955097198\n",
      "Batch loss: 0.06393072009086609\n",
      "Batch loss: 0.07865442335605621\n",
      "Batch loss: 0.05103612318634987\n",
      "Batch loss: 0.059069495648145676\n",
      "Batch loss: 0.062007732689380646\n",
      "Batch loss: 0.07576699554920197\n",
      "Batch loss: 0.07446907460689545\n",
      "Batch loss: 0.11154954135417938\n",
      "Batch loss: 0.09500987082719803\n",
      "Batch loss: 0.04493667930364609\n",
      "Batch loss: 0.10092325508594513\n",
      "Batch loss: 0.14182183146476746\n",
      "Batch loss: 0.09959603101015091\n",
      "Batch loss: 0.06161730736494064\n",
      "Batch loss: 0.10020033270120621\n",
      "Batch loss: 0.05602278560400009\n",
      "Batch loss: 0.07090740650892258\n",
      "Batch loss: 0.06676038354635239\n",
      "Batch loss: 0.09253763407468796\n",
      "Batch loss: 0.05407828465104103\n",
      "Batch loss: 0.06904981285333633\n",
      "Batch loss: 0.10277850180864334\n",
      "Batch loss: 0.09983279556035995\n",
      "Batch loss: 0.06331036984920502\n",
      "Batch loss: 0.1239137202501297\n",
      "Batch loss: 0.13148820400238037\n",
      "Batch loss: 0.07442827522754669\n",
      "Batch loss: 0.11286100745201111\n",
      "Batch loss: 0.15562114119529724\n",
      "Batch loss: 0.14622537791728973\n",
      "Batch loss: 0.09480533003807068\n",
      "Batch loss: 0.04607908055186272\n",
      "Batch loss: 0.1097085028886795\n",
      "Batch loss: 0.11070040613412857\n",
      "Batch loss: 0.11314242333173752\n",
      "Batch loss: 0.072171151638031\n",
      "Batch loss: 0.0610155425965786\n",
      "Batch loss: 0.20787878334522247\n",
      "Batch loss: 0.10717908293008804\n",
      "Batch loss: 0.09893112629652023\n",
      "Batch loss: 0.03197294846177101\n",
      "Batch loss: 0.04061849042773247\n",
      "Batch loss: 0.12198242545127869\n",
      "Batch loss: 0.11114272475242615\n",
      "Batch loss: 0.04228431358933449\n",
      "Batch loss: 0.09205343574285507\n",
      "Batch loss: 0.06291384249925613\n",
      "Batch loss: 0.12645405530929565\n",
      "Batch loss: 0.09890248626470566\n",
      "Batch loss: 0.1442720741033554\n",
      "Batch loss: 0.0903259888291359\n",
      "Batch loss: 0.1203221008181572\n",
      "Batch loss: 0.10308148711919785\n",
      "Batch loss: 0.11435700953006744\n",
      "Batch loss: 0.10717331618070602\n",
      "Batch loss: 0.14684191346168518\n",
      "Batch loss: 0.05706215277314186\n",
      "Batch loss: 0.053958188742399216\n",
      "Batch loss: 0.05735405907034874\n",
      "Batch loss: 0.06912040710449219\n",
      "Batch loss: 0.1489500254392624\n",
      "Batch loss: 0.03763667866587639\n",
      "Batch loss: 0.04489218071103096\n",
      "Batch loss: 0.05347147211432457\n",
      "Batch loss: 0.09579659253358841\n",
      "Batch loss: 0.07049936801195145\n",
      "Batch loss: 0.11491341888904572\n",
      "Batch loss: 0.11167250573635101\n",
      "Batch loss: 0.09322721511125565\n",
      "Batch loss: 0.1238611564040184\n",
      "Batch loss: 0.09350025653839111\n",
      "Batch loss: 0.05671297758817673\n",
      "Batch loss: 0.08108257502317429\n",
      "Batch loss: 0.09035731106996536\n",
      "Batch loss: 0.07928355783224106\n",
      "Batch loss: 0.08461210131645203\n",
      "Batch loss: 0.09227026998996735\n",
      "Batch loss: 0.08697030693292618\n",
      "Batch loss: 0.08316943794488907\n",
      "Batch loss: 0.20083072781562805\n",
      "Batch loss: 0.025704167783260345\n",
      "Batch loss: 0.06649713963270187\n",
      "Batch loss: 0.08977189660072327\n",
      "Batch loss: 0.10256838798522949\n",
      "Batch loss: 0.053797777742147446\n",
      "Batch loss: 0.07693413645029068\n",
      "Batch loss: 0.05468109995126724\n",
      "Batch loss: 0.06523322314023972\n",
      "Batch loss: 0.08538258075714111\n",
      "Batch loss: 0.03973941132426262\n",
      "Batch loss: 0.04981637001037598\n",
      "Batch loss: 0.1053774356842041\n",
      "Batch loss: 0.07607774436473846\n",
      "Batch loss: 0.04807290434837341\n",
      "Batch loss: 0.03617541864514351\n",
      "Batch loss: 0.08894761651754379\n",
      "Batch loss: 0.10556735843420029\n",
      "Batch loss: 0.10116720199584961\n",
      "Batch loss: 0.13257363438606262\n",
      "Batch loss: 0.0803670808672905\n",
      "Batch loss: 0.12534725666046143\n",
      "Batch loss: 0.13111139833927155\n",
      "Batch loss: 0.10029687732458115\n",
      "Batch loss: 0.052132491022348404\n",
      "Batch loss: 0.10015226155519485\n",
      "Batch loss: 0.05296001210808754\n",
      "Batch loss: 0.10702753067016602\n",
      "Batch loss: 0.0690050721168518\n",
      "Batch loss: 0.12530475854873657\n",
      "Batch loss: 0.10806971043348312\n",
      "Batch loss: 0.087905153632164\n",
      "Batch loss: 0.07227913290262222\n",
      "Batch loss: 0.059256333857774734\n",
      "Batch loss: 0.06974367052316666\n",
      "Batch loss: 0.06722566485404968\n",
      "Batch loss: 0.09023688733577728\n",
      "Batch loss: 0.12130887806415558\n",
      "Batch loss: 0.04552318900823593\n",
      "Batch loss: 0.06966298818588257\n",
      "Batch loss: 0.04942036420106888\n",
      "Batch loss: 0.07672032713890076\n",
      "Batch loss: 0.1325194090604782\n",
      "Batch loss: 0.03902977332472801\n",
      "Batch loss: 0.1200883761048317\n",
      "Batch loss: 0.06532058119773865\n",
      "Batch loss: 0.13926872611045837\n",
      "Batch loss: 0.06079195812344551\n",
      "Batch loss: 0.05759568512439728\n",
      "Batch loss: 0.13905999064445496\n",
      "Batch loss: 0.10348029434680939\n",
      "Batch loss: 0.08457984775304794\n",
      "Batch loss: 0.12391690909862518\n",
      "Batch loss: 0.07993015646934509\n",
      "Batch loss: 0.13734488189220428\n",
      "Batch loss: 0.10951859503984451\n",
      "Batch loss: 0.10652244091033936\n",
      "Batch loss: 0.05951321870088577\n",
      "Batch loss: 0.06533204764127731\n",
      "Batch loss: 0.06726108491420746\n",
      "Batch loss: 0.07080445438623428\n",
      "Batch loss: 0.08208735287189484\n",
      "Batch loss: 0.05404616519808769\n",
      "Batch loss: 0.04144377261400223\n",
      "Batch loss: 0.08722832798957825\n",
      "Batch loss: 0.09060678631067276\n",
      "Batch loss: 0.10042894631624222\n",
      "Batch loss: 0.17537397146224976\n",
      "Batch loss: 0.04335081949830055\n",
      "Batch loss: 0.06962335109710693\n",
      "Batch loss: 0.10929365456104279\n",
      "Batch loss: 0.10723244398832321\n",
      "Batch loss: 0.08486389368772507\n",
      "Batch loss: 0.09698545187711716\n",
      "Batch loss: 0.05312398448586464\n",
      "Batch loss: 0.1402619332075119\n",
      "Batch loss: 0.10849262773990631\n",
      "Batch loss: 0.0638510212302208\n",
      "Batch loss: 0.08436820656061172\n",
      "Batch loss: 0.11426486074924469\n",
      "Batch loss: 0.06862036138772964\n",
      "Batch loss: 0.06216765567660332\n",
      "Batch loss: 0.043891292065382004\n",
      "Batch loss: 0.05767813324928284\n",
      "Batch loss: 0.03676716238260269\n",
      "Batch loss: 0.08386851847171783\n",
      "Batch loss: 0.04660884663462639\n",
      "Batch loss: 0.08354699611663818\n",
      "Batch loss: 0.02931249514222145\n",
      "Batch loss: 0.07476351410150528\n",
      "Batch loss: 0.0739009752869606\n",
      "Batch loss: 0.06972670555114746\n",
      "Batch loss: 0.07168632745742798\n",
      "Batch loss: 0.11709801107645035\n",
      "Batch loss: 0.15674500167369843\n",
      "Batch loss: 0.09166942536830902\n",
      "Batch loss: 0.10219373553991318\n",
      "Batch loss: 0.047650959342718124\n",
      "Batch loss: 0.06715957075357437\n",
      "Batch loss: 0.10213776677846909\n",
      "Batch loss: 0.0698123425245285\n",
      "Batch loss: 0.1004418209195137\n",
      "Batch loss: 0.0624002180993557\n",
      "Batch loss: 0.1363985240459442\n",
      "Batch loss: 0.061749666929244995\n",
      "Batch loss: 0.049337539821863174\n",
      "Batch loss: 0.12049483507871628\n",
      "Batch loss: 0.08811011910438538\n",
      "Batch loss: 0.030998235568404198\n",
      "Batch loss: 0.09980792552232742\n",
      "Batch loss: 0.0514906644821167\n",
      "Batch loss: 0.05900935083627701\n",
      "Batch loss: 0.06281311810016632\n",
      "Batch loss: 0.09991689026355743\n",
      "Batch loss: 0.02822772040963173\n",
      "Batch loss: 0.054164592176675797\n",
      "Batch loss: 0.11316930502653122\n",
      "Batch loss: 0.10393073409795761\n",
      "Batch loss: 0.12970823049545288\n",
      "Batch loss: 0.08340363204479218\n",
      "Batch loss: 0.1002630963921547\n",
      "Batch loss: 0.12464985251426697\n",
      "Batch loss: 0.09438439458608627\n",
      "Batch loss: 0.11075695604085922\n",
      "Batch loss: 0.04308801889419556\n",
      "Batch loss: 0.10761244595050812\n",
      "Batch loss: 0.12146946787834167\n",
      "Batch loss: 0.0911109670996666\n",
      "Batch loss: 0.10993189364671707\n",
      "Batch loss: 0.06460408121347427\n",
      "Batch loss: 0.09562920778989792\n",
      "Batch loss: 0.06072976440191269\n",
      "Batch loss: 0.1019228994846344\n",
      "Batch loss: 0.09957719594240189\n",
      "Batch loss: 0.1102454662322998\n",
      "Batch loss: 0.045292627066373825\n",
      "Batch loss: 0.13953237235546112\n",
      "Batch loss: 0.07333929091691971\n",
      "Batch loss: 0.07123410701751709\n",
      "Batch loss: 0.08609167486429214\n",
      "Batch loss: 0.1915634721517563\n",
      "Batch loss: 0.056671466678380966\n",
      "Batch loss: 0.08087777346372604\n",
      "Batch loss: 0.1727277636528015\n",
      "Batch loss: 0.07785385847091675\n",
      "Batch loss: 0.1427944302558899\n",
      "Batch loss: 0.08226265013217926\n",
      "Batch loss: 0.0854242667555809\n",
      "Batch loss: 0.12415052205324173\n",
      "Batch loss: 0.17516809701919556\n",
      "Batch loss: 0.11960484087467194\n",
      "Batch loss: 0.12127336859703064\n",
      "Batch loss: 0.05873734503984451\n",
      "Batch loss: 0.03359772264957428\n",
      "Batch loss: 0.05748094245791435\n",
      "Batch loss: 0.06730027496814728\n",
      "Batch loss: 0.09054809808731079\n",
      "Batch loss: 0.07216773927211761\n",
      "Batch loss: 0.06349187344312668\n",
      "Batch loss: 0.09986831247806549\n",
      "Batch loss: 0.11081963032484055\n",
      "Batch loss: 0.1358034312725067\n",
      "Batch loss: 0.07299323379993439\n",
      "Batch loss: 0.11019956320524216\n",
      "Batch loss: 0.11085537821054459\n",
      "Batch loss: 0.11362573504447937\n",
      "Batch loss: 0.06709108501672745\n",
      "Batch loss: 0.048669930547475815\n",
      "Batch loss: 0.12961937487125397\n",
      "Batch loss: 0.05655508115887642\n",
      "Batch loss: 0.06678447872400284\n",
      "Batch loss: 0.09016364067792892\n",
      "Batch loss: 0.041594311594963074\n",
      "Batch loss: 0.03661002218723297\n",
      "Batch loss: 0.061305515468120575\n",
      "Batch loss: 0.06932573765516281\n",
      "Batch loss: 0.053030721843242645\n",
      "Batch loss: 0.04981390759348869\n",
      "Batch loss: 0.04682450741529465\n",
      "Batch loss: 0.06742803752422333\n",
      "Batch loss: 0.11327684670686722\n",
      "Batch loss: 0.10141308605670929\n",
      "Batch loss: 0.06115933880209923\n",
      "Batch loss: 0.04641050100326538\n",
      "Batch loss: 0.05540288984775543\n",
      "Batch loss: 0.06526455283164978\n",
      "Batch loss: 0.06309404969215393\n",
      "Batch loss: 0.08338719606399536\n",
      "Batch loss: 0.06725265085697174\n",
      "Batch loss: 0.08063717186450958\n",
      "Batch loss: 0.08165507763624191\n",
      "Batch loss: 0.149780735373497\n",
      "Batch loss: 0.07347528636455536\n",
      "Batch loss: 0.08310232311487198\n",
      "Batch loss: 0.13321034610271454\n",
      "Batch loss: 0.11806927621364594\n",
      "Batch loss: 0.03672906383872032\n",
      "Batch loss: 0.09953833371400833\n",
      "Batch loss: 0.04445873573422432\n",
      "Batch loss: 0.06811326742172241\n",
      "Batch loss: 0.11278102546930313\n",
      "Batch loss: 0.12783081829547882\n",
      "Batch loss: 0.019596436992287636\n",
      "Batch loss: 0.06438779085874557\n",
      "Batch loss: 0.12179740518331528\n",
      "Batch loss: 0.07988939434289932\n",
      "Batch loss: 0.07689467817544937\n",
      "Batch loss: 0.03377636894583702\n",
      "Batch loss: 0.08262907713651657\n",
      "Batch loss: 0.08592084795236588\n",
      "Batch loss: 0.05560845509171486\n",
      "Batch loss: 0.13094943761825562\n",
      "Batch loss: 0.05400114506483078\n",
      "Batch loss: 0.036658693104982376\n",
      "Batch loss: 0.04985872656106949\n",
      "Batch loss: 0.10607635229825974\n",
      "Batch loss: 0.047056786715984344\n",
      "Batch loss: 0.09488417208194733\n",
      "Batch loss: 0.04780992865562439\n",
      "Batch loss: 0.06462784856557846\n",
      "Batch loss: 0.07421889901161194\n",
      "Batch loss: 0.05523187294602394\n",
      "Batch loss: 0.0502849705517292\n",
      "Batch loss: 0.09430716186761856\n",
      "Batch loss: 0.12134460359811783\n",
      "Batch loss: 0.0510379895567894\n",
      "Batch loss: 0.07416168600320816\n",
      "Batch loss: 0.08385749161243439\n",
      "Batch loss: 0.05330346152186394\n",
      "Batch loss: 0.1009276732802391\n",
      "Batch loss: 0.0836004912853241\n",
      "Batch loss: 0.052396222949028015\n",
      "Batch loss: 0.10484523326158524\n",
      "Batch loss: 0.09699096530675888\n",
      "Batch loss: 0.043802957981824875\n",
      "Batch loss: 0.0951709896326065\n",
      "Batch loss: 0.07019469141960144\n",
      "Batch loss: 0.08765721321105957\n",
      "Batch loss: 0.07228489965200424\n",
      "Batch loss: 0.0727527067065239\n",
      "Batch loss: 0.07601109892129898\n",
      "Batch loss: 0.037921059876680374\n",
      "Batch loss: 0.10333088785409927\n",
      "Batch loss: 0.058206312358379364\n",
      "Batch loss: 0.15752577781677246\n",
      "Batch loss: 0.1042809933423996\n",
      "Batch loss: 0.2042963057756424\n",
      "Batch loss: 0.1110062375664711\n",
      "Batch loss: 0.12843112647533417\n",
      "Batch loss: 0.09733535349369049\n",
      "Batch loss: 0.07018234580755234\n",
      "Batch loss: 0.07582253962755203\n",
      "Batch loss: 0.12410347163677216\n",
      "Batch loss: 0.1267920881509781\n",
      "Batch loss: 0.055987682193517685\n",
      "Batch loss: 0.10743597894906998\n",
      "Batch loss: 0.04811538755893707\n",
      "Batch loss: 0.07763593643903732\n",
      "Batch loss: 0.08238562941551208\n",
      "Batch loss: 0.05316988378763199\n",
      "Batch loss: 0.1108073964715004\n",
      "Batch loss: 0.08227081596851349\n",
      "Batch loss: 0.0353793166577816\n",
      "Batch loss: 0.06035028398036957\n",
      "Batch loss: 0.07387813180685043\n",
      "Batch loss: 0.0292506106197834\n",
      "Batch loss: 0.08833732455968857\n",
      "Batch loss: 0.06429384648799896\n",
      "Batch loss: 0.19235187768936157\n",
      "Batch loss: 0.07943662256002426\n",
      "Batch loss: 0.05563445761799812\n",
      "Batch loss: 0.15090982615947723\n",
      "Batch loss: 0.09017615765333176\n",
      "Batch loss: 0.055569685995578766\n",
      "Batch loss: 0.08337964862585068\n",
      "Batch loss: 0.0834885761141777\n",
      "Batch loss: 0.10016097128391266\n",
      "Batch loss: 0.08981635421514511\n",
      "Batch loss: 0.03054291568696499\n",
      "Batch loss: 0.07554846256971359\n",
      "Batch loss: 0.07824914157390594\n",
      "Batch loss: 0.05684232711791992\n",
      "Batch loss: 0.03542567044496536\n",
      "Epoch [7/100], Loss: 0.0852\n",
      "Validation Loss: 0.4858, Accuracy: 89.19%\n",
      "Batch loss: 0.11545661091804504\n",
      "Batch loss: 0.053188666701316833\n",
      "Batch loss: 0.10668735206127167\n",
      "Batch loss: 0.10127781331539154\n",
      "Batch loss: 0.06414564698934555\n",
      "Batch loss: 0.06117098033428192\n",
      "Batch loss: 0.1925123631954193\n",
      "Batch loss: 0.10934711247682571\n",
      "Batch loss: 0.07237853854894638\n",
      "Batch loss: 0.07133908569812775\n",
      "Batch loss: 0.0507870614528656\n",
      "Batch loss: 0.10754400491714478\n",
      "Batch loss: 0.04937538132071495\n",
      "Batch loss: 0.06635257601737976\n",
      "Batch loss: 0.07677699625492096\n",
      "Batch loss: 0.07047265768051147\n",
      "Batch loss: 0.06785639375448227\n",
      "Batch loss: 0.07593555748462677\n",
      "Batch loss: 0.02596450038254261\n",
      "Batch loss: 0.07386542111635208\n",
      "Batch loss: 0.09338666498661041\n",
      "Batch loss: 0.04187360033392906\n",
      "Batch loss: 0.05883369222283363\n",
      "Batch loss: 0.050537239760160446\n",
      "Batch loss: 0.03368968516588211\n",
      "Batch loss: 0.08355497568845749\n",
      "Batch loss: 0.024234645068645477\n",
      "Batch loss: 0.07033936679363251\n",
      "Batch loss: 0.05396318808197975\n",
      "Batch loss: 0.09306126832962036\n",
      "Batch loss: 0.11352428048849106\n",
      "Batch loss: 0.035518404096364975\n",
      "Batch loss: 0.08909111469984055\n",
      "Batch loss: 0.04743888974189758\n",
      "Batch loss: 0.028203466907143593\n",
      "Batch loss: 0.14391128718852997\n",
      "Batch loss: 0.07534142583608627\n",
      "Batch loss: 0.07751306146383286\n",
      "Batch loss: 0.15861020982265472\n",
      "Batch loss: 0.0788021981716156\n",
      "Batch loss: 0.12148962914943695\n",
      "Batch loss: 0.08690132945775986\n",
      "Batch loss: 0.1047334372997284\n",
      "Batch loss: 0.070100337266922\n",
      "Batch loss: 0.14758728444576263\n",
      "Batch loss: 0.08228879421949387\n",
      "Batch loss: 0.050472840666770935\n",
      "Batch loss: 0.05440584570169449\n",
      "Batch loss: 0.04882790520787239\n",
      "Batch loss: 0.0452161580324173\n",
      "Batch loss: 0.07183294743299484\n",
      "Batch loss: 0.0838717371225357\n",
      "Batch loss: 0.03288549557328224\n",
      "Batch loss: 0.10152123868465424\n",
      "Batch loss: 0.04675449803471565\n",
      "Batch loss: 0.09095250070095062\n",
      "Batch loss: 0.09592515230178833\n",
      "Batch loss: 0.0595606304705143\n",
      "Batch loss: 0.10012940317392349\n",
      "Batch loss: 0.0866626724600792\n",
      "Batch loss: 0.0707504153251648\n",
      "Batch loss: 0.06173369660973549\n",
      "Batch loss: 0.07286593317985535\n",
      "Batch loss: 0.11018166691064835\n",
      "Batch loss: 0.03870714455842972\n",
      "Batch loss: 0.07103997468948364\n",
      "Batch loss: 0.06650399416685104\n",
      "Batch loss: 0.07063537836074829\n",
      "Batch loss: 0.04085944592952728\n",
      "Batch loss: 0.07005023211240768\n",
      "Batch loss: 0.042543891817331314\n",
      "Batch loss: 0.039938244968652725\n",
      "Batch loss: 0.03736657649278641\n",
      "Batch loss: 0.054652947932481766\n",
      "Batch loss: 0.014302543364465237\n",
      "Batch loss: 0.08005983382463455\n",
      "Batch loss: 0.048138465732336044\n",
      "Batch loss: 0.11663170903921127\n",
      "Batch loss: 0.04247455671429634\n",
      "Batch loss: 0.10293281078338623\n",
      "Batch loss: 0.017906133085489273\n",
      "Batch loss: 0.05743144825100899\n",
      "Batch loss: 0.1056385487318039\n",
      "Batch loss: 0.09898333996534348\n",
      "Batch loss: 0.06052134931087494\n",
      "Batch loss: 0.07796109467744827\n",
      "Batch loss: 0.025319576263427734\n",
      "Batch loss: 0.08280669897794724\n",
      "Batch loss: 0.035824503749608994\n",
      "Batch loss: 0.05940956622362137\n",
      "Batch loss: 0.061367955058813095\n",
      "Batch loss: 0.03182555362582207\n",
      "Batch loss: 0.14283105731010437\n",
      "Batch loss: 0.07142305374145508\n",
      "Batch loss: 0.0468359999358654\n",
      "Batch loss: 0.07129201292991638\n",
      "Batch loss: 0.05477050319314003\n",
      "Batch loss: 0.15003716945648193\n",
      "Batch loss: 0.06836020946502686\n",
      "Batch loss: 0.05185340344905853\n",
      "Batch loss: 0.05225014314055443\n",
      "Batch loss: 0.012649456970393658\n",
      "Batch loss: 0.03694228082895279\n",
      "Batch loss: 0.07339056581258774\n",
      "Batch loss: 0.035708121955394745\n",
      "Batch loss: 0.06437721103429794\n",
      "Batch loss: 0.14284884929656982\n",
      "Batch loss: 0.08381705731153488\n",
      "Batch loss: 0.025764087215065956\n",
      "Batch loss: 0.06231703981757164\n",
      "Batch loss: 0.06439124792814255\n",
      "Batch loss: 0.030686553567647934\n",
      "Batch loss: 0.026774967089295387\n",
      "Batch loss: 0.06598003208637238\n",
      "Batch loss: 0.0845162644982338\n",
      "Batch loss: 0.10872948169708252\n",
      "Batch loss: 0.04101696237921715\n",
      "Batch loss: 0.031286366283893585\n",
      "Batch loss: 0.032161664217710495\n",
      "Batch loss: 0.07869106531143188\n",
      "Batch loss: 0.042685605585575104\n",
      "Batch loss: 0.09359362721443176\n",
      "Batch loss: 0.10760745406150818\n",
      "Batch loss: 0.0528818778693676\n",
      "Batch loss: 0.10873112827539444\n",
      "Batch loss: 0.07224634289741516\n",
      "Batch loss: 0.038608960807323456\n",
      "Batch loss: 0.10151640325784683\n",
      "Batch loss: 0.15844503045082092\n",
      "Batch loss: 0.08307870477437973\n",
      "Batch loss: 0.06743734329938889\n",
      "Batch loss: 0.12447674572467804\n",
      "Batch loss: 0.08164120465517044\n",
      "Batch loss: 0.06741863489151001\n",
      "Batch loss: 0.11327986419200897\n",
      "Batch loss: 0.08400242775678635\n",
      "Batch loss: 0.05682770907878876\n",
      "Batch loss: 0.04159265756607056\n",
      "Batch loss: 0.06295961141586304\n",
      "Batch loss: 0.04334156960248947\n",
      "Batch loss: 0.06716612726449966\n",
      "Batch loss: 0.036646708846092224\n",
      "Batch loss: 0.07592914998531342\n",
      "Batch loss: 0.0860036164522171\n",
      "Batch loss: 0.08221013098955154\n",
      "Batch loss: 0.053711168467998505\n",
      "Batch loss: 0.0571804977953434\n",
      "Batch loss: 0.06350275874137878\n",
      "Batch loss: 0.055047642439603806\n",
      "Batch loss: 0.06924055516719818\n",
      "Batch loss: 0.08515570312738419\n",
      "Batch loss: 0.07113513350486755\n",
      "Batch loss: 0.10472514480352402\n",
      "Batch loss: 0.06233181059360504\n",
      "Batch loss: 0.10222364962100983\n",
      "Batch loss: 0.12361167371273041\n",
      "Batch loss: 0.10280246287584305\n",
      "Batch loss: 0.03535466268658638\n",
      "Batch loss: 0.046149540692567825\n",
      "Batch loss: 0.0658298209309578\n",
      "Batch loss: 0.036087967455387115\n",
      "Batch loss: 0.12663327157497406\n",
      "Batch loss: 0.06889233738183975\n",
      "Batch loss: 0.10372699797153473\n",
      "Batch loss: 0.05451104789972305\n",
      "Batch loss: 0.10811629891395569\n",
      "Batch loss: 0.04640989378094673\n",
      "Batch loss: 0.10524400323629379\n",
      "Batch loss: 0.03682974725961685\n",
      "Batch loss: 0.05377659946680069\n",
      "Batch loss: 0.048340655863285065\n",
      "Batch loss: 0.09098952263593674\n",
      "Batch loss: 0.06343594938516617\n",
      "Batch loss: 0.06732401996850967\n",
      "Batch loss: 0.05157202109694481\n",
      "Batch loss: 0.1288442462682724\n",
      "Batch loss: 0.09690959006547928\n",
      "Batch loss: 0.05616780370473862\n",
      "Batch loss: 0.09698395431041718\n",
      "Batch loss: 0.05502437427639961\n",
      "Batch loss: 0.07844200730323792\n",
      "Batch loss: 0.09252655506134033\n",
      "Batch loss: 0.021071268245577812\n",
      "Batch loss: 0.16586746275424957\n",
      "Batch loss: 0.07884012907743454\n",
      "Batch loss: 0.04567287862300873\n",
      "Batch loss: 0.047815170139074326\n",
      "Batch loss: 0.06566561758518219\n",
      "Batch loss: 0.053872376680374146\n",
      "Batch loss: 0.0794467180967331\n",
      "Batch loss: 0.07656684517860413\n",
      "Batch loss: 0.050453756004571915\n",
      "Batch loss: 0.06830991804599762\n",
      "Batch loss: 0.05591316148638725\n",
      "Batch loss: 0.047418832778930664\n",
      "Batch loss: 0.04766813665628433\n",
      "Batch loss: 0.0323922224342823\n",
      "Batch loss: 0.11312016099691391\n",
      "Batch loss: 0.07053638994693756\n",
      "Batch loss: 0.066689133644104\n",
      "Batch loss: 0.08645699173212051\n",
      "Batch loss: 0.10652138292789459\n",
      "Batch loss: 0.03079511597752571\n",
      "Batch loss: 0.08642444759607315\n",
      "Batch loss: 0.13970845937728882\n",
      "Batch loss: 0.07995553314685822\n",
      "Batch loss: 0.06786517053842545\n",
      "Batch loss: 0.061753176152706146\n",
      "Batch loss: 0.06821934878826141\n",
      "Batch loss: 0.0751928836107254\n",
      "Batch loss: 0.0333937369287014\n",
      "Batch loss: 0.0384095199406147\n",
      "Batch loss: 0.04276980832219124\n",
      "Batch loss: 0.09582266211509705\n",
      "Batch loss: 0.0634193941950798\n",
      "Batch loss: 0.09500348567962646\n",
      "Batch loss: 0.03903217986226082\n",
      "Batch loss: 0.05667522922158241\n",
      "Batch loss: 0.021265076473355293\n",
      "Batch loss: 0.0669841393828392\n",
      "Batch loss: 0.037417422980070114\n",
      "Batch loss: 0.0762312114238739\n",
      "Batch loss: 0.03377801924943924\n",
      "Batch loss: 0.0662616416811943\n",
      "Batch loss: 0.030982790514826775\n",
      "Batch loss: 0.05183572322130203\n",
      "Batch loss: 0.04852822422981262\n",
      "Batch loss: 0.08936973661184311\n",
      "Batch loss: 0.12045003473758698\n",
      "Batch loss: 0.04560016095638275\n",
      "Batch loss: 0.0658285915851593\n",
      "Batch loss: 0.03243099898099899\n",
      "Batch loss: 0.07728612422943115\n",
      "Batch loss: 0.02537962980568409\n",
      "Batch loss: 0.04048197716474533\n",
      "Batch loss: 0.08481761068105698\n",
      "Batch loss: 0.036466553807258606\n",
      "Batch loss: 0.11706044524908066\n",
      "Batch loss: 0.04615221545100212\n",
      "Batch loss: 0.03052394464612007\n",
      "Batch loss: 0.08171913772821426\n",
      "Batch loss: 0.07013900578022003\n",
      "Batch loss: 0.033139847218990326\n",
      "Batch loss: 0.11317326873540878\n",
      "Batch loss: 0.028798427432775497\n",
      "Batch loss: 0.04142256826162338\n",
      "Batch loss: 0.04109783470630646\n",
      "Batch loss: 0.07310816645622253\n",
      "Batch loss: 0.037552785128355026\n",
      "Batch loss: 0.04169146344065666\n",
      "Batch loss: 0.06570469588041306\n",
      "Batch loss: 0.1068882942199707\n",
      "Batch loss: 0.06262719631195068\n",
      "Batch loss: 0.09207327663898468\n",
      "Batch loss: 0.06418965756893158\n",
      "Batch loss: 0.18514037132263184\n",
      "Batch loss: 0.07884427905082703\n",
      "Batch loss: 0.07348079234361649\n",
      "Batch loss: 0.048281896859407425\n",
      "Batch loss: 0.11747956275939941\n",
      "Batch loss: 0.09086285531520844\n",
      "Batch loss: 0.07472258806228638\n",
      "Batch loss: 0.07901500910520554\n",
      "Batch loss: 0.029043996706604958\n",
      "Batch loss: 0.03597757965326309\n",
      "Batch loss: 0.04462919384241104\n",
      "Batch loss: 0.07283799350261688\n",
      "Batch loss: 0.07638730108737946\n",
      "Batch loss: 0.049296360462903976\n",
      "Batch loss: 0.0627690926194191\n",
      "Batch loss: 0.07402564585208893\n",
      "Batch loss: 0.041934140026569366\n",
      "Batch loss: 0.06972024589776993\n",
      "Batch loss: 0.030572637915611267\n",
      "Batch loss: 0.10461273044347763\n",
      "Batch loss: 0.06263095140457153\n",
      "Batch loss: 0.04312984272837639\n",
      "Batch loss: 0.10406273603439331\n",
      "Batch loss: 0.09237529337406158\n",
      "Batch loss: 0.13785746693611145\n",
      "Batch loss: 0.033611711114645004\n",
      "Batch loss: 0.056789692491292953\n",
      "Batch loss: 0.1102062538266182\n",
      "Batch loss: 0.11845406889915466\n",
      "Batch loss: 0.10275473445653915\n",
      "Batch loss: 0.12633387744426727\n",
      "Batch loss: 0.06604345887899399\n",
      "Batch loss: 0.09468654543161392\n",
      "Batch loss: 0.01901349611580372\n",
      "Batch loss: 0.13663361966609955\n",
      "Batch loss: 0.04170415922999382\n",
      "Batch loss: 0.06329549849033356\n",
      "Batch loss: 0.05575646460056305\n",
      "Batch loss: 0.048854950815439224\n",
      "Batch loss: 0.07954660803079605\n",
      "Batch loss: 0.157823845744133\n",
      "Batch loss: 0.04558493569493294\n",
      "Batch loss: 0.047864869236946106\n",
      "Batch loss: 0.0925775095820427\n",
      "Batch loss: 0.07375874370336533\n",
      "Batch loss: 0.07360275089740753\n",
      "Batch loss: 0.026428913697600365\n",
      "Batch loss: 0.07278255373239517\n",
      "Batch loss: 0.059993818402290344\n",
      "Batch loss: 0.04587016627192497\n",
      "Batch loss: 0.04946224391460419\n",
      "Batch loss: 0.03464735299348831\n",
      "Batch loss: 0.0224074088037014\n",
      "Batch loss: 0.028207873925566673\n",
      "Batch loss: 0.09961740672588348\n",
      "Batch loss: 0.043142739683389664\n",
      "Batch loss: 0.050615034997463226\n",
      "Batch loss: 0.06271091103553772\n",
      "Batch loss: 0.06685078144073486\n",
      "Batch loss: 0.07032783329486847\n",
      "Batch loss: 0.07814285159111023\n",
      "Batch loss: 0.03612828254699707\n",
      "Batch loss: 0.023711737245321274\n",
      "Batch loss: 0.04630173370242119\n",
      "Batch loss: 0.04693768173456192\n",
      "Batch loss: 0.06636805832386017\n",
      "Batch loss: 0.07107875496149063\n",
      "Batch loss: 0.019361097365617752\n",
      "Batch loss: 0.06873060017824173\n",
      "Batch loss: 0.04013902321457863\n",
      "Batch loss: 0.09585534036159515\n",
      "Batch loss: 0.07899408042430878\n",
      "Batch loss: 0.09874770790338516\n",
      "Batch loss: 0.04759162664413452\n",
      "Batch loss: 0.047026630491018295\n",
      "Batch loss: 0.03484390676021576\n",
      "Batch loss: 0.10636579990386963\n",
      "Batch loss: 0.05730919912457466\n",
      "Batch loss: 0.0539567731320858\n",
      "Batch loss: 0.08719298988580704\n",
      "Batch loss: 0.07537036389112473\n",
      "Batch loss: 0.07328490167856216\n",
      "Batch loss: 0.0385759174823761\n",
      "Batch loss: 0.06753969937562943\n",
      "Batch loss: 0.030228575691580772\n",
      "Batch loss: 0.09000305086374283\n",
      "Batch loss: 0.032302819192409515\n",
      "Batch loss: 0.043809160590171814\n",
      "Batch loss: 0.05911467224359512\n",
      "Batch loss: 0.05042766034603119\n",
      "Batch loss: 0.11938914656639099\n",
      "Batch loss: 0.05871404707431793\n",
      "Batch loss: 0.06705864518880844\n",
      "Batch loss: 0.042791370302438736\n",
      "Batch loss: 0.043692946434020996\n",
      "Batch loss: 0.05865592509508133\n",
      "Batch loss: 0.10766643285751343\n",
      "Batch loss: 0.057569973170757294\n",
      "Batch loss: 0.04272758588194847\n",
      "Batch loss: 0.0930735319852829\n",
      "Batch loss: 0.02223780006170273\n",
      "Batch loss: 0.03356088697910309\n",
      "Batch loss: 0.10532481968402863\n",
      "Batch loss: 0.045581020414829254\n",
      "Batch loss: 0.10753878206014633\n",
      "Batch loss: 0.060285892337560654\n",
      "Batch loss: 0.057133328169584274\n",
      "Batch loss: 0.04754425585269928\n",
      "Batch loss: 0.09090273082256317\n",
      "Batch loss: 0.06116192787885666\n",
      "Batch loss: 0.05375310033559799\n",
      "Batch loss: 0.10717248916625977\n",
      "Batch loss: 0.10511338710784912\n",
      "Batch loss: 0.01681050844490528\n",
      "Batch loss: 0.13188591599464417\n",
      "Batch loss: 0.04458889365196228\n",
      "Batch loss: 0.125424325466156\n",
      "Batch loss: 0.07245813310146332\n",
      "Batch loss: 0.04838097095489502\n",
      "Batch loss: 0.0798332542181015\n",
      "Batch loss: 0.03311208263039589\n",
      "Batch loss: 0.03815958648920059\n",
      "Batch loss: 0.05992847681045532\n",
      "Batch loss: 0.0790872797369957\n",
      "Batch loss: 0.06843778491020203\n",
      "Batch loss: 0.14420068264007568\n",
      "Batch loss: 0.1204773485660553\n",
      "Batch loss: 0.09870996326208115\n",
      "Batch loss: 0.0432739332318306\n",
      "Batch loss: 0.04576810821890831\n",
      "Batch loss: 0.027379721403121948\n",
      "Batch loss: 0.05754546448588371\n",
      "Batch loss: 0.08326412737369537\n",
      "Batch loss: 0.06315529346466064\n",
      "Batch loss: 0.06412835419178009\n",
      "Batch loss: 0.09274332225322723\n",
      "Batch loss: 0.034937359392642975\n",
      "Batch loss: 0.046085529029369354\n",
      "Batch loss: 0.07365737855434418\n",
      "Batch loss: 0.09634315222501755\n",
      "Batch loss: 0.0664055198431015\n",
      "Batch loss: 0.05280803143978119\n",
      "Batch loss: 0.08971971273422241\n",
      "Batch loss: 0.07044651359319687\n",
      "Batch loss: 0.052231885492801666\n",
      "Batch loss: 0.050972942262887955\n",
      "Batch loss: 0.04674769192934036\n",
      "Batch loss: 0.12738290429115295\n",
      "Batch loss: 0.039494890719652176\n",
      "Batch loss: 0.031991928815841675\n",
      "Batch loss: 0.10243020206689835\n",
      "Batch loss: 0.06065787374973297\n",
      "Batch loss: 0.05317474156618118\n",
      "Batch loss: 0.04389769211411476\n",
      "Batch loss: 0.10599833726882935\n",
      "Batch loss: 0.10840574651956558\n",
      "Batch loss: 0.06298866122961044\n",
      "Batch loss: 0.04178638383746147\n",
      "Batch loss: 0.05622006952762604\n",
      "Batch loss: 0.15966418385505676\n",
      "Batch loss: 0.08750137686729431\n",
      "Batch loss: 0.02872552163898945\n",
      "Epoch [8/100], Loss: 0.0691\n",
      "Validation Loss: 0.3856, Accuracy: 88.52%\n",
      "Batch loss: 0.06484433263540268\n",
      "Batch loss: 0.10397998988628387\n",
      "Batch loss: 0.0498042032122612\n",
      "Batch loss: 0.09746939688920975\n",
      "Batch loss: 0.03957595303654671\n",
      "Batch loss: 0.033094145357608795\n",
      "Batch loss: 0.1515703946352005\n",
      "Batch loss: 0.08570943027734756\n",
      "Batch loss: 0.08345290273427963\n",
      "Batch loss: 0.05350528657436371\n",
      "Batch loss: 0.08546596765518188\n",
      "Batch loss: 0.07751394063234329\n",
      "Batch loss: 0.046442970633506775\n",
      "Batch loss: 0.0691278725862503\n",
      "Batch loss: 0.04330246150493622\n",
      "Batch loss: 0.09177152812480927\n",
      "Batch loss: 0.061812952160835266\n",
      "Batch loss: 0.06379514187574387\n",
      "Batch loss: 0.024352962151169777\n",
      "Batch loss: 0.11016690731048584\n",
      "Batch loss: 0.05970723554491997\n",
      "Batch loss: 0.08240310847759247\n",
      "Batch loss: 0.1176224797964096\n",
      "Batch loss: 0.040745317935943604\n",
      "Batch loss: 0.04741910845041275\n",
      "Batch loss: 0.04581458866596222\n",
      "Batch loss: 0.0629509687423706\n",
      "Batch loss: 0.05715109035372734\n",
      "Batch loss: 0.05098692700266838\n",
      "Batch loss: 0.03054259717464447\n",
      "Batch loss: 0.10136013478040695\n",
      "Batch loss: 0.12110510468482971\n",
      "Batch loss: 0.035660188645124435\n",
      "Batch loss: 0.037088483572006226\n",
      "Batch loss: 0.05959051474928856\n",
      "Batch loss: 0.054188281297683716\n",
      "Batch loss: 0.042036477476358414\n",
      "Batch loss: 0.030746590346097946\n",
      "Batch loss: 0.08645851165056229\n",
      "Batch loss: 0.08054192364215851\n",
      "Batch loss: 0.07397625595331192\n",
      "Batch loss: 0.03686244785785675\n",
      "Batch loss: 0.1362670361995697\n",
      "Batch loss: 0.14791686832904816\n",
      "Batch loss: 0.09161888062953949\n",
      "Batch loss: 0.06601037085056305\n",
      "Batch loss: 0.12258169054985046\n",
      "Batch loss: 0.08460632711648941\n",
      "Batch loss: 0.05807377025485039\n",
      "Batch loss: 0.04527672380208969\n",
      "Batch loss: 0.04170752689242363\n",
      "Batch loss: 0.16498209536075592\n",
      "Batch loss: 0.0550813190639019\n",
      "Batch loss: 0.05227695032954216\n",
      "Batch loss: 0.056201376020908356\n",
      "Batch loss: 0.07845447212457657\n",
      "Batch loss: 0.04725376144051552\n",
      "Batch loss: 0.04157606512308121\n",
      "Batch loss: 0.023598067462444305\n",
      "Batch loss: 0.0400952585041523\n",
      "Batch loss: 0.05007205903530121\n",
      "Batch loss: 0.09271925687789917\n",
      "Batch loss: 0.057149119675159454\n",
      "Batch loss: 0.10676812380552292\n",
      "Batch loss: 0.04223204404115677\n",
      "Batch loss: 0.07822427153587341\n",
      "Batch loss: 0.07222945988178253\n",
      "Batch loss: 0.0901426300406456\n",
      "Batch loss: 0.0732223317027092\n",
      "Batch loss: 0.07197427749633789\n",
      "Batch loss: 0.08057783544063568\n",
      "Batch loss: 0.031942568719387054\n",
      "Batch loss: 0.05646609142422676\n",
      "Batch loss: 0.026807226240634918\n",
      "Batch loss: 0.04312795773148537\n",
      "Batch loss: 0.09024182707071304\n",
      "Batch loss: 0.06856894493103027\n",
      "Batch loss: 0.034442655742168427\n",
      "Batch loss: 0.0399429053068161\n",
      "Batch loss: 0.07354395091533661\n",
      "Batch loss: 0.01978141814470291\n",
      "Batch loss: 0.06978568434715271\n",
      "Batch loss: 0.09077653288841248\n",
      "Batch loss: 0.12424115836620331\n",
      "Batch loss: 0.0898333340883255\n",
      "Batch loss: 0.10842502862215042\n",
      "Batch loss: 0.05486038699746132\n",
      "Batch loss: 0.07029780000448227\n",
      "Batch loss: 0.03345129266381264\n",
      "Batch loss: 0.04434063285589218\n",
      "Batch loss: 0.06603115051984787\n",
      "Batch loss: 0.031599123030900955\n",
      "Batch loss: 0.10727675259113312\n",
      "Batch loss: 0.10981859266757965\n",
      "Batch loss: 0.06889251619577408\n",
      "Batch loss: 0.04619189351797104\n",
      "Batch loss: 0.027688534930348396\n",
      "Batch loss: 0.09242943674325943\n",
      "Batch loss: 0.08304869383573532\n",
      "Batch loss: 0.03363903611898422\n",
      "Batch loss: 0.07636275142431259\n",
      "Batch loss: 0.04620258882641792\n",
      "Batch loss: 0.06115274503827095\n",
      "Batch loss: 0.030003683641552925\n",
      "Batch loss: 0.05240214988589287\n",
      "Batch loss: 0.041080184280872345\n",
      "Batch loss: 0.08707176148891449\n",
      "Batch loss: 0.06732312589883804\n",
      "Batch loss: 0.03540700674057007\n",
      "Batch loss: 0.04636413976550102\n",
      "Batch loss: 0.03859228640794754\n",
      "Batch loss: 0.03296696022152901\n",
      "Batch loss: 0.01940963789820671\n",
      "Batch loss: 0.039512354880571365\n",
      "Batch loss: 0.044097140431404114\n",
      "Batch loss: 0.058080270886421204\n",
      "Batch loss: 0.016070224344730377\n",
      "Batch loss: 0.06438414752483368\n",
      "Batch loss: 0.028367213904857635\n",
      "Batch loss: 0.04570944234728813\n",
      "Batch loss: 0.029583033174276352\n",
      "Batch loss: 0.022509176284074783\n",
      "Batch loss: 0.044346701353788376\n",
      "Batch loss: 0.04275105148553848\n",
      "Batch loss: 0.09926202148199081\n",
      "Batch loss: 0.046096742153167725\n",
      "Batch loss: 0.018427954986691475\n",
      "Batch loss: 0.04806329309940338\n",
      "Batch loss: 0.035472139716148376\n",
      "Batch loss: 0.05980731546878815\n",
      "Batch loss: 0.036625608801841736\n",
      "Batch loss: 0.047470711171627045\n",
      "Batch loss: 0.10139907151460648\n",
      "Batch loss: 0.050784047693014145\n",
      "Batch loss: 0.02891719341278076\n",
      "Batch loss: 0.01684250868856907\n",
      "Batch loss: 0.041030216962099075\n",
      "Batch loss: 0.02904113009572029\n",
      "Batch loss: 0.030337972566485405\n",
      "Batch loss: 0.015070943161845207\n",
      "Batch loss: 0.05512762442231178\n",
      "Batch loss: 0.046640556305646896\n",
      "Batch loss: 0.0860927477478981\n",
      "Batch loss: 0.07217887043952942\n",
      "Batch loss: 0.03199568763375282\n",
      "Batch loss: 0.06811881065368652\n",
      "Batch loss: 0.031540244817733765\n",
      "Batch loss: 0.03242378681898117\n",
      "Batch loss: 0.03270624950528145\n",
      "Batch loss: 0.05372899770736694\n",
      "Batch loss: 0.03259400650858879\n",
      "Batch loss: 0.053987953811883926\n",
      "Batch loss: 0.10017847269773483\n",
      "Batch loss: 0.05096019431948662\n",
      "Batch loss: 0.08433165401220322\n",
      "Batch loss: 0.19476254284381866\n",
      "Batch loss: 0.06592454016208649\n",
      "Batch loss: 0.02654024213552475\n",
      "Batch loss: 0.018765773624181747\n",
      "Batch loss: 0.06565818190574646\n",
      "Batch loss: 0.05498488247394562\n",
      "Batch loss: 0.046295128762722015\n",
      "Batch loss: 0.0669529139995575\n",
      "Batch loss: 0.10578791797161102\n",
      "Batch loss: 0.06645584106445312\n",
      "Batch loss: 0.03509613126516342\n",
      "Batch loss: 0.03202032297849655\n",
      "Batch loss: 0.04169280454516411\n",
      "Batch loss: 0.04318142682313919\n",
      "Batch loss: 0.017212210223078728\n",
      "Batch loss: 0.08073054254055023\n",
      "Batch loss: 0.05334058403968811\n",
      "Batch loss: 0.041031572967767715\n",
      "Batch loss: 0.050945065915584564\n",
      "Batch loss: 0.06423544138669968\n",
      "Batch loss: 0.026291709393262863\n",
      "Batch loss: 0.047682929784059525\n",
      "Batch loss: 0.01034960150718689\n",
      "Batch loss: 0.06282421946525574\n",
      "Batch loss: 0.051895786076784134\n",
      "Batch loss: 0.035525381565093994\n",
      "Batch loss: 0.015187563374638557\n",
      "Batch loss: 0.029047032818198204\n",
      "Batch loss: 0.09242824465036392\n",
      "Batch loss: 0.07356338948011398\n",
      "Batch loss: 0.07419805228710175\n",
      "Batch loss: 0.03950607031583786\n",
      "Batch loss: 0.04261103272438049\n",
      "Batch loss: 0.03881721571087837\n",
      "Batch loss: 0.06917548179626465\n",
      "Batch loss: 0.04919657111167908\n",
      "Batch loss: 0.08150137960910797\n",
      "Batch loss: 0.11289126425981522\n",
      "Batch loss: 0.04068975895643234\n",
      "Batch loss: 0.04114450514316559\n",
      "Batch loss: 0.01769898645579815\n",
      "Batch loss: 0.026336468756198883\n",
      "Batch loss: 0.05158235505223274\n",
      "Batch loss: 0.056486256420612335\n",
      "Batch loss: 0.06980998814105988\n",
      "Batch loss: 0.05219240486621857\n",
      "Batch loss: 0.07468242943286896\n",
      "Batch loss: 0.026066873222589493\n",
      "Batch loss: 0.014715137891471386\n",
      "Batch loss: 0.08620864152908325\n",
      "Batch loss: 0.08977048099040985\n",
      "Batch loss: 0.05139091610908508\n",
      "Batch loss: 0.02781480923295021\n",
      "Batch loss: 0.09867249429225922\n",
      "Batch loss: 0.08537568151950836\n",
      "Batch loss: 0.11104340106248856\n",
      "Batch loss: 0.07718319445848465\n",
      "Batch loss: 0.07606495916843414\n",
      "Batch loss: 0.061097826808691025\n",
      "Batch loss: 0.04459507018327713\n",
      "Batch loss: 0.06917285174131393\n",
      "Batch loss: 0.08390822261571884\n",
      "Batch loss: 0.041142553091049194\n",
      "Batch loss: 0.02049451693892479\n",
      "Batch loss: 0.03510765731334686\n",
      "Batch loss: 0.07870505750179291\n",
      "Batch loss: 0.01751435548067093\n",
      "Batch loss: 0.03106505051255226\n",
      "Batch loss: 0.06878550350666046\n",
      "Batch loss: 0.035394906997680664\n",
      "Batch loss: 0.11778656393289566\n",
      "Batch loss: 0.024038461968302727\n",
      "Batch loss: 0.08348847925662994\n",
      "Batch loss: 0.07864813506603241\n",
      "Batch loss: 0.03549816459417343\n",
      "Batch loss: 0.06194315850734711\n",
      "Batch loss: 0.01543875690549612\n",
      "Batch loss: 0.02100941725075245\n",
      "Batch loss: 0.055385708808898926\n",
      "Batch loss: 0.06203937530517578\n",
      "Batch loss: 0.08238393068313599\n",
      "Batch loss: 0.03846186026930809\n",
      "Batch loss: 0.07762899994850159\n",
      "Batch loss: 0.05725518614053726\n",
      "Batch loss: 0.045408863574266434\n",
      "Batch loss: 0.07899291068315506\n",
      "Batch loss: 0.0769299864768982\n",
      "Batch loss: 0.011651569977402687\n",
      "Batch loss: 0.0405312143266201\n",
      "Batch loss: 0.054354988038539886\n",
      "Batch loss: 0.05796530842781067\n",
      "Batch loss: 0.041486043483018875\n",
      "Batch loss: 0.05553620681166649\n",
      "Batch loss: 0.03760751709342003\n",
      "Batch loss: 0.039922479540109634\n",
      "Batch loss: 0.09502005577087402\n",
      "Batch loss: 0.06013944000005722\n",
      "Batch loss: 0.01686803065240383\n",
      "Batch loss: 0.04893265292048454\n",
      "Batch loss: 0.06858672201633453\n",
      "Batch loss: 0.07495026290416718\n",
      "Batch loss: 0.06786689907312393\n",
      "Batch loss: 0.06104941666126251\n",
      "Batch loss: 0.09357113391160965\n",
      "Batch loss: 0.12703491747379303\n",
      "Batch loss: 0.05910511314868927\n",
      "Batch loss: 0.08597694337368011\n",
      "Batch loss: 0.0813271626830101\n",
      "Batch loss: 0.03926026076078415\n",
      "Batch loss: 0.024653257802128792\n",
      "Batch loss: 0.03386792168021202\n",
      "Batch loss: 0.0284716933965683\n",
      "Batch loss: 0.05024104565382004\n",
      "Batch loss: 0.04480040445923805\n",
      "Batch loss: 0.023497285321354866\n",
      "Batch loss: 0.11424081027507782\n",
      "Batch loss: 0.08470228314399719\n",
      "Batch loss: 0.11077654361724854\n",
      "Batch loss: 0.1442713737487793\n",
      "Batch loss: 0.059109047055244446\n",
      "Batch loss: 0.044204231351614\n",
      "Batch loss: 0.035512231290340424\n",
      "Batch loss: 0.0797119066119194\n",
      "Batch loss: 0.06035410612821579\n",
      "Batch loss: 0.08118928223848343\n",
      "Batch loss: 0.029800711199641228\n",
      "Batch loss: 0.02720988541841507\n",
      "Batch loss: 0.14026981592178345\n",
      "Batch loss: 0.1113823726773262\n",
      "Batch loss: 0.11360756307840347\n",
      "Batch loss: 0.1026315838098526\n",
      "Batch loss: 0.03286222368478775\n",
      "Batch loss: 0.05159223824739456\n",
      "Batch loss: 0.10387405008077621\n",
      "Batch loss: 0.04148353263735771\n",
      "Batch loss: 0.028084512799978256\n",
      "Batch loss: 0.06693009287118912\n",
      "Batch loss: 0.03731830045580864\n",
      "Batch loss: 0.04296274855732918\n",
      "Batch loss: 0.07715187221765518\n",
      "Batch loss: 0.09851780533790588\n",
      "Batch loss: 0.028747284784913063\n",
      "Batch loss: 0.03074069321155548\n",
      "Batch loss: 0.07791510969400406\n",
      "Batch loss: 0.09906454384326935\n",
      "Batch loss: 0.08102422207593918\n",
      "Batch loss: 0.022173788398504257\n",
      "Batch loss: 0.039268918335437775\n",
      "Batch loss: 0.03315330669283867\n",
      "Batch loss: 0.02376924827694893\n",
      "Batch loss: 0.05703470855951309\n",
      "Batch loss: 0.022828340530395508\n",
      "Batch loss: 0.02557964436709881\n",
      "Batch loss: 0.02362862043082714\n",
      "Batch loss: 0.05416648089885712\n",
      "Batch loss: 0.04639515280723572\n",
      "Batch loss: 0.03709520399570465\n",
      "Batch loss: 0.037619855254888535\n",
      "Batch loss: 0.04003085196018219\n",
      "Batch loss: 0.03861561790108681\n",
      "Batch loss: 0.0742870643734932\n",
      "Batch loss: 0.022705556824803352\n",
      "Batch loss: 0.028592562302947044\n",
      "Batch loss: 0.06496389210224152\n",
      "Batch loss: 0.04663413390517235\n",
      "Batch loss: 0.03879418224096298\n",
      "Batch loss: 0.04176224395632744\n",
      "Batch loss: 0.041751258075237274\n",
      "Batch loss: 0.08226220309734344\n",
      "Batch loss: 0.014972313307225704\n",
      "Batch loss: 0.06625868380069733\n",
      "Batch loss: 0.028837749734520912\n",
      "Batch loss: 0.09432614594697952\n",
      "Batch loss: 0.02868770621716976\n",
      "Batch loss: 0.04261305183172226\n",
      "Batch loss: 0.04113619774580002\n",
      "Batch loss: 0.029494185000658035\n",
      "Batch loss: 0.030297188088297844\n",
      "Batch loss: 0.06278140097856522\n",
      "Batch loss: 0.08132115006446838\n",
      "Batch loss: 0.03629891201853752\n",
      "Batch loss: 0.010165954008698463\n",
      "Batch loss: 0.035294756293296814\n",
      "Batch loss: 0.08414214849472046\n",
      "Batch loss: 0.03093096613883972\n",
      "Batch loss: 0.07444846630096436\n",
      "Batch loss: 0.01878311112523079\n",
      "Batch loss: 0.03862177953124046\n",
      "Batch loss: 0.06682863086462021\n",
      "Batch loss: 0.02218528464436531\n",
      "Batch loss: 0.07338346540927887\n",
      "Batch loss: 0.09484241902828217\n",
      "Batch loss: 0.04396224766969681\n",
      "Batch loss: 0.03869883343577385\n",
      "Batch loss: 0.034542229026556015\n",
      "Batch loss: 0.11393211781978607\n",
      "Batch loss: 0.03922651335597038\n",
      "Batch loss: 0.04933791235089302\n",
      "Batch loss: 0.02013782039284706\n",
      "Batch loss: 0.04690416529774666\n",
      "Batch loss: 0.022171970456838608\n",
      "Batch loss: 0.08484643697738647\n",
      "Batch loss: 0.055639103055000305\n",
      "Batch loss: 0.07334449142217636\n",
      "Batch loss: 0.040241677314043045\n",
      "Batch loss: 0.02683706022799015\n",
      "Batch loss: 0.08096889406442642\n",
      "Batch loss: 0.02812720090150833\n",
      "Batch loss: 0.06264688819646835\n",
      "Batch loss: 0.1275356113910675\n",
      "Batch loss: 0.026933293789625168\n",
      "Batch loss: 0.08381017297506332\n",
      "Batch loss: 0.05007026344537735\n",
      "Batch loss: 0.025841370224952698\n",
      "Batch loss: 0.14394955337047577\n",
      "Batch loss: 0.04451071470975876\n",
      "Batch loss: 0.06487959623336792\n",
      "Batch loss: 0.056664157658815384\n",
      "Batch loss: 0.056043341755867004\n",
      "Batch loss: 0.04328948259353638\n",
      "Batch loss: 0.04369930922985077\n",
      "Batch loss: 0.12948745489120483\n",
      "Batch loss: 0.03429742902517319\n",
      "Batch loss: 0.1241762638092041\n",
      "Batch loss: 0.06958450376987457\n",
      "Batch loss: 0.06072894111275673\n",
      "Batch loss: 0.09285897016525269\n",
      "Batch loss: 0.03664806857705116\n",
      "Batch loss: 0.04966478422284126\n",
      "Batch loss: 0.01802125945687294\n",
      "Batch loss: 0.04295838251709938\n",
      "Batch loss: 0.07048548758029938\n",
      "Batch loss: 0.05003596097230911\n",
      "Batch loss: 0.03829284757375717\n",
      "Batch loss: 0.05134345591068268\n",
      "Batch loss: 0.07005753368139267\n",
      "Batch loss: 0.0355021096765995\n",
      "Batch loss: 0.04717816039919853\n",
      "Batch loss: 0.10259724408388138\n",
      "Batch loss: 0.09001965820789337\n",
      "Batch loss: 0.06350655853748322\n",
      "Batch loss: 0.025621866807341576\n",
      "Batch loss: 0.05144118145108223\n",
      "Batch loss: 0.028927603736519814\n",
      "Batch loss: 0.02219950594007969\n",
      "Batch loss: 0.05782462656497955\n",
      "Batch loss: 0.018972693011164665\n",
      "Batch loss: 0.10504689067602158\n",
      "Batch loss: 0.05640077590942383\n",
      "Batch loss: 0.026748377829790115\n",
      "Batch loss: 0.1080501601099968\n",
      "Batch loss: 0.05650388076901436\n",
      "Batch loss: 0.043674126267433167\n",
      "Batch loss: 0.0114003149792552\n",
      "Batch loss: 0.0806364119052887\n",
      "Batch loss: 0.03184521943330765\n",
      "Batch loss: 0.01963922567665577\n",
      "Batch loss: 0.010584183968603611\n",
      "Batch loss: 0.03887384012341499\n",
      "Batch loss: 0.05719134211540222\n",
      "Batch loss: 0.046700119972229004\n",
      "Batch loss: 0.08917633444070816\n",
      "Epoch [9/100], Loss: 0.0576\n",
      "Validation Loss: 0.7332, Accuracy: 88.49%\n",
      "Batch loss: 0.03598976507782936\n",
      "Batch loss: 0.027198655530810356\n",
      "Batch loss: 0.022357720881700516\n",
      "Batch loss: 0.07698053866624832\n",
      "Batch loss: 0.02688896842300892\n",
      "Batch loss: 0.04518686234951019\n",
      "Batch loss: 0.08558174967765808\n",
      "Batch loss: 0.06797736138105392\n",
      "Batch loss: 0.15052233636379242\n",
      "Batch loss: 0.043742433190345764\n",
      "Batch loss: 0.03224508464336395\n",
      "Batch loss: 0.05342062562704086\n",
      "Batch loss: 0.044259969145059586\n",
      "Batch loss: 0.1156550943851471\n",
      "Batch loss: 0.04114984720945358\n",
      "Batch loss: 0.02272944711148739\n",
      "Batch loss: 0.03998171538114548\n",
      "Batch loss: 0.05306044965982437\n",
      "Batch loss: 0.011147264391183853\n",
      "Batch loss: 0.06977880001068115\n",
      "Batch loss: 0.046987056732177734\n",
      "Batch loss: 0.033359210938215256\n",
      "Batch loss: 0.06267035752534866\n",
      "Batch loss: 0.024459993466734886\n",
      "Batch loss: 0.019107157364487648\n",
      "Batch loss: 0.0485200397670269\n",
      "Batch loss: 0.07313796877861023\n",
      "Batch loss: 0.09761980921030045\n",
      "Batch loss: 0.05275431647896767\n",
      "Batch loss: 0.020842667669057846\n",
      "Batch loss: 0.0764835849404335\n",
      "Batch loss: 0.10544904321432114\n",
      "Batch loss: 0.03849667310714722\n",
      "Batch loss: 0.012920210137963295\n",
      "Batch loss: 0.027644379064440727\n",
      "Batch loss: 0.07270905375480652\n",
      "Batch loss: 0.03976612910628319\n",
      "Batch loss: 0.04673845320940018\n",
      "Batch loss: 0.09113059937953949\n",
      "Batch loss: 0.0271291546523571\n",
      "Batch loss: 0.026693280786275864\n",
      "Batch loss: 0.04936659708619118\n",
      "Batch loss: 0.054997239261865616\n",
      "Batch loss: 0.07809507846832275\n",
      "Batch loss: 0.026409214362502098\n",
      "Batch loss: 0.04799286276102066\n",
      "Batch loss: 0.039142463356256485\n",
      "Batch loss: 0.058559805154800415\n",
      "Batch loss: 0.07369707524776459\n",
      "Batch loss: 0.026921359822154045\n",
      "Batch loss: 0.07888026535511017\n",
      "Batch loss: 0.08215019106864929\n",
      "Batch loss: 0.033502139151096344\n",
      "Batch loss: 0.04766188561916351\n",
      "Batch loss: 0.0411958247423172\n",
      "Batch loss: 0.05329476296901703\n",
      "Batch loss: 0.05799880623817444\n",
      "Batch loss: 0.03826127573847771\n",
      "Batch loss: 0.028929749503731728\n",
      "Batch loss: 0.05033007636666298\n",
      "Batch loss: 0.11454637348651886\n",
      "Batch loss: 0.041680190712213516\n",
      "Batch loss: 0.051165785640478134\n",
      "Batch loss: 0.07132486999034882\n",
      "Batch loss: 0.014464503154158592\n",
      "Batch loss: 0.03345148637890816\n",
      "Batch loss: 0.07438390702009201\n",
      "Batch loss: 0.09862998872995377\n",
      "Batch loss: 0.06795419007539749\n",
      "Batch loss: 0.03949015215039253\n",
      "Batch loss: 0.09212705492973328\n",
      "Batch loss: 0.060400255024433136\n",
      "Batch loss: 0.023144414648413658\n",
      "Batch loss: 0.07265481352806091\n",
      "Batch loss: 0.03476671129465103\n",
      "Batch loss: 0.045491866767406464\n",
      "Batch loss: 0.03936422988772392\n",
      "Batch loss: 0.07994826883077621\n",
      "Batch loss: 0.050618741661310196\n",
      "Batch loss: 0.07948382943868637\n",
      "Batch loss: 0.010802132077515125\n",
      "Batch loss: 0.07317731529474258\n",
      "Batch loss: 0.062403079122304916\n",
      "Batch loss: 0.08225283771753311\n",
      "Batch loss: 0.05057213827967644\n",
      "Batch loss: 0.06598386913537979\n",
      "Batch loss: 0.021617107093334198\n",
      "Batch loss: 0.03256016969680786\n",
      "Batch loss: 0.07330373674631119\n",
      "Batch loss: 0.04835129901766777\n",
      "Batch loss: 0.0619654580950737\n",
      "Batch loss: 0.05606364086270332\n",
      "Batch loss: 0.055630989372730255\n",
      "Batch loss: 0.07120432704687119\n",
      "Batch loss: 0.02448977902531624\n",
      "Batch loss: 0.03393023461103439\n",
      "Batch loss: 0.03411896526813507\n",
      "Batch loss: 0.03342857584357262\n",
      "Batch loss: 0.047743361443281174\n",
      "Batch loss: 0.09939688444137573\n",
      "Batch loss: 0.09649483859539032\n",
      "Batch loss: 0.05377648398280144\n",
      "Batch loss: 0.03617217764258385\n",
      "Batch loss: 0.050749748945236206\n",
      "Batch loss: 0.014539584517478943\n",
      "Batch loss: 0.04901423305273056\n",
      "Batch loss: 0.052197158336639404\n",
      "Batch loss: 0.1149447038769722\n",
      "Batch loss: 0.032461196184158325\n",
      "Batch loss: 0.025921449065208435\n",
      "Batch loss: 0.03253566473722458\n",
      "Batch loss: 0.016716791316866875\n",
      "Batch loss: 0.013619521632790565\n",
      "Batch loss: 0.014839896000921726\n",
      "Batch loss: 0.03130462020635605\n",
      "Batch loss: 0.05471841245889664\n",
      "Batch loss: 0.027498798444867134\n",
      "Batch loss: 0.006956775672733784\n",
      "Batch loss: 0.023951949551701546\n",
      "Batch loss: 0.044591955840587616\n",
      "Batch loss: 0.020433099940419197\n",
      "Batch loss: 0.06760557740926743\n",
      "Batch loss: 0.017156217247247696\n",
      "Batch loss: 0.06242414563894272\n",
      "Batch loss: 0.06348740309476852\n",
      "Batch loss: 0.08383054286241531\n",
      "Batch loss: 0.026189018040895462\n",
      "Batch loss: 0.037907544523477554\n",
      "Batch loss: 0.03663928806781769\n",
      "Batch loss: 0.051348552107810974\n",
      "Batch loss: 0.04632047191262245\n",
      "Batch loss: 0.04090644419193268\n",
      "Batch loss: 0.0765979215502739\n",
      "Batch loss: 0.10319048911333084\n",
      "Batch loss: 0.030126623809337616\n",
      "Batch loss: 0.033295609056949615\n",
      "Batch loss: 0.02444329671561718\n",
      "Batch loss: 0.0320701040327549\n",
      "Batch loss: 0.028886761516332626\n",
      "Batch loss: 0.011653834953904152\n",
      "Batch loss: 0.03056986629962921\n",
      "Batch loss: 0.06891182065010071\n",
      "Batch loss: 0.03634160757064819\n",
      "Batch loss: 0.05661318078637123\n",
      "Batch loss: 0.03409624844789505\n",
      "Batch loss: 0.021441351622343063\n",
      "Batch loss: 0.029449690133333206\n",
      "Batch loss: 0.03483433648943901\n",
      "Batch loss: 0.04504438489675522\n",
      "Batch loss: 0.03896477073431015\n",
      "Batch loss: 0.02363663539290428\n",
      "Batch loss: 0.021497879177331924\n",
      "Batch loss: 0.08195985108613968\n",
      "Batch loss: 0.08642782270908356\n",
      "Batch loss: 0.05836673080921173\n",
      "Batch loss: 0.04123654589056969\n",
      "Batch loss: 0.1073596253991127\n",
      "Batch loss: 0.04158547893166542\n",
      "Batch loss: 0.035790979862213135\n",
      "Batch loss: 0.021254047751426697\n",
      "Batch loss: 0.012861926108598709\n",
      "Batch loss: 0.055586326867341995\n",
      "Batch loss: 0.043693434447050095\n",
      "Batch loss: 0.05417187884449959\n",
      "Batch loss: 0.035311318933963776\n",
      "Batch loss: 0.07443200051784515\n",
      "Batch loss: 0.03902743384242058\n",
      "Batch loss: 0.02244928479194641\n",
      "Batch loss: 0.056472048163414\n",
      "Batch loss: 0.05394335091114044\n",
      "Batch loss: 0.04347847029566765\n",
      "Batch loss: 0.02853851206600666\n",
      "Batch loss: 0.04564231261610985\n",
      "Batch loss: 0.017779555171728134\n",
      "Batch loss: 0.061495065689086914\n",
      "Batch loss: 0.10352415591478348\n",
      "Batch loss: 0.04702000692486763\n",
      "Batch loss: 0.024966532364487648\n",
      "Batch loss: 0.06682727485895157\n",
      "Batch loss: 0.06796872615814209\n",
      "Batch loss: 0.02688584476709366\n",
      "Batch loss: 0.08241415023803711\n",
      "Batch loss: 0.04328686371445656\n",
      "Batch loss: 0.061158277094364166\n",
      "Batch loss: 0.046065907925367355\n",
      "Batch loss: 0.08519614487886429\n",
      "Batch loss: 0.04630853235721588\n",
      "Batch loss: 0.023956626653671265\n",
      "Batch loss: 0.029414525255560875\n",
      "Batch loss: 0.04144444316625595\n",
      "Batch loss: 0.04270508512854576\n",
      "Batch loss: 0.049637991935014725\n",
      "Batch loss: 0.049475185573101044\n",
      "Batch loss: 0.029839158058166504\n",
      "Batch loss: 0.038569360971450806\n",
      "Batch loss: 0.018388312309980392\n",
      "Batch loss: 0.05167573317885399\n",
      "Batch loss: 0.055923979729413986\n",
      "Batch loss: 0.08629319071769714\n",
      "Batch loss: 0.05973824858665466\n",
      "Batch loss: 0.10225465893745422\n",
      "Batch loss: 0.09382618218660355\n",
      "Batch loss: 0.04840392246842384\n",
      "Batch loss: 0.0358663834631443\n",
      "Batch loss: 0.10023999214172363\n",
      "Batch loss: 0.040404703468084335\n",
      "Batch loss: 0.05425135791301727\n",
      "Batch loss: 0.14178505539894104\n",
      "Batch loss: 0.009610693901777267\n",
      "Batch loss: 0.043969932943582535\n",
      "Batch loss: 0.03430689126253128\n",
      "Batch loss: 0.09351124614477158\n",
      "Batch loss: 0.04137042537331581\n",
      "Batch loss: 0.047375112771987915\n",
      "Batch loss: 0.11516691744327545\n",
      "Batch loss: 0.01576397381722927\n",
      "Batch loss: 0.032228097319602966\n",
      "Batch loss: 0.04024235159158707\n",
      "Batch loss: 0.01766376942396164\n",
      "Batch loss: 0.03371015191078186\n",
      "Batch loss: 0.03669433668255806\n",
      "Batch loss: 0.02716817334294319\n",
      "Batch loss: 0.03380386531352997\n",
      "Batch loss: 0.08745787292718887\n",
      "Batch loss: 0.044125527143478394\n",
      "Batch loss: 0.035977866500616074\n",
      "Batch loss: 0.014033771120011806\n",
      "Batch loss: 0.07241129875183105\n",
      "Batch loss: 0.06427504122257233\n",
      "Batch loss: 0.052835069596767426\n",
      "Batch loss: 0.029338151216506958\n",
      "Batch loss: 0.023619143292307854\n",
      "Batch loss: 0.03969784453511238\n",
      "Batch loss: 0.03172959387302399\n",
      "Batch loss: 0.03592202812433243\n",
      "Batch loss: 0.08713176101446152\n",
      "Batch loss: 0.09261669218540192\n",
      "Batch loss: 0.07248801738023758\n",
      "Batch loss: 0.08408331871032715\n",
      "Batch loss: 0.02586521953344345\n",
      "Batch loss: 0.05580822378396988\n",
      "Batch loss: 0.044113919138908386\n",
      "Batch loss: 0.04687819257378578\n",
      "Batch loss: 0.04155230149626732\n",
      "Batch loss: 0.028995826840400696\n",
      "Batch loss: 0.04081392660737038\n",
      "Batch loss: 0.08302044868469238\n",
      "Batch loss: 0.022252384573221207\n",
      "Batch loss: 0.018527258187532425\n",
      "Batch loss: 0.03369377180933952\n",
      "Batch loss: 0.01874806545674801\n",
      "Batch loss: 0.14282198250293732\n",
      "Batch loss: 0.048010487109422684\n",
      "Batch loss: 0.06953200697898865\n",
      "Batch loss: 0.05952678993344307\n",
      "Batch loss: 0.045191433280706406\n",
      "Batch loss: 0.04652830585837364\n",
      "Batch loss: 0.032551754266023636\n",
      "Batch loss: 0.022115616127848625\n",
      "Batch loss: 0.07364534586668015\n",
      "Batch loss: 0.060338906943798065\n",
      "Batch loss: 0.07296460121870041\n",
      "Batch loss: 0.052336305379867554\n",
      "Batch loss: 0.02143603190779686\n",
      "Batch loss: 0.03228861093521118\n",
      "Batch loss: 0.015269092284142971\n",
      "Batch loss: 0.07753665745258331\n",
      "Batch loss: 0.04272208735346794\n",
      "Batch loss: 0.01960412785410881\n",
      "Batch loss: 0.07578516751527786\n",
      "Batch loss: 0.044241972267627716\n",
      "Batch loss: 0.038308050483465195\n",
      "Batch loss: 0.026827936992049217\n",
      "Batch loss: 0.046777207404375076\n",
      "Batch loss: 0.04540177062153816\n",
      "Batch loss: 0.02891753427684307\n",
      "Batch loss: 0.03845572844147682\n",
      "Batch loss: 0.011353486217558384\n",
      "Batch loss: 0.02922488935291767\n",
      "Batch loss: 0.10145901888608932\n",
      "Batch loss: 0.06454694271087646\n",
      "Batch loss: 0.01407986506819725\n",
      "Batch loss: 0.07378792762756348\n",
      "Batch loss: 0.07132812589406967\n",
      "Batch loss: 0.02269832044839859\n",
      "Batch loss: 0.049164850264787674\n",
      "Batch loss: 0.030672963708639145\n",
      "Batch loss: 0.016000879928469658\n",
      "Batch loss: 0.028574928641319275\n",
      "Batch loss: 0.08770820498466492\n",
      "Batch loss: 0.02454269304871559\n",
      "Batch loss: 0.0325278677046299\n",
      "Batch loss: 0.029816722497344017\n",
      "Batch loss: 0.06054532155394554\n",
      "Batch loss: 0.04298939183354378\n",
      "Batch loss: 0.029510946944355965\n",
      "Batch loss: 0.04501109570264816\n",
      "Batch loss: 0.09223070740699768\n",
      "Batch loss: 0.05062110349535942\n",
      "Batch loss: 0.06737469136714935\n",
      "Batch loss: 0.015163958072662354\n",
      "Batch loss: 0.019070984795689583\n",
      "Batch loss: 0.07521407306194305\n",
      "Batch loss: 0.014051465317606926\n",
      "Batch loss: 0.04644041880965233\n",
      "Batch loss: 0.08223898708820343\n",
      "Batch loss: 0.012792019173502922\n",
      "Batch loss: 0.04859966039657593\n",
      "Batch loss: 0.07341904938220978\n",
      "Batch loss: 0.03313186764717102\n",
      "Batch loss: 0.07389937341213226\n",
      "Batch loss: 0.027014046907424927\n",
      "Batch loss: 0.01802149973809719\n",
      "Batch loss: 0.009470373392105103\n",
      "Batch loss: 0.04565262794494629\n",
      "Batch loss: 0.04442660138010979\n",
      "Batch loss: 0.019723812118172646\n",
      "Batch loss: 0.013077201321721077\n",
      "Batch loss: 0.017300941050052643\n",
      "Batch loss: 0.03982635214924812\n",
      "Batch loss: 0.04122230038046837\n",
      "Batch loss: 0.030803706496953964\n",
      "Batch loss: 0.07233542203903198\n",
      "Batch loss: 0.08518044650554657\n",
      "Batch loss: 0.02609650231897831\n",
      "Batch loss: 0.06428747624158859\n",
      "Batch loss: 0.06732342392206192\n",
      "Batch loss: 0.03137670084834099\n",
      "Batch loss: 0.05713104456663132\n",
      "Batch loss: 0.04725181311368942\n",
      "Batch loss: 0.016937311738729477\n",
      "Batch loss: 0.0934477373957634\n",
      "Batch loss: 0.047543369233608246\n",
      "Batch loss: 0.03789004683494568\n",
      "Batch loss: 0.08418839424848557\n",
      "Batch loss: 0.039112892001867294\n",
      "Batch loss: 0.004503085743635893\n",
      "Batch loss: 0.016182973980903625\n",
      "Batch loss: 0.03158232197165489\n",
      "Batch loss: 0.014785656705498695\n",
      "Batch loss: 0.035260897129774094\n",
      "Batch loss: 0.07408294081687927\n",
      "Batch loss: 0.011281548999249935\n",
      "Batch loss: 0.10152103751897812\n",
      "Batch loss: 0.049006137996912\n",
      "Batch loss: 0.03965422138571739\n",
      "Batch loss: 0.06238628178834915\n",
      "Batch loss: 0.09588174521923065\n",
      "Batch loss: 0.01611674763262272\n",
      "Batch loss: 0.052580736577510834\n",
      "Batch loss: 0.042701683938503265\n",
      "Batch loss: 0.0394362136721611\n",
      "Batch loss: 0.03549233451485634\n",
      "Batch loss: 0.026404503732919693\n",
      "Batch loss: 0.041403066366910934\n",
      "Batch loss: 0.023647580295801163\n",
      "Batch loss: 0.020042937248945236\n",
      "Batch loss: 0.10046352446079254\n",
      "Batch loss: 0.046476688235998154\n",
      "Batch loss: 0.04033098369836807\n",
      "Batch loss: 0.03022269532084465\n",
      "Batch loss: 0.03831889107823372\n",
      "Batch loss: 0.025802193209528923\n",
      "Batch loss: 0.042665377259254456\n",
      "Batch loss: 0.06706845015287399\n",
      "Batch loss: 0.05863985791802406\n",
      "Batch loss: 0.024851301684975624\n",
      "Batch loss: 0.027020124718546867\n",
      "Batch loss: 0.048668328672647476\n",
      "Batch loss: 0.08304864168167114\n",
      "Batch loss: 0.0409042127430439\n",
      "Batch loss: 0.014853417873382568\n",
      "Batch loss: 0.0440325066447258\n",
      "Batch loss: 0.06785604357719421\n",
      "Batch loss: 0.04297058656811714\n",
      "Batch loss: 0.012232251465320587\n",
      "Batch loss: 0.055800482630729675\n",
      "Batch loss: 0.058776628226041794\n",
      "Batch loss: 0.070217564702034\n",
      "Batch loss: 0.08465424925088882\n",
      "Batch loss: 0.05796750262379646\n",
      "Batch loss: 0.053093116730451584\n",
      "Batch loss: 0.06850902736186981\n",
      "Batch loss: 0.0841996967792511\n",
      "Batch loss: 0.019956210628151894\n",
      "Batch loss: 0.022359108552336693\n",
      "Batch loss: 0.024804461747407913\n",
      "Batch loss: 0.04004295542836189\n",
      "Batch loss: 0.03311006724834442\n",
      "Batch loss: 0.049515824764966965\n",
      "Batch loss: 0.036759816110134125\n",
      "Batch loss: 0.057286832481622696\n",
      "Batch loss: 0.01570892706513405\n",
      "Batch loss: 0.0447942316532135\n",
      "Batch loss: 0.049736037850379944\n",
      "Batch loss: 0.05366220697760582\n",
      "Batch loss: 0.015533990226686\n",
      "Batch loss: 0.019195757806301117\n",
      "Batch loss: 0.04926477372646332\n",
      "Batch loss: 0.009391698986291885\n",
      "Batch loss: 0.01877189800143242\n",
      "Batch loss: 0.07684966176748276\n",
      "Batch loss: 0.08283688873052597\n",
      "Batch loss: 0.023020880296826363\n",
      "Batch loss: 0.04443831741809845\n",
      "Batch loss: 0.0240702535957098\n",
      "Batch loss: 0.026960214599967003\n",
      "Batch loss: 0.06553878635168076\n",
      "Batch loss: 0.024348178878426552\n",
      "Batch loss: 0.03575757145881653\n",
      "Batch loss: 0.06557492911815643\n",
      "Batch loss: 0.026388294994831085\n",
      "Batch loss: 0.023671885952353477\n",
      "Batch loss: 0.02761681377887726\n",
      "Batch loss: 0.025103846564888954\n",
      "Batch loss: 0.02043061889708042\n",
      "Batch loss: 0.07665740698575974\n",
      "Epoch [10/100], Loss: 0.0474\n",
      "Validation Loss: 0.6958, Accuracy: 84.53%\n",
      "Batch loss: 0.04179247468709946\n",
      "Batch loss: 0.02834411710500717\n",
      "Batch loss: 0.03914562240242958\n",
      "Batch loss: 0.059670813381671906\n",
      "Batch loss: 0.06714046746492386\n",
      "Batch loss: 0.09548089653253555\n",
      "Batch loss: 0.05339967459440231\n",
      "Batch loss: 0.014039216563105583\n",
      "Batch loss: 0.03320734202861786\n",
      "Batch loss: 0.01557373721152544\n",
      "Batch loss: 0.03760439157485962\n",
      "Batch loss: 0.03462439775466919\n",
      "Batch loss: 0.04035120829939842\n",
      "Batch loss: 0.08091310411691666\n",
      "Batch loss: 0.034623172134160995\n",
      "Batch loss: 0.06621099263429642\n",
      "Batch loss: 0.027538210153579712\n",
      "Batch loss: 0.026849055662751198\n",
      "Batch loss: 0.008117116056382656\n",
      "Batch loss: 0.05160974711179733\n",
      "Batch loss: 0.017626110464334488\n",
      "Batch loss: 0.022079095244407654\n",
      "Batch loss: 0.028110213577747345\n",
      "Batch loss: 0.013641469180583954\n",
      "Batch loss: 0.035064369440078735\n",
      "Batch loss: 0.010590527206659317\n",
      "Batch loss: 0.03337334468960762\n",
      "Batch loss: 0.07954277843236923\n",
      "Batch loss: 0.05536016821861267\n",
      "Batch loss: 0.021715905517339706\n",
      "Batch loss: 0.12751822173595428\n",
      "Batch loss: 0.025962011888623238\n",
      "Batch loss: 0.03780321776866913\n",
      "Batch loss: 0.04477604478597641\n",
      "Batch loss: 0.049531325697898865\n",
      "Batch loss: 0.04666095972061157\n",
      "Batch loss: 0.025056414306163788\n",
      "Batch loss: 0.013643027283251286\n",
      "Batch loss: 0.050057392567396164\n",
      "Batch loss: 0.02321787178516388\n",
      "Batch loss: 0.05855530500411987\n",
      "Batch loss: 0.019648060202598572\n",
      "Batch loss: 0.007500894833356142\n",
      "Batch loss: 0.056470759212970734\n",
      "Batch loss: 0.05356605350971222\n",
      "Batch loss: 0.07339494675397873\n",
      "Batch loss: 0.03984106704592705\n",
      "Batch loss: 0.02103271335363388\n",
      "Batch loss: 0.02424614690244198\n",
      "Batch loss: 0.04432682693004608\n",
      "Batch loss: 0.051133230328559875\n",
      "Batch loss: 0.016512323170900345\n",
      "Batch loss: 0.0628531277179718\n",
      "Batch loss: 0.06619613617658615\n",
      "Batch loss: 0.047802239656448364\n",
      "Batch loss: 0.038906969130039215\n",
      "Batch loss: 0.03288821876049042\n",
      "Batch loss: 0.03131178766489029\n",
      "Batch loss: 0.08029261231422424\n",
      "Batch loss: 0.045821793377399445\n",
      "Batch loss: 0.02831794135272503\n",
      "Batch loss: 0.02334064617753029\n",
      "Batch loss: 0.05575628951191902\n",
      "Batch loss: 0.049505602568387985\n",
      "Batch loss: 0.030807068571448326\n",
      "Batch loss: 0.0837726816534996\n",
      "Batch loss: 0.07213325798511505\n",
      "Batch loss: 0.05969559773802757\n",
      "Batch loss: 0.027110345661640167\n",
      "Batch loss: 0.04216401278972626\n",
      "Batch loss: 0.023830275982618332\n",
      "Batch loss: 0.008739140816032887\n",
      "Batch loss: 0.05849793925881386\n",
      "Batch loss: 0.01615706831216812\n",
      "Batch loss: 0.018617069348692894\n",
      "Batch loss: 0.031348325312137604\n",
      "Batch loss: 0.13763867318630219\n",
      "Batch loss: 0.05999854579567909\n",
      "Batch loss: 0.050638213753700256\n",
      "Batch loss: 0.04161539673805237\n",
      "Batch loss: 0.026738101616501808\n",
      "Batch loss: 0.04351387917995453\n",
      "Batch loss: 0.04565590247511864\n",
      "Batch loss: 0.05982248857617378\n",
      "Batch loss: 0.017244692891836166\n",
      "Batch loss: 0.03605068102478981\n",
      "Batch loss: 0.011994872242212296\n",
      "Batch loss: 0.04553676396608353\n",
      "Batch loss: 0.026279764249920845\n",
      "Batch loss: 0.006997341755777597\n",
      "Batch loss: 0.034983061254024506\n",
      "Batch loss: 0.04182739183306694\n",
      "Batch loss: 0.06560040265321732\n",
      "Batch loss: 0.08576799929141998\n",
      "Batch loss: 0.0587117001414299\n",
      "Batch loss: 0.07659201323986053\n",
      "Batch loss: 0.09098517149686813\n",
      "Batch loss: 0.05650743097066879\n",
      "Batch loss: 0.08738476037979126\n",
      "Batch loss: 0.06333566457033157\n",
      "Batch loss: 0.04737001284956932\n",
      "Batch loss: 0.020435115322470665\n",
      "Batch loss: 0.027038533240556717\n",
      "Batch loss: 0.04187851399183273\n",
      "Batch loss: 0.022456862032413483\n",
      "Batch loss: 0.04898730665445328\n",
      "Batch loss: 0.03864991292357445\n",
      "Batch loss: 0.06469596177339554\n",
      "Batch loss: 0.08838094770908356\n",
      "Batch loss: 0.04079313948750496\n",
      "Batch loss: 0.07723093777894974\n",
      "Batch loss: 0.030995285138487816\n",
      "Batch loss: 0.024394867941737175\n",
      "Batch loss: 0.05540674179792404\n",
      "Batch loss: 0.04128243774175644\n",
      "Batch loss: 0.06179842725396156\n",
      "Batch loss: 0.01529947854578495\n",
      "Batch loss: 0.0291037168353796\n",
      "Batch loss: 0.019646450877189636\n",
      "Batch loss: 0.026679694652557373\n",
      "Batch loss: 0.04909557104110718\n",
      "Batch loss: 0.04723343625664711\n",
      "Batch loss: 0.10481001436710358\n",
      "Batch loss: 0.03264974057674408\n",
      "Batch loss: 0.13640367984771729\n",
      "Batch loss: 0.0206216461956501\n",
      "Batch loss: 0.04233413189649582\n",
      "Batch loss: 0.026356540620326996\n",
      "Batch loss: 0.02732001803815365\n",
      "Batch loss: 0.03222094476222992\n",
      "Batch loss: 0.022788796573877335\n",
      "Batch loss: 0.04367360100150108\n",
      "Batch loss: 0.07980279624462128\n",
      "Batch loss: 0.07346445322036743\n",
      "Batch loss: 0.0515880286693573\n",
      "Batch loss: 0.012316592037677765\n",
      "Batch loss: 0.019156700000166893\n",
      "Batch loss: 0.0570344440639019\n",
      "Batch loss: 0.019417399540543556\n",
      "Batch loss: 0.018283523619174957\n",
      "Batch loss: 0.03192596137523651\n",
      "Batch loss: 0.04627789184451103\n",
      "Batch loss: 0.024854043498635292\n",
      "Batch loss: 0.052964646369218826\n",
      "Batch loss: 0.023067360743880272\n",
      "Batch loss: 0.07726798951625824\n",
      "Batch loss: 0.03308071196079254\n",
      "Batch loss: 0.009261589497327805\n",
      "Batch loss: 0.042157355695962906\n",
      "Batch loss: 0.019218124449253082\n",
      "Batch loss: 0.02025163359940052\n",
      "Batch loss: 0.054062578827142715\n",
      "Batch loss: 0.04113076254725456\n",
      "Batch loss: 0.03302216902375221\n",
      "Batch loss: 0.03930831700563431\n",
      "Batch loss: 0.016253503039479256\n",
      "Batch loss: 0.06055103987455368\n",
      "Batch loss: 0.036536410450935364\n",
      "Batch loss: 0.03661505877971649\n",
      "Batch loss: 0.025812458246946335\n",
      "Batch loss: 0.01111865695565939\n",
      "Batch loss: 0.06890878826379776\n",
      "Batch loss: 0.024157192558050156\n",
      "Batch loss: 0.030834605917334557\n",
      "Batch loss: 0.04096484184265137\n",
      "Batch loss: 0.01608646847307682\n",
      "Batch loss: 0.01845174841582775\n",
      "Batch loss: 0.02787472866475582\n",
      "Batch loss: 0.008603389374911785\n",
      "Batch loss: 0.04873521625995636\n",
      "Batch loss: 0.05008591338992119\n",
      "Batch loss: 0.0916215032339096\n",
      "Batch loss: 0.04258838668465614\n",
      "Batch loss: 0.07998973876237869\n",
      "Batch loss: 0.026607844978570938\n",
      "Batch loss: 0.039558444172143936\n",
      "Batch loss: 0.0784265473484993\n",
      "Batch loss: 0.01676458865404129\n",
      "Batch loss: 0.05119788646697998\n",
      "Batch loss: 0.020080989226698875\n",
      "Batch loss: 0.021486839279532433\n",
      "Batch loss: 0.031198034062981606\n",
      "Batch loss: 0.018778879195451736\n",
      "Batch loss: 0.03775429725646973\n",
      "Batch loss: 0.037261441349983215\n",
      "Batch loss: 0.031005537137389183\n",
      "Batch loss: 0.03473467007279396\n",
      "Batch loss: 0.020624525845050812\n",
      "Batch loss: 0.03168732672929764\n",
      "Batch loss: 0.06149721145629883\n",
      "Batch loss: 0.024646291509270668\n",
      "Batch loss: 0.03295221924781799\n",
      "Batch loss: 0.03729492798447609\n",
      "Batch loss: 0.04528266564011574\n",
      "Batch loss: 0.013574761338531971\n",
      "Batch loss: 0.011789790354669094\n",
      "Batch loss: 0.007640966214239597\n",
      "Batch loss: 0.013723727315664291\n",
      "Batch loss: 0.026251912117004395\n",
      "Batch loss: 0.035572443157434464\n",
      "Batch loss: 0.0427059531211853\n",
      "Batch loss: 0.05242691934108734\n",
      "Batch loss: 0.026893114671111107\n",
      "Batch loss: 0.03432862088084221\n",
      "Batch loss: 0.06275246292352676\n",
      "Batch loss: 0.07549145072698593\n",
      "Batch loss: 0.04465733468532562\n",
      "Batch loss: 0.06335195899009705\n",
      "Batch loss: 0.04832692816853523\n",
      "Batch loss: 0.0781509205698967\n",
      "Batch loss: 0.021262813359498978\n",
      "Batch loss: 0.04827112704515457\n",
      "Batch loss: 0.06519847363233566\n",
      "Batch loss: 0.028239475563168526\n",
      "Batch loss: 0.005647814366966486\n",
      "Batch loss: 0.0035865299869328737\n",
      "Batch loss: 0.01040794886648655\n",
      "Batch loss: 0.14246845245361328\n",
      "Batch loss: 0.03798249736428261\n",
      "Batch loss: 0.02075805515050888\n",
      "Batch loss: 0.03738578036427498\n",
      "Batch loss: 0.021367130801081657\n",
      "Batch loss: 0.02731076441705227\n",
      "Batch loss: 0.09455148130655289\n",
      "Batch loss: 0.035825710743665695\n",
      "Batch loss: 0.06523402780294418\n",
      "Batch loss: 0.02406894974410534\n",
      "Batch loss: 0.052944473922252655\n",
      "Batch loss: 0.024186117574572563\n",
      "Batch loss: 0.02469845861196518\n",
      "Batch loss: 0.022182824090123177\n",
      "Batch loss: 0.008591619320213795\n",
      "Batch loss: 0.0220553707331419\n",
      "Batch loss: 0.045890867710113525\n",
      "Batch loss: 0.017527146264910698\n",
      "Batch loss: 0.04346263036131859\n",
      "Batch loss: 0.02623351849615574\n",
      "Batch loss: 0.0690237432718277\n",
      "Batch loss: 0.003698061453178525\n",
      "Batch loss: 0.0409841351211071\n",
      "Batch loss: 0.0916655883193016\n",
      "Batch loss: 0.02274862676858902\n",
      "Batch loss: 0.0136872548609972\n",
      "Batch loss: 0.021091273054480553\n",
      "Batch loss: 0.03953642398118973\n",
      "Batch loss: 0.015913410112261772\n",
      "Batch loss: 0.04730577766895294\n",
      "Batch loss: 0.03318824991583824\n",
      "Batch loss: 0.016731606796383858\n",
      "Batch loss: 0.012279130518436432\n",
      "Batch loss: 0.036575060337781906\n",
      "Batch loss: 0.06652792543172836\n",
      "Batch loss: 0.010120843537151814\n",
      "Batch loss: 0.04208977520465851\n",
      "Batch loss: 0.0652938112616539\n",
      "Batch loss: 0.016101956367492676\n",
      "Batch loss: 0.03537760674953461\n",
      "Batch loss: 0.02716386877000332\n",
      "Batch loss: 0.06618285179138184\n",
      "Batch loss: 0.05422555282711983\n",
      "Batch loss: 0.01433043647557497\n",
      "Batch loss: 0.013409744016826153\n",
      "Batch loss: 0.04409116879105568\n",
      "Batch loss: 0.05394862964749336\n",
      "Batch loss: 0.04293902590870857\n",
      "Batch loss: 0.006890703458338976\n",
      "Batch loss: 0.07735385745763779\n",
      "Batch loss: 0.03755315765738487\n",
      "Batch loss: 0.05111600458621979\n",
      "Batch loss: 0.01927550509572029\n",
      "Batch loss: 0.052880872040987015\n",
      "Batch loss: 0.02641872875392437\n",
      "Batch loss: 0.025386514142155647\n",
      "Batch loss: 0.011462757363915443\n",
      "Batch loss: 0.049351319670677185\n",
      "Batch loss: 0.011444401927292347\n",
      "Batch loss: 0.01005873829126358\n",
      "Batch loss: 0.039387162774801254\n",
      "Batch loss: 0.026065943762660027\n",
      "Batch loss: 0.06040222570300102\n",
      "Batch loss: 0.03045748360455036\n",
      "Batch loss: 0.021447934210300446\n",
      "Batch loss: 0.1488264799118042\n",
      "Batch loss: 0.05782711133360863\n",
      "Batch loss: 0.035450320690870285\n",
      "Batch loss: 0.02593955770134926\n",
      "Batch loss: 0.1002466008067131\n",
      "Batch loss: 0.012043178081512451\n",
      "Batch loss: 0.03300681710243225\n",
      "Batch loss: 0.004485222976654768\n",
      "Batch loss: 0.05924385040998459\n",
      "Batch loss: 0.01368444599211216\n",
      "Batch loss: 0.08125581592321396\n",
      "Batch loss: 0.02228148840367794\n",
      "Batch loss: 0.06653152406215668\n",
      "Batch loss: 0.02148628421127796\n",
      "Batch loss: 0.04009779542684555\n",
      "Batch loss: 0.00541716068983078\n",
      "Batch loss: 0.04364083707332611\n",
      "Batch loss: 0.0324532724916935\n",
      "Batch loss: 0.024671191349625587\n",
      "Batch loss: 0.008440508507192135\n",
      "Batch loss: 0.03337298333644867\n",
      "Batch loss: 0.01767463982105255\n",
      "Batch loss: 0.012627040036022663\n",
      "Batch loss: 0.015446341596543789\n",
      "Batch loss: 0.04950057715177536\n",
      "Batch loss: 0.044263094663619995\n",
      "Batch loss: 0.048325128853321075\n",
      "Batch loss: 0.006682703737169504\n",
      "Batch loss: 0.04081011191010475\n",
      "Batch loss: 0.03642023727297783\n",
      "Batch loss: 0.049706730991601944\n",
      "Batch loss: 0.0389702171087265\n",
      "Batch loss: 0.011176514439284801\n",
      "Batch loss: 0.018283313140273094\n",
      "Batch loss: 0.026752114295959473\n",
      "Batch loss: 0.010473977774381638\n",
      "Batch loss: 0.06789332628250122\n",
      "Batch loss: 0.03431253880262375\n",
      "Batch loss: 0.05818478763103485\n",
      "Batch loss: 0.027061481028795242\n",
      "Batch loss: 0.03439044579863548\n",
      "Batch loss: 0.05506531521677971\n",
      "Batch loss: 0.031117845326662064\n",
      "Batch loss: 0.05628881976008415\n",
      "Batch loss: 0.028160538524389267\n",
      "Batch loss: 0.04456712678074837\n",
      "Batch loss: 0.06419961899518967\n",
      "Batch loss: 0.054979849606752396\n",
      "Batch loss: 0.02986626699566841\n",
      "Batch loss: 0.06041932478547096\n",
      "Batch loss: 0.029989007860422134\n",
      "Batch loss: 0.03728387877345085\n",
      "Batch loss: 0.034097298979759216\n",
      "Batch loss: 0.06558695435523987\n",
      "Batch loss: 0.015817582607269287\n",
      "Batch loss: 0.020738646388053894\n",
      "Batch loss: 0.11240758746862411\n",
      "Batch loss: 0.03142863139510155\n",
      "Batch loss: 0.06120679900050163\n",
      "Batch loss: 0.017529550939798355\n",
      "Batch loss: 0.02507779560983181\n",
      "Batch loss: 0.049013495445251465\n",
      "Batch loss: 0.0071531678549945354\n",
      "Batch loss: 0.042586810886859894\n",
      "Batch loss: 0.03163733333349228\n",
      "Batch loss: 0.029115531593561172\n",
      "Batch loss: 0.03230196237564087\n",
      "Batch loss: 0.01831487938761711\n",
      "Batch loss: 0.031179510056972504\n",
      "Batch loss: 0.042777251452207565\n",
      "Batch loss: 0.014737166464328766\n",
      "Batch loss: 0.043427273631095886\n",
      "Batch loss: 0.089579276740551\n",
      "Batch loss: 0.03478158637881279\n",
      "Batch loss: 0.016630331054329872\n",
      "Batch loss: 0.038891296833753586\n",
      "Batch loss: 0.09754389524459839\n",
      "Batch loss: 0.036502495408058167\n",
      "Batch loss: 0.012488052248954773\n",
      "Batch loss: 0.06588145345449448\n",
      "Batch loss: 0.0045699914917349815\n",
      "Batch loss: 0.027325617149472237\n",
      "Batch loss: 0.06278835237026215\n",
      "Batch loss: 0.02567155659198761\n",
      "Batch loss: 0.02007279358804226\n",
      "Batch loss: 0.03585450351238251\n",
      "Batch loss: 0.013407601043581963\n",
      "Batch loss: 0.008321208879351616\n",
      "Batch loss: 0.02520047128200531\n",
      "Batch loss: 0.042079128324985504\n",
      "Batch loss: 0.03404071927070618\n",
      "Batch loss: 0.03290710970759392\n",
      "Batch loss: 0.048006556928157806\n",
      "Batch loss: 0.021581212058663368\n",
      "Batch loss: 0.0490863136947155\n",
      "Batch loss: 0.032142117619514465\n",
      "Batch loss: 0.036642689257860184\n",
      "Batch loss: 0.012851106002926826\n",
      "Batch loss: 0.0747527927160263\n",
      "Batch loss: 0.04295259341597557\n",
      "Batch loss: 0.03215258941054344\n",
      "Batch loss: 0.04625852033495903\n",
      "Batch loss: 0.06393706053495407\n",
      "Batch loss: 0.03671843931078911\n",
      "Batch loss: 0.043122898787260056\n",
      "Batch loss: 0.0513748861849308\n",
      "Batch loss: 0.060664668679237366\n",
      "Batch loss: 0.039402592927217484\n",
      "Batch loss: 0.017206119373440742\n",
      "Batch loss: 0.025799088180065155\n",
      "Batch loss: 0.010720106773078442\n",
      "Batch loss: 0.012565508484840393\n",
      "Batch loss: 0.02356877364218235\n",
      "Batch loss: 0.027622943744063377\n",
      "Batch loss: 0.027305487543344498\n",
      "Batch loss: 0.006528370548039675\n",
      "Batch loss: 0.04032820463180542\n",
      "Batch loss: 0.006277997512370348\n",
      "Batch loss: 0.020788686349987984\n",
      "Batch loss: 0.01875944994390011\n",
      "Batch loss: 0.13341352343559265\n",
      "Batch loss: 0.04720131307840347\n",
      "Batch loss: 0.010220030322670937\n",
      "Batch loss: 0.043559569865465164\n",
      "Batch loss: 0.13002192974090576\n",
      "Batch loss: 0.06878773868083954\n",
      "Batch loss: 0.020785016939044\n",
      "Batch loss: 0.04191434010863304\n",
      "Batch loss: 0.05591890588402748\n",
      "Batch loss: 0.08920805156230927\n",
      "Batch loss: 0.007898999378085136\n",
      "Batch loss: 0.056286439299583435\n",
      "Batch loss: 0.03367822989821434\n",
      "Batch loss: 0.08176606893539429\n",
      "Batch loss: 0.02646789140999317\n",
      "Epoch [11/100], Loss: 0.0397\n",
      "Validation Loss: 0.3631, Accuracy: 91.57%\n",
      "Batch loss: 0.021605277433991432\n",
      "Batch loss: 0.03504403680562973\n",
      "Batch loss: 0.0423877015709877\n",
      "Batch loss: 0.017998995259404182\n",
      "Batch loss: 0.0331636518239975\n",
      "Batch loss: 0.038019731640815735\n",
      "Batch loss: 0.1134517565369606\n",
      "Batch loss: 0.055302225053310394\n",
      "Batch loss: 0.046375472098588943\n",
      "Batch loss: 0.009528560563921928\n",
      "Batch loss: 0.11346083879470825\n",
      "Batch loss: 0.06179030239582062\n",
      "Batch loss: 0.017399923875927925\n",
      "Batch loss: 0.12156882882118225\n",
      "Batch loss: 0.025469647720456123\n",
      "Batch loss: 0.10705705732107162\n",
      "Batch loss: 0.05578825622797012\n",
      "Batch loss: 0.056128401309251785\n",
      "Batch loss: 0.02162335440516472\n",
      "Batch loss: 0.06305955350399017\n",
      "Batch loss: 0.05313679203391075\n",
      "Batch loss: 0.027401696890592575\n",
      "Batch loss: 0.0337919183075428\n",
      "Batch loss: 0.029702385887503624\n",
      "Batch loss: 0.010385802946984768\n",
      "Batch loss: 0.023107679560780525\n",
      "Batch loss: 0.02400824800133705\n",
      "Batch loss: 0.02929576113820076\n",
      "Batch loss: 0.02278381586074829\n",
      "Batch loss: 0.03100985288619995\n",
      "Batch loss: 0.03997116535902023\n",
      "Batch loss: 0.04622512683272362\n",
      "Batch loss: 0.07498747110366821\n",
      "Batch loss: 0.03338427469134331\n",
      "Batch loss: 0.02429037354886532\n",
      "Batch loss: 0.06055143475532532\n",
      "Batch loss: 0.0586848184466362\n",
      "Batch loss: 0.05854346603155136\n",
      "Batch loss: 0.039453160017728806\n",
      "Batch loss: 0.06205429509282112\n",
      "Batch loss: 0.03256446495652199\n",
      "Batch loss: 0.026650773361325264\n",
      "Batch loss: 0.019871503114700317\n",
      "Batch loss: 0.027692969888448715\n",
      "Batch loss: 0.013066506013274193\n",
      "Batch loss: 0.05141964927315712\n",
      "Batch loss: 0.025317536666989326\n",
      "Batch loss: 0.05255027115345001\n",
      "Batch loss: 0.014908459968864918\n",
      "Batch loss: 0.02358067035675049\n",
      "Batch loss: 0.023638207465410233\n",
      "Batch loss: 0.024267368018627167\n",
      "Batch loss: 0.04047708213329315\n",
      "Batch loss: 0.03343168646097183\n",
      "Batch loss: 0.01571076177060604\n",
      "Batch loss: 0.038136161863803864\n",
      "Batch loss: 0.023077744990587234\n",
      "Batch loss: 0.026401452720165253\n",
      "Batch loss: 0.020116934552788734\n",
      "Batch loss: 0.06770210713148117\n",
      "Batch loss: 0.04494122415781021\n",
      "Batch loss: 0.04212387278676033\n",
      "Batch loss: 0.02866789884865284\n",
      "Batch loss: 0.01805010810494423\n",
      "Batch loss: 0.012029250152409077\n",
      "Batch loss: 0.04316627234220505\n",
      "Batch loss: 0.026479411870241165\n",
      "Batch loss: 0.029720192775130272\n",
      "Batch loss: 0.012449990957975388\n",
      "Batch loss: 0.05279906466603279\n",
      "Batch loss: 0.017703499644994736\n",
      "Batch loss: 0.05488191172480583\n",
      "Batch loss: 0.014116954058408737\n",
      "Batch loss: 0.013977648690342903\n",
      "Batch loss: 0.009204518049955368\n",
      "Batch loss: 0.012397076934576035\n",
      "Batch loss: 0.028768204152584076\n",
      "Batch loss: 0.02044672891497612\n",
      "Batch loss: 0.03202393278479576\n",
      "Batch loss: 0.1117875725030899\n",
      "Batch loss: 0.03343374282121658\n",
      "Batch loss: 0.023221254348754883\n",
      "Batch loss: 0.03933624178171158\n",
      "Batch loss: 0.04405881091952324\n",
      "Batch loss: 0.04622120410203934\n",
      "Batch loss: 0.07042109221220016\n",
      "Batch loss: 0.013773789629340172\n",
      "Batch loss: 0.0432220883667469\n",
      "Batch loss: 0.020740052685141563\n",
      "Batch loss: 0.010017607361078262\n",
      "Batch loss: 0.05988733470439911\n",
      "Batch loss: 0.025659935548901558\n",
      "Batch loss: 0.029430894181132317\n",
      "Batch loss: 0.03272810950875282\n",
      "Batch loss: 0.010137921199202538\n",
      "Batch loss: 0.01866251416504383\n",
      "Batch loss: 0.0340990349650383\n",
      "Batch loss: 0.022436441853642464\n",
      "Batch loss: 0.038023289293050766\n",
      "Batch loss: 0.011472603306174278\n",
      "Batch loss: 0.023522719740867615\n",
      "Batch loss: 0.04114513471722603\n",
      "Batch loss: 0.02693837136030197\n",
      "Batch loss: 0.0308693777769804\n",
      "Batch loss: 0.02965797297656536\n",
      "Batch loss: 0.04325122386217117\n",
      "Batch loss: 0.029462845996022224\n",
      "Batch loss: 0.030903592705726624\n",
      "Batch loss: 0.017581820487976074\n",
      "Batch loss: 0.023350493982434273\n",
      "Batch loss: 0.03772829473018646\n",
      "Batch loss: 0.04908626526594162\n",
      "Batch loss: 0.01511978916823864\n",
      "Batch loss: 0.007640754338353872\n",
      "Batch loss: 0.04222218319773674\n",
      "Batch loss: 0.02861586958169937\n",
      "Batch loss: 0.02468850463628769\n",
      "Batch loss: 0.0059331264346838\n",
      "Batch loss: 0.01929483376443386\n",
      "Batch loss: 0.03849635273218155\n",
      "Batch loss: 0.007738291285932064\n",
      "Batch loss: 0.013681363314390182\n",
      "Batch loss: 0.05130293592810631\n",
      "Batch loss: 0.0586901530623436\n",
      "Batch loss: 0.0635359063744545\n",
      "Batch loss: 0.07140219956636429\n",
      "Batch loss: 0.015509901568293571\n",
      "Batch loss: 0.0354071706533432\n",
      "Batch loss: 0.018698444589972496\n",
      "Batch loss: 0.08545713871717453\n",
      "Batch loss: 0.016184747219085693\n",
      "Batch loss: 0.023092394694685936\n",
      "Batch loss: 0.04530733823776245\n",
      "Batch loss: 0.0053504412062466145\n",
      "Batch loss: 0.048767901957035065\n",
      "Batch loss: 0.01454284880310297\n",
      "Batch loss: 0.010301456786692142\n",
      "Batch loss: 0.03788748383522034\n",
      "Batch loss: 0.027806708589196205\n",
      "Batch loss: 0.004747400060296059\n",
      "Batch loss: 0.02430594153702259\n",
      "Batch loss: 0.026599138975143433\n",
      "Batch loss: 0.051202431321144104\n",
      "Batch loss: 0.033466678112745285\n",
      "Batch loss: 0.005045151803642511\n",
      "Batch loss: 0.02504756860435009\n",
      "Batch loss: 0.049058422446250916\n",
      "Batch loss: 0.017049714922904968\n",
      "Batch loss: 0.03053978830575943\n",
      "Batch loss: 0.0482313334941864\n",
      "Batch loss: 0.04282339662313461\n",
      "Batch loss: 0.03225255757570267\n",
      "Batch loss: 0.11521601676940918\n",
      "Batch loss: 0.03132366016507149\n",
      "Batch loss: 0.034116361290216446\n",
      "Batch loss: 0.05766405165195465\n",
      "Batch loss: 0.04448064789175987\n",
      "Batch loss: 0.030667632818222046\n",
      "Batch loss: 0.08715631067752838\n",
      "Batch loss: 0.09448030591011047\n",
      "Batch loss: 0.005925786215811968\n",
      "Batch loss: 0.015442741103470325\n",
      "Batch loss: 0.021767159923911095\n",
      "Batch loss: 0.011126956902444363\n",
      "Batch loss: 0.0317537859082222\n",
      "Batch loss: 0.07465650141239166\n",
      "Batch loss: 0.013734831474721432\n",
      "Batch loss: 0.02390287071466446\n",
      "Batch loss: 0.014650896191596985\n",
      "Batch loss: 0.09290650486946106\n",
      "Batch loss: 0.023726962506771088\n",
      "Batch loss: 0.00846847239881754\n",
      "Batch loss: 0.010408238507807255\n",
      "Batch loss: 0.06489478051662445\n",
      "Batch loss: 0.0273036677390337\n",
      "Batch loss: 0.02099805697798729\n",
      "Batch loss: 0.03658825159072876\n",
      "Batch loss: 0.023307550698518753\n",
      "Batch loss: 0.09703587740659714\n",
      "Batch loss: 0.015497321262955666\n",
      "Batch loss: 0.03349433094263077\n",
      "Batch loss: 0.047030601650476456\n",
      "Batch loss: 0.007382816635072231\n",
      "Batch loss: 0.05123700574040413\n",
      "Batch loss: 0.03941122815012932\n",
      "Batch loss: 0.03568192571401596\n",
      "Batch loss: 0.01786753162741661\n",
      "Batch loss: 0.06883587688207626\n",
      "Batch loss: 0.02170446328818798\n",
      "Batch loss: 0.04810265451669693\n",
      "Batch loss: 0.0064355190843343735\n",
      "Batch loss: 0.03073500655591488\n",
      "Batch loss: 0.06288110464811325\n",
      "Batch loss: 0.05497398599982262\n",
      "Batch loss: 0.023516524583101273\n",
      "Batch loss: 0.02550775744020939\n",
      "Batch loss: 0.01700161211192608\n",
      "Batch loss: 0.023050054907798767\n",
      "Batch loss: 0.03917624428868294\n",
      "Batch loss: 0.022170301526784897\n",
      "Batch loss: 0.010363223031163216\n",
      "Batch loss: 0.05552475526928902\n",
      "Batch loss: 0.010092324577271938\n",
      "Batch loss: 0.04190601035952568\n",
      "Batch loss: 0.06371917575597763\n",
      "Batch loss: 0.03277575969696045\n",
      "Batch loss: 0.03575984761118889\n",
      "Batch loss: 0.04516034573316574\n",
      "Batch loss: 0.03207247331738472\n",
      "Batch loss: 0.030882759019732475\n",
      "Batch loss: 0.019582418724894524\n",
      "Batch loss: 0.05145950987935066\n",
      "Batch loss: 0.03789220005273819\n",
      "Batch loss: 0.04277091845870018\n",
      "Batch loss: 0.012381281703710556\n",
      "Batch loss: 0.005294573958963156\n",
      "Batch loss: 0.00273919221945107\n",
      "Batch loss: 0.006966762710362673\n",
      "Batch loss: 0.01004816498607397\n",
      "Batch loss: 0.027859050780534744\n",
      "Batch loss: 0.01139117032289505\n",
      "Batch loss: 0.006572380196303129\n",
      "Batch loss: 0.016822271049022675\n",
      "Batch loss: 0.06681344658136368\n",
      "Batch loss: 0.020071983337402344\n",
      "Batch loss: 0.027829155325889587\n",
      "Batch loss: 0.02461382932960987\n",
      "Batch loss: 0.04222172498703003\n",
      "Batch loss: 0.026017682626843452\n",
      "Batch loss: 0.03301791474223137\n",
      "Batch loss: 0.03573461249470711\n",
      "Batch loss: 0.020558837801218033\n",
      "Batch loss: 0.05786282569169998\n",
      "Batch loss: 0.028451478108763695\n",
      "Batch loss: 0.02737385407090187\n",
      "Batch loss: 0.051878415048122406\n",
      "Batch loss: 0.09596756845712662\n",
      "Batch loss: 0.07165072113275528\n",
      "Batch loss: 0.050961438566446304\n",
      "Batch loss: 0.024497242644429207\n",
      "Batch loss: 0.1038949191570282\n",
      "Batch loss: 0.07278457283973694\n",
      "Batch loss: 0.010017319582402706\n",
      "Batch loss: 0.04301908239722252\n",
      "Batch loss: 0.023117346689105034\n",
      "Batch loss: 0.025781437754631042\n",
      "Batch loss: 0.03577722981572151\n",
      "Batch loss: 0.05690367892384529\n",
      "Batch loss: 0.022809337824583054\n",
      "Batch loss: 0.05304260924458504\n",
      "Batch loss: 0.011757954955101013\n",
      "Batch loss: 0.0768548920750618\n",
      "Batch loss: 0.03901373967528343\n",
      "Batch loss: 0.015742214396595955\n",
      "Batch loss: 0.10425513237714767\n",
      "Batch loss: 0.03253725916147232\n",
      "Batch loss: 0.03135133162140846\n",
      "Batch loss: 0.0682886391878128\n",
      "Batch loss: 0.04091768339276314\n",
      "Batch loss: 0.04278068616986275\n",
      "Batch loss: 0.03307434171438217\n",
      "Batch loss: 0.014205262996256351\n",
      "Batch loss: 0.06813134998083115\n",
      "Batch loss: 0.020125847309827805\n",
      "Batch loss: 0.011358890682458878\n",
      "Batch loss: 0.026172075420618057\n",
      "Batch loss: 0.01114196702837944\n",
      "Batch loss: 0.015029928646981716\n",
      "Batch loss: 0.06371276080608368\n",
      "Batch loss: 0.06157202273607254\n",
      "Batch loss: 0.04810015857219696\n",
      "Batch loss: 0.014981892891228199\n",
      "Batch loss: 0.031439006328582764\n",
      "Batch loss: 0.046627819538116455\n",
      "Batch loss: 0.05149838328361511\n",
      "Batch loss: 0.015382771380245686\n",
      "Batch loss: 0.05735445022583008\n",
      "Batch loss: 0.11031105369329453\n",
      "Batch loss: 0.02022816613316536\n",
      "Batch loss: 0.10367614775896072\n",
      "Batch loss: 0.015756558626890182\n",
      "Batch loss: 0.040672313421964645\n",
      "Batch loss: 0.044516559690237045\n",
      "Batch loss: 0.14019589126110077\n",
      "Batch loss: 0.05813475698232651\n",
      "Batch loss: 0.04111369699239731\n",
      "Batch loss: 0.025874895974993706\n",
      "Batch loss: 0.011020440608263016\n",
      "Batch loss: 0.05649720877408981\n",
      "Batch loss: 0.03532642498612404\n",
      "Batch loss: 0.0187339149415493\n",
      "Batch loss: 0.020844362676143646\n",
      "Batch loss: 0.011080294847488403\n",
      "Batch loss: 0.02660343423485756\n",
      "Batch loss: 0.03038638085126877\n",
      "Batch loss: 0.05321531370282173\n",
      "Batch loss: 0.013883681036531925\n",
      "Batch loss: 0.01456333976238966\n",
      "Batch loss: 0.046771854162216187\n",
      "Batch loss: 0.05471771955490112\n",
      "Batch loss: 0.04305633157491684\n",
      "Batch loss: 0.013144346885383129\n",
      "Batch loss: 0.021192558109760284\n",
      "Batch loss: 0.02688235603272915\n",
      "Batch loss: 0.019967835396528244\n",
      "Batch loss: 0.04365827515721321\n",
      "Batch loss: 0.0120054567232728\n",
      "Batch loss: 0.022064881399273872\n",
      "Batch loss: 0.07184452563524246\n",
      "Batch loss: 0.03738003596663475\n",
      "Batch loss: 0.01812799647450447\n",
      "Batch loss: 0.04195539280772209\n",
      "Batch loss: 0.025458157062530518\n",
      "Batch loss: 0.009835203178226948\n",
      "Batch loss: 0.016928941011428833\n",
      "Batch loss: 0.019016865640878677\n",
      "Batch loss: 0.04069683700799942\n",
      "Batch loss: 0.06053057312965393\n",
      "Batch loss: 0.022276699542999268\n",
      "Batch loss: 0.02776094526052475\n",
      "Batch loss: 0.01433119922876358\n",
      "Batch loss: 0.034828606992959976\n",
      "Batch loss: 0.007479363586753607\n",
      "Batch loss: 0.08218949288129807\n",
      "Batch loss: 0.0060828072018921375\n",
      "Batch loss: 0.0376582033932209\n",
      "Batch loss: 0.015824997797608376\n",
      "Batch loss: 0.05597063899040222\n",
      "Batch loss: 0.010963371954858303\n",
      "Batch loss: 0.02416696585714817\n",
      "Batch loss: 0.026009345427155495\n",
      "Batch loss: 0.0474141389131546\n",
      "Batch loss: 0.022425152361392975\n",
      "Batch loss: 0.03896021097898483\n",
      "Batch loss: 0.03105362132191658\n",
      "Batch loss: 0.040895015001297\n",
      "Batch loss: 0.004940376617014408\n",
      "Batch loss: 0.052648965269327164\n",
      "Batch loss: 0.02006249874830246\n",
      "Batch loss: 0.006237555295228958\n",
      "Batch loss: 0.03789540007710457\n",
      "Batch loss: 0.016055384650826454\n",
      "Batch loss: 0.040712323039770126\n",
      "Batch loss: 0.02732849307358265\n",
      "Batch loss: 0.01588440127670765\n",
      "Batch loss: 0.02779299020767212\n",
      "Batch loss: 0.020662881433963776\n",
      "Batch loss: 0.018179984763264656\n",
      "Batch loss: 0.04608084633946419\n",
      "Batch loss: 0.016920434311032295\n",
      "Batch loss: 0.009266573935747147\n",
      "Batch loss: 0.024991167709231377\n",
      "Batch loss: 0.007054833695292473\n",
      "Batch loss: 0.03601938113570213\n",
      "Batch loss: 0.04807385057210922\n",
      "Batch loss: 0.0071839578449726105\n",
      "Batch loss: 0.021149616688489914\n",
      "Batch loss: 0.06379052996635437\n",
      "Batch loss: 0.038987524807453156\n",
      "Batch loss: 0.01208601426333189\n",
      "Batch loss: 0.03266676887869835\n",
      "Batch loss: 0.01967756263911724\n",
      "Batch loss: 0.028476258739829063\n",
      "Batch loss: 0.011403189972043037\n",
      "Batch loss: 0.02695339545607567\n",
      "Batch loss: 0.005693991202861071\n",
      "Batch loss: 0.049550920724868774\n",
      "Batch loss: 0.04319972172379494\n",
      "Batch loss: 0.02449292317032814\n",
      "Batch loss: 0.028107991442084312\n",
      "Batch loss: 0.007785922382026911\n",
      "Batch loss: 0.016794659197330475\n",
      "Batch loss: 0.0161221195012331\n",
      "Batch loss: 0.04692132771015167\n",
      "Batch loss: 0.07645583897829056\n",
      "Batch loss: 0.030355175957083702\n",
      "Batch loss: 0.02615135721862316\n",
      "Batch loss: 0.006466950289905071\n",
      "Batch loss: 0.03498024493455887\n",
      "Batch loss: 0.0544356033205986\n",
      "Batch loss: 0.026535101234912872\n",
      "Batch loss: 0.03720478340983391\n",
      "Batch loss: 0.03904620185494423\n",
      "Batch loss: 0.03276422619819641\n",
      "Batch loss: 0.016749203205108643\n",
      "Batch loss: 0.05408950522542\n",
      "Batch loss: 0.017225060611963272\n",
      "Batch loss: 0.011646484024822712\n",
      "Batch loss: 0.023294294252991676\n",
      "Batch loss: 0.020482640713453293\n",
      "Batch loss: 0.011426392942667007\n",
      "Batch loss: 0.009074058383703232\n",
      "Batch loss: 0.027139652520418167\n",
      "Batch loss: 0.032177701592445374\n",
      "Batch loss: 0.046108588576316833\n",
      "Batch loss: 0.011663496494293213\n",
      "Batch loss: 0.007777880877256393\n",
      "Batch loss: 0.011250288225710392\n",
      "Batch loss: 0.025983253493905067\n",
      "Batch loss: 0.018111318349838257\n",
      "Batch loss: 0.01371029857546091\n",
      "Batch loss: 0.03406285122036934\n",
      "Batch loss: 0.03565291687846184\n",
      "Batch loss: 0.005415385123342276\n",
      "Batch loss: 0.02485053241252899\n",
      "Batch loss: 0.0382654182612896\n",
      "Batch loss: 0.062497179955244064\n",
      "Batch loss: 0.06706318259239197\n",
      "Batch loss: 0.027106091380119324\n",
      "Batch loss: 0.009985843673348427\n",
      "Batch loss: 0.06764941662549973\n",
      "Batch loss: 0.03299776837229729\n",
      "Batch loss: 0.017007499933242798\n",
      "Batch loss: 0.04756573215126991\n",
      "Batch loss: 0.005605604499578476\n",
      "Batch loss: 0.027400296181440353\n",
      "Batch loss: 0.014976784586906433\n",
      "Epoch [12/100], Loss: 0.0341\n",
      "Validation Loss: 0.5508, Accuracy: 89.40%\n",
      "No improvement for 10 epoches. Early stopping.\n",
      "Start training Median_Blur model.\n",
      "Batch loss: 1.2792119979858398\n",
      "Batch loss: 0.7733622193336487\n",
      "Batch loss: 0.7509021162986755\n",
      "Batch loss: 0.5605213046073914\n",
      "Batch loss: 0.6033391356468201\n",
      "Batch loss: 0.6885485053062439\n",
      "Batch loss: 0.8130018711090088\n",
      "Batch loss: 0.7911163568496704\n",
      "Batch loss: 1.017130732536316\n",
      "Batch loss: 0.5283432006835938\n",
      "Batch loss: 0.6914689540863037\n",
      "Batch loss: 0.6058281660079956\n",
      "Batch loss: 0.564338743686676\n",
      "Batch loss: 0.8194076418876648\n",
      "Batch loss: 0.6691105961799622\n",
      "Batch loss: 0.5113935470581055\n",
      "Batch loss: 0.368728905916214\n",
      "Batch loss: 0.431267112493515\n",
      "Batch loss: 0.32490280270576477\n",
      "Batch loss: 0.49859416484832764\n",
      "Batch loss: 0.4537408649921417\n",
      "Batch loss: 0.3370848596096039\n",
      "Batch loss: 0.30692675709724426\n",
      "Batch loss: 0.43608060479164124\n",
      "Batch loss: 0.32779428362846375\n",
      "Batch loss: 0.282255083322525\n",
      "Batch loss: 0.49699434638023376\n",
      "Batch loss: 0.4834297299385071\n",
      "Batch loss: 0.3016919195652008\n",
      "Batch loss: 0.3213796615600586\n",
      "Batch loss: 0.37648338079452515\n",
      "Batch loss: 0.27353525161743164\n",
      "Batch loss: 0.36306700110435486\n",
      "Batch loss: 0.21772417426109314\n",
      "Batch loss: 0.2729997932910919\n",
      "Batch loss: 0.3447124660015106\n",
      "Batch loss: 0.3600535988807678\n",
      "Batch loss: 0.24506747722625732\n",
      "Batch loss: 0.3542649745941162\n",
      "Batch loss: 0.2598423957824707\n",
      "Batch loss: 0.33476966619491577\n",
      "Batch loss: 0.2710852026939392\n",
      "Batch loss: 0.3665076494216919\n",
      "Batch loss: 0.24618986248970032\n",
      "Batch loss: 0.4263346493244171\n",
      "Batch loss: 0.2986501455307007\n",
      "Batch loss: 0.341622918844223\n",
      "Batch loss: 0.23957525193691254\n",
      "Batch loss: 0.18739376962184906\n",
      "Batch loss: 0.24576528370380402\n",
      "Batch loss: 0.2889772653579712\n",
      "Batch loss: 0.4477168917655945\n",
      "Batch loss: 0.3044155538082123\n",
      "Batch loss: 0.26146200299263\n",
      "Batch loss: 0.34832531213760376\n",
      "Batch loss: 0.38635483384132385\n",
      "Batch loss: 0.28659358620643616\n",
      "Batch loss: 0.29395049810409546\n",
      "Batch loss: 0.28050848841667175\n",
      "Batch loss: 0.2787124514579773\n",
      "Batch loss: 0.307269811630249\n",
      "Batch loss: 0.22248367965221405\n",
      "Batch loss: 0.24428918957710266\n",
      "Batch loss: 0.34914320707321167\n",
      "Batch loss: 0.32486453652381897\n",
      "Batch loss: 0.39921945333480835\n",
      "Batch loss: 0.31364285945892334\n",
      "Batch loss: 0.3298947215080261\n",
      "Batch loss: 0.3275430202484131\n",
      "Batch loss: 0.30765846371650696\n",
      "Batch loss: 0.24995265901088715\n",
      "Batch loss: 0.3752913475036621\n",
      "Batch loss: 0.2274528592824936\n",
      "Batch loss: 0.2163415402173996\n",
      "Batch loss: 0.23705853521823883\n",
      "Batch loss: 0.316354364156723\n",
      "Batch loss: 0.28264138102531433\n",
      "Batch loss: 0.26991569995880127\n",
      "Batch loss: 0.18230560421943665\n",
      "Batch loss: 0.2702120840549469\n",
      "Batch loss: 0.2349468320608139\n",
      "Batch loss: 0.2851179838180542\n",
      "Batch loss: 0.3079032897949219\n",
      "Batch loss: 0.38939982652664185\n",
      "Batch loss: 0.2678035795688629\n",
      "Batch loss: 0.2293006181716919\n",
      "Batch loss: 0.17019622027873993\n",
      "Batch loss: 0.42188262939453125\n",
      "Batch loss: 0.40828728675842285\n",
      "Batch loss: 0.2781851887702942\n",
      "Batch loss: 0.3072158992290497\n",
      "Batch loss: 0.24542103707790375\n",
      "Batch loss: 0.3288355767726898\n",
      "Batch loss: 0.23813708126544952\n",
      "Batch loss: 0.3718172609806061\n",
      "Batch loss: 0.19574014842510223\n",
      "Batch loss: 0.21946406364440918\n",
      "Batch loss: 0.32779455184936523\n",
      "Batch loss: 0.250201940536499\n",
      "Batch loss: 0.3103480339050293\n",
      "Batch loss: 0.2397269606590271\n",
      "Batch loss: 0.22080324590206146\n",
      "Batch loss: 0.2843615710735321\n",
      "Batch loss: 0.23225656151771545\n",
      "Batch loss: 0.2688283622264862\n",
      "Batch loss: 0.2802194356918335\n",
      "Batch loss: 0.3333577811717987\n",
      "Batch loss: 0.27439460158348083\n",
      "Batch loss: 0.19876772165298462\n",
      "Batch loss: 0.36047905683517456\n",
      "Batch loss: 0.2840818464756012\n",
      "Batch loss: 0.19320647418498993\n",
      "Batch loss: 0.2093508392572403\n",
      "Batch loss: 0.2438586801290512\n",
      "Batch loss: 0.232832670211792\n",
      "Batch loss: 0.28276121616363525\n",
      "Batch loss: 0.2798338234424591\n",
      "Batch loss: 0.18340779840946198\n",
      "Batch loss: 0.22550050914287567\n",
      "Batch loss: 0.19635941088199615\n",
      "Batch loss: 0.23555675148963928\n",
      "Batch loss: 0.22728978097438812\n",
      "Batch loss: 0.24830125272274017\n",
      "Batch loss: 0.13410238921642303\n",
      "Batch loss: 0.29696500301361084\n",
      "Batch loss: 0.32476258277893066\n",
      "Batch loss: 0.26571762561798096\n",
      "Batch loss: 0.2662220001220703\n",
      "Batch loss: 0.2149471640586853\n",
      "Batch loss: 0.29682067036628723\n",
      "Batch loss: 0.22200538218021393\n",
      "Batch loss: 0.394635945558548\n",
      "Batch loss: 0.2372312992811203\n",
      "Batch loss: 0.23620010912418365\n",
      "Batch loss: 0.3394524157047272\n",
      "Batch loss: 0.2004842758178711\n",
      "Batch loss: 0.3095989525318146\n",
      "Batch loss: 0.23942047357559204\n",
      "Batch loss: 0.3321923017501831\n",
      "Batch loss: 0.16003553569316864\n",
      "Batch loss: 0.17747367918491364\n",
      "Batch loss: 0.17509162425994873\n",
      "Batch loss: 0.2287323772907257\n",
      "Batch loss: 0.2925651967525482\n",
      "Batch loss: 0.2323620766401291\n",
      "Batch loss: 0.2972138524055481\n",
      "Batch loss: 0.2459256649017334\n",
      "Batch loss: 0.18766163289546967\n",
      "Batch loss: 0.21809056401252747\n",
      "Batch loss: 0.15842118859291077\n",
      "Batch loss: 0.193185955286026\n",
      "Batch loss: 0.28577476739883423\n",
      "Batch loss: 0.2780502438545227\n",
      "Batch loss: 0.37307009100914\n",
      "Batch loss: 0.36542895436286926\n",
      "Batch loss: 0.32159751653671265\n",
      "Batch loss: 0.23544356226921082\n",
      "Batch loss: 0.18685242533683777\n",
      "Batch loss: 0.2511749863624573\n",
      "Batch loss: 0.24341031908988953\n",
      "Batch loss: 0.15532474219799042\n",
      "Batch loss: 0.2557809054851532\n",
      "Batch loss: 0.19516581296920776\n",
      "Batch loss: 0.2571808695793152\n",
      "Batch loss: 0.3019675612449646\n",
      "Batch loss: 0.281206876039505\n",
      "Batch loss: 0.20083391666412354\n",
      "Batch loss: 0.23208779096603394\n",
      "Batch loss: 0.18773379921913147\n",
      "Batch loss: 0.18994784355163574\n",
      "Batch loss: 0.2607477307319641\n",
      "Batch loss: 0.2938977777957916\n",
      "Batch loss: 0.15940743684768677\n",
      "Batch loss: 0.18062399327754974\n",
      "Batch loss: 0.15612414479255676\n",
      "Batch loss: 0.29465001821517944\n",
      "Batch loss: 0.2906557321548462\n",
      "Batch loss: 0.16919410228729248\n",
      "Batch loss: 0.20266197621822357\n",
      "Batch loss: 0.18183864653110504\n",
      "Batch loss: 0.22839809954166412\n",
      "Batch loss: 0.16425520181655884\n",
      "Batch loss: 0.11306263506412506\n",
      "Batch loss: 0.2963745892047882\n",
      "Batch loss: 0.2695601284503937\n",
      "Batch loss: 0.16889126598834991\n",
      "Batch loss: 0.2445002943277359\n",
      "Batch loss: 0.22533896565437317\n",
      "Batch loss: 0.2109806388616562\n",
      "Batch loss: 0.17212970554828644\n",
      "Batch loss: 0.21136118471622467\n",
      "Batch loss: 0.26157069206237793\n",
      "Batch loss: 0.24339964985847473\n",
      "Batch loss: 0.24921521544456482\n",
      "Batch loss: 0.15392577648162842\n",
      "Batch loss: 0.1771903932094574\n",
      "Batch loss: 0.15461233258247375\n",
      "Batch loss: 0.3023759424686432\n",
      "Batch loss: 0.2173846811056137\n",
      "Batch loss: 0.21090185642242432\n",
      "Batch loss: 0.24074892699718475\n",
      "Batch loss: 0.2849106192588806\n",
      "Batch loss: 0.16495440900325775\n",
      "Batch loss: 0.15967333316802979\n",
      "Batch loss: 0.22847655415534973\n",
      "Batch loss: 0.333356648683548\n",
      "Batch loss: 0.25716784596443176\n",
      "Batch loss: 0.2581334412097931\n",
      "Batch loss: 0.13571123778820038\n",
      "Batch loss: 0.16314025223255157\n",
      "Batch loss: 0.16505691409111023\n",
      "Batch loss: 0.2186053991317749\n",
      "Batch loss: 0.183732807636261\n",
      "Batch loss: 0.36887139081954956\n",
      "Batch loss: 0.2447231560945511\n",
      "Batch loss: 0.15451951324939728\n",
      "Batch loss: 0.21140620112419128\n",
      "Batch loss: 0.23073749244213104\n",
      "Batch loss: 0.20054610073566437\n",
      "Batch loss: 0.26061713695526123\n",
      "Batch loss: 0.1959955096244812\n",
      "Batch loss: 0.16103889048099518\n",
      "Batch loss: 0.15746140480041504\n",
      "Batch loss: 0.39753156900405884\n",
      "Batch loss: 0.19025017321109772\n",
      "Batch loss: 0.3373934328556061\n",
      "Batch loss: 0.17741410434246063\n",
      "Batch loss: 0.27694961428642273\n",
      "Batch loss: 0.27738654613494873\n",
      "Batch loss: 0.1844501942396164\n",
      "Batch loss: 0.3361523449420929\n",
      "Batch loss: 0.2581375539302826\n",
      "Batch loss: 0.19124098122119904\n",
      "Batch loss: 0.16759702563285828\n",
      "Batch loss: 0.1899920105934143\n",
      "Batch loss: 0.28651097416877747\n",
      "Batch loss: 0.2014336735010147\n",
      "Batch loss: 0.2435343712568283\n",
      "Batch loss: 0.1309927999973297\n",
      "Batch loss: 0.2326827049255371\n",
      "Batch loss: 0.30007150769233704\n",
      "Batch loss: 0.3030681610107422\n",
      "Batch loss: 0.20416761934757233\n",
      "Batch loss: 0.21172493696212769\n",
      "Batch loss: 0.21532446146011353\n",
      "Batch loss: 0.198782280087471\n",
      "Batch loss: 0.14015308022499084\n",
      "Batch loss: 0.2339915633201599\n",
      "Batch loss: 0.18608899414539337\n",
      "Batch loss: 0.18018265068531036\n",
      "Batch loss: 0.24395465850830078\n",
      "Batch loss: 0.24735210835933685\n",
      "Batch loss: 0.3207482397556305\n",
      "Batch loss: 0.34322598576545715\n",
      "Batch loss: 0.22262334823608398\n",
      "Batch loss: 0.4522726237773895\n",
      "Batch loss: 0.3201858103275299\n",
      "Batch loss: 0.23914331197738647\n",
      "Batch loss: 0.264458030462265\n",
      "Batch loss: 0.23532959818840027\n",
      "Batch loss: 0.21267947554588318\n",
      "Batch loss: 0.2300935983657837\n",
      "Batch loss: 0.3392523229122162\n",
      "Batch loss: 0.2223120778799057\n",
      "Batch loss: 0.2917712926864624\n",
      "Batch loss: 0.14180651307106018\n",
      "Batch loss: 0.2505345344543457\n",
      "Batch loss: 0.20303425192832947\n",
      "Batch loss: 0.2100435197353363\n",
      "Batch loss: 0.25691553950309753\n",
      "Batch loss: 0.28182002902030945\n",
      "Batch loss: 0.23458313941955566\n",
      "Batch loss: 0.1701454520225525\n",
      "Batch loss: 0.2020498514175415\n",
      "Batch loss: 0.24587805569171906\n",
      "Batch loss: 0.16859480738639832\n",
      "Batch loss: 0.1832285374403\n",
      "Batch loss: 0.3122919201850891\n",
      "Batch loss: 0.1882033795118332\n",
      "Batch loss: 0.2748146653175354\n",
      "Batch loss: 0.21391043066978455\n",
      "Batch loss: 0.12041403353214264\n",
      "Batch loss: 0.30991846323013306\n",
      "Batch loss: 0.43254008889198303\n",
      "Batch loss: 0.25783297419548035\n",
      "Batch loss: 0.2894115746021271\n",
      "Batch loss: 0.21737641096115112\n",
      "Batch loss: 0.15945565700531006\n",
      "Batch loss: 0.18802082538604736\n",
      "Batch loss: 0.13614298403263092\n",
      "Batch loss: 0.18036174774169922\n",
      "Batch loss: 0.2013043612241745\n",
      "Batch loss: 0.1760890930891037\n",
      "Batch loss: 0.20200835168361664\n",
      "Batch loss: 0.19972917437553406\n",
      "Batch loss: 0.2788349986076355\n",
      "Batch loss: 0.23201805353164673\n",
      "Batch loss: 0.17371217906475067\n",
      "Batch loss: 0.2490793615579605\n",
      "Batch loss: 0.1816578507423401\n",
      "Batch loss: 0.2986956536769867\n",
      "Batch loss: 0.18545731902122498\n",
      "Batch loss: 0.19898660480976105\n",
      "Batch loss: 0.16383421421051025\n",
      "Batch loss: 0.290580153465271\n",
      "Batch loss: 0.22729673981666565\n",
      "Batch loss: 0.11966194212436676\n",
      "Batch loss: 0.20136207342147827\n",
      "Batch loss: 0.18383078277111053\n",
      "Batch loss: 0.30506402254104614\n",
      "Batch loss: 0.22643877565860748\n",
      "Batch loss: 0.19184258580207825\n",
      "Batch loss: 0.18854168057441711\n",
      "Batch loss: 0.25777432322502136\n",
      "Batch loss: 0.2700046896934509\n",
      "Batch loss: 0.3418830931186676\n",
      "Batch loss: 0.19534125924110413\n",
      "Batch loss: 0.20983931422233582\n",
      "Batch loss: 0.21401000022888184\n",
      "Batch loss: 0.1921101212501526\n",
      "Batch loss: 0.2435063123703003\n",
      "Batch loss: 0.17041054368019104\n",
      "Batch loss: 0.20488512516021729\n",
      "Batch loss: 0.2528315484523773\n",
      "Batch loss: 0.20268993079662323\n",
      "Batch loss: 0.22316382825374603\n",
      "Batch loss: 0.25418180227279663\n",
      "Batch loss: 0.2713843882083893\n",
      "Batch loss: 0.183777317404747\n",
      "Batch loss: 0.3261043131351471\n",
      "Batch loss: 0.13567663729190826\n",
      "Batch loss: 0.2701672315597534\n",
      "Batch loss: 0.10373999923467636\n",
      "Batch loss: 0.15433821082115173\n",
      "Batch loss: 0.21506763994693756\n",
      "Batch loss: 0.15168648958206177\n",
      "Batch loss: 0.14141541719436646\n",
      "Batch loss: 0.17316165566444397\n",
      "Batch loss: 0.203895702958107\n",
      "Batch loss: 0.16170278191566467\n",
      "Batch loss: 0.20047639310359955\n",
      "Batch loss: 0.129817932844162\n",
      "Batch loss: 0.15244154632091522\n",
      "Batch loss: 0.2775479257106781\n",
      "Batch loss: 0.12111273407936096\n",
      "Batch loss: 0.2532918453216553\n",
      "Batch loss: 0.26456189155578613\n",
      "Batch loss: 0.3669801652431488\n",
      "Batch loss: 0.16589215397834778\n",
      "Batch loss: 0.15452001988887787\n",
      "Batch loss: 0.16098114848136902\n",
      "Batch loss: 0.20898392796516418\n",
      "Batch loss: 0.09191280603408813\n",
      "Batch loss: 0.17676986753940582\n",
      "Batch loss: 0.20033137500286102\n",
      "Batch loss: 0.1676356941461563\n",
      "Batch loss: 0.1307060718536377\n",
      "Batch loss: 0.2866304814815521\n",
      "Batch loss: 0.276091068983078\n",
      "Batch loss: 0.3087213933467865\n",
      "Batch loss: 0.17354579269886017\n",
      "Batch loss: 0.19752953946590424\n",
      "Batch loss: 0.23365576565265656\n",
      "Batch loss: 0.3178514540195465\n",
      "Batch loss: 0.21156355738639832\n",
      "Batch loss: 0.1985446810722351\n",
      "Batch loss: 0.2471812218427658\n",
      "Batch loss: 0.19916710257530212\n",
      "Batch loss: 0.16454210877418518\n",
      "Batch loss: 0.3160395622253418\n",
      "Batch loss: 0.19861657917499542\n",
      "Batch loss: 0.21900060772895813\n",
      "Batch loss: 0.24296171963214874\n",
      "Batch loss: 0.19697025418281555\n",
      "Batch loss: 0.23478911817073822\n",
      "Batch loss: 0.164102241396904\n",
      "Batch loss: 0.21868866682052612\n",
      "Batch loss: 0.15346795320510864\n",
      "Batch loss: 0.2404601275920868\n",
      "Batch loss: 0.275575190782547\n",
      "Batch loss: 0.46984875202178955\n",
      "Batch loss: 0.1827440857887268\n",
      "Batch loss: 0.15041905641555786\n",
      "Batch loss: 0.26966169476509094\n",
      "Batch loss: 0.1298597753047943\n",
      "Batch loss: 0.23492416739463806\n",
      "Batch loss: 0.22812341153621674\n",
      "Batch loss: 0.15511265397071838\n",
      "Batch loss: 0.19056962430477142\n",
      "Batch loss: 0.17550696432590485\n",
      "Batch loss: 0.19945356249809265\n",
      "Batch loss: 0.2540617883205414\n",
      "Batch loss: 0.1382593959569931\n",
      "Batch loss: 0.15781639516353607\n",
      "Batch loss: 0.2864015996456146\n",
      "Batch loss: 0.23190531134605408\n",
      "Batch loss: 0.0680348351597786\n",
      "Batch loss: 0.20645073056221008\n",
      "Batch loss: 0.1739591360092163\n",
      "Batch loss: 0.1971672922372818\n",
      "Batch loss: 0.19382312893867493\n",
      "Batch loss: 0.13756074011325836\n",
      "Batch loss: 0.2958124279975891\n",
      "Batch loss: 0.17512327432632446\n",
      "Batch loss: 0.10078264772891998\n",
      "Batch loss: 0.3307885527610779\n",
      "Batch loss: 0.2710540294647217\n",
      "Batch loss: 0.1776801496744156\n",
      "Batch loss: 0.16388708353042603\n",
      "Batch loss: 0.2690920829772949\n",
      "Batch loss: 0.18742291629314423\n",
      "Batch loss: 0.22959579527378082\n",
      "Batch loss: 0.11033274233341217\n",
      "Batch loss: 0.17210182547569275\n",
      "Batch loss: 0.2482093721628189\n",
      "Batch loss: 0.17347975075244904\n",
      "Batch loss: 0.11014015972614288\n",
      "Epoch [1/100], Loss: 0.2620\n",
      "Validation Loss: 0.3370, Accuracy: 90.57%\n",
      "Batch loss: 0.2960405647754669\n",
      "Batch loss: 0.2593270540237427\n",
      "Batch loss: 0.22412614524364471\n",
      "Batch loss: 0.27181151509284973\n",
      "Batch loss: 0.2459557205438614\n",
      "Batch loss: 0.24781538546085358\n",
      "Batch loss: 0.5170997381210327\n",
      "Batch loss: 0.25294750928878784\n",
      "Batch loss: 0.25844934582710266\n",
      "Batch loss: 0.18799205124378204\n",
      "Batch loss: 0.29535287618637085\n",
      "Batch loss: 0.27104833722114563\n",
      "Batch loss: 0.19850338995456696\n",
      "Batch loss: 0.2771925628185272\n",
      "Batch loss: 0.27017903327941895\n",
      "Batch loss: 0.2007633000612259\n",
      "Batch loss: 0.17851847410202026\n",
      "Batch loss: 0.26467326283454895\n",
      "Batch loss: 0.130918949842453\n",
      "Batch loss: 0.19687166810035706\n",
      "Batch loss: 0.15914484858512878\n",
      "Batch loss: 0.200599804520607\n",
      "Batch loss: 0.17863595485687256\n",
      "Batch loss: 0.1671486496925354\n",
      "Batch loss: 0.12418938428163528\n",
      "Batch loss: 0.14343275129795074\n",
      "Batch loss: 0.20849649608135223\n",
      "Batch loss: 0.2800975441932678\n",
      "Batch loss: 0.12125952541828156\n",
      "Batch loss: 0.1497618705034256\n",
      "Batch loss: 0.2764442563056946\n",
      "Batch loss: 0.1671762317419052\n",
      "Batch loss: 0.2062305361032486\n",
      "Batch loss: 0.15884770452976227\n",
      "Batch loss: 0.119601771235466\n",
      "Batch loss: 0.26575180888175964\n",
      "Batch loss: 0.22486726939678192\n",
      "Batch loss: 0.15418891608715057\n",
      "Batch loss: 0.2544926106929779\n",
      "Batch loss: 0.1229834109544754\n",
      "Batch loss: 0.1672976016998291\n",
      "Batch loss: 0.17881856858730316\n",
      "Batch loss: 0.21221768856048584\n",
      "Batch loss: 0.14474447071552277\n",
      "Batch loss: 0.21376048028469086\n",
      "Batch loss: 0.16116215288639069\n",
      "Batch loss: 0.19707895815372467\n",
      "Batch loss: 0.16031932830810547\n",
      "Batch loss: 0.09920594096183777\n",
      "Batch loss: 0.22295618057250977\n",
      "Batch loss: 0.20898516476154327\n",
      "Batch loss: 0.22002935409545898\n",
      "Batch loss: 0.14535842835903168\n",
      "Batch loss: 0.17162325978279114\n",
      "Batch loss: 0.19756869971752167\n",
      "Batch loss: 0.22102941572666168\n",
      "Batch loss: 0.2070736587047577\n",
      "Batch loss: 0.19165101647377014\n",
      "Batch loss: 0.16256606578826904\n",
      "Batch loss: 0.16443678736686707\n",
      "Batch loss: 0.18266524374485016\n",
      "Batch loss: 0.15565285086631775\n",
      "Batch loss: 0.1941208392381668\n",
      "Batch loss: 0.24547165632247925\n",
      "Batch loss: 0.13691408932209015\n",
      "Batch loss: 0.20296987891197205\n",
      "Batch loss: 0.17816409468650818\n",
      "Batch loss: 0.25431859493255615\n",
      "Batch loss: 0.17622795701026917\n",
      "Batch loss: 0.18492662906646729\n",
      "Batch loss: 0.189229816198349\n",
      "Batch loss: 0.21854151785373688\n",
      "Batch loss: 0.14261287450790405\n",
      "Batch loss: 0.16872167587280273\n",
      "Batch loss: 0.1414976865053177\n",
      "Batch loss: 0.20205692946910858\n",
      "Batch loss: 0.2124384343624115\n",
      "Batch loss: 0.17862652242183685\n",
      "Batch loss: 0.14552368223667145\n",
      "Batch loss: 0.19430655241012573\n",
      "Batch loss: 0.11696580052375793\n",
      "Batch loss: 0.10833358764648438\n",
      "Batch loss: 0.19561012089252472\n",
      "Batch loss: 0.33867308497428894\n",
      "Batch loss: 0.18972933292388916\n",
      "Batch loss: 0.12426326423883438\n",
      "Batch loss: 0.08091998845338821\n",
      "Batch loss: 0.18856847286224365\n",
      "Batch loss: 0.18728899955749512\n",
      "Batch loss: 0.16670820116996765\n",
      "Batch loss: 0.2881620526313782\n",
      "Batch loss: 0.11178575456142426\n",
      "Batch loss: 0.29306158423423767\n",
      "Batch loss: 0.1720142513513565\n",
      "Batch loss: 0.18691836297512054\n",
      "Batch loss: 0.12466183304786682\n",
      "Batch loss: 0.1234411969780922\n",
      "Batch loss: 0.2204485684633255\n",
      "Batch loss: 0.18631161749362946\n",
      "Batch loss: 0.20960231125354767\n",
      "Batch loss: 0.12883013486862183\n",
      "Batch loss: 0.11276078224182129\n",
      "Batch loss: 0.18490667641162872\n",
      "Batch loss: 0.1839556246995926\n",
      "Batch loss: 0.18013134598731995\n",
      "Batch loss: 0.1530642956495285\n",
      "Batch loss: 0.16997140645980835\n",
      "Batch loss: 0.1923002153635025\n",
      "Batch loss: 0.163338303565979\n",
      "Batch loss: 0.27275487780570984\n",
      "Batch loss: 0.20118868350982666\n",
      "Batch loss: 0.14442510902881622\n",
      "Batch loss: 0.16245833039283752\n",
      "Batch loss: 0.13398414850234985\n",
      "Batch loss: 0.16012845933437347\n",
      "Batch loss: 0.1872706562280655\n",
      "Batch loss: 0.15407590568065643\n",
      "Batch loss: 0.09506045281887054\n",
      "Batch loss: 0.1125493049621582\n",
      "Batch loss: 0.16170160472393036\n",
      "Batch loss: 0.1735660433769226\n",
      "Batch loss: 0.1766805350780487\n",
      "Batch loss: 0.23051978647708893\n",
      "Batch loss: 0.12542147934436798\n",
      "Batch loss: 0.23182640969753265\n",
      "Batch loss: 0.2799558639526367\n",
      "Batch loss: 0.18043507635593414\n",
      "Batch loss: 0.16490674018859863\n",
      "Batch loss: 0.166292205452919\n",
      "Batch loss: 0.1805037260055542\n",
      "Batch loss: 0.16980575025081635\n",
      "Batch loss: 0.24919480085372925\n",
      "Batch loss: 0.1902681291103363\n",
      "Batch loss: 0.1899193525314331\n",
      "Batch loss: 0.2629118263721466\n",
      "Batch loss: 0.08942655473947525\n",
      "Batch loss: 0.17915651202201843\n",
      "Batch loss: 0.16016121208667755\n",
      "Batch loss: 0.20884481072425842\n",
      "Batch loss: 0.08139708638191223\n",
      "Batch loss: 0.14874109625816345\n",
      "Batch loss: 0.14154748618602753\n",
      "Batch loss: 0.21873757243156433\n",
      "Batch loss: 0.23298615217208862\n",
      "Batch loss: 0.17022940516471863\n",
      "Batch loss: 0.18035045266151428\n",
      "Batch loss: 0.14859645068645477\n",
      "Batch loss: 0.15364627540111542\n",
      "Batch loss: 0.1639939248561859\n",
      "Batch loss: 0.12121520936489105\n",
      "Batch loss: 0.1442558914422989\n",
      "Batch loss: 0.1916927993297577\n",
      "Batch loss: 0.21952785551548004\n",
      "Batch loss: 0.3073883354663849\n",
      "Batch loss: 0.20733365416526794\n",
      "Batch loss: 0.21127963066101074\n",
      "Batch loss: 0.1954507678747177\n",
      "Batch loss: 0.12656255066394806\n",
      "Batch loss: 0.14281819760799408\n",
      "Batch loss: 0.1742287576198578\n",
      "Batch loss: 0.12677395343780518\n",
      "Batch loss: 0.18553006649017334\n",
      "Batch loss: 0.17934687435626984\n",
      "Batch loss: 0.20033304393291473\n",
      "Batch loss: 0.21540188789367676\n",
      "Batch loss: 0.19738835096359253\n",
      "Batch loss: 0.14135728776454926\n",
      "Batch loss: 0.13018344342708588\n",
      "Batch loss: 0.18464942276477814\n",
      "Batch loss: 0.14706595242023468\n",
      "Batch loss: 0.20223718881607056\n",
      "Batch loss: 0.24652697145938873\n",
      "Batch loss: 0.09249512851238251\n",
      "Batch loss: 0.1567947119474411\n",
      "Batch loss: 0.11989456415176392\n",
      "Batch loss: 0.2412894070148468\n",
      "Batch loss: 0.25029903650283813\n",
      "Batch loss: 0.10094985365867615\n",
      "Batch loss: 0.19854512810707092\n",
      "Batch loss: 0.1561415195465088\n",
      "Batch loss: 0.23131825029850006\n",
      "Batch loss: 0.10598202794790268\n",
      "Batch loss: 0.0866936668753624\n",
      "Batch loss: 0.20982782542705536\n",
      "Batch loss: 0.2088160514831543\n",
      "Batch loss: 0.1433776468038559\n",
      "Batch loss: 0.18673129379749298\n",
      "Batch loss: 0.1775013953447342\n",
      "Batch loss: 0.17619359493255615\n",
      "Batch loss: 0.1336480677127838\n",
      "Batch loss: 0.14276228845119476\n",
      "Batch loss: 0.13282708823680878\n",
      "Batch loss: 0.2127380073070526\n",
      "Batch loss: 0.14769518375396729\n",
      "Batch loss: 0.12091648578643799\n",
      "Batch loss: 0.1502925455570221\n",
      "Batch loss: 0.0982455164194107\n",
      "Batch loss: 0.23697002232074738\n",
      "Batch loss: 0.14951609075069427\n",
      "Batch loss: 0.1519702672958374\n",
      "Batch loss: 0.2259453982114792\n",
      "Batch loss: 0.22929342091083527\n",
      "Batch loss: 0.13948531448841095\n",
      "Batch loss: 0.10061363130807877\n",
      "Batch loss: 0.22537565231323242\n",
      "Batch loss: 0.24474143981933594\n",
      "Batch loss: 0.27955347299575806\n",
      "Batch loss: 0.22904086112976074\n",
      "Batch loss: 0.1082758679986\n",
      "Batch loss: 0.1552073359489441\n",
      "Batch loss: 0.10805369913578033\n",
      "Batch loss: 0.17291370034217834\n",
      "Batch loss: 0.17104268074035645\n",
      "Batch loss: 0.238966166973114\n",
      "Batch loss: 0.19774387776851654\n",
      "Batch loss: 0.16771268844604492\n",
      "Batch loss: 0.18859174847602844\n",
      "Batch loss: 0.1928459256887436\n",
      "Batch loss: 0.11267215013504028\n",
      "Batch loss: 0.22294650971889496\n",
      "Batch loss: 0.16738133132457733\n",
      "Batch loss: 0.16701596975326538\n",
      "Batch loss: 0.1284700483083725\n",
      "Batch loss: 0.29428428411483765\n",
      "Batch loss: 0.11170605570077896\n",
      "Batch loss: 0.25957971811294556\n",
      "Batch loss: 0.11541765183210373\n",
      "Batch loss: 0.19934925436973572\n",
      "Batch loss: 0.30310574173927307\n",
      "Batch loss: 0.10940120369195938\n",
      "Batch loss: 0.2221735566854477\n",
      "Batch loss: 0.14858120679855347\n",
      "Batch loss: 0.13016602396965027\n",
      "Batch loss: 0.1531047523021698\n",
      "Batch loss: 0.13196727633476257\n",
      "Batch loss: 0.24152757227420807\n",
      "Batch loss: 0.1275147646665573\n",
      "Batch loss: 0.1972377449274063\n",
      "Batch loss: 0.11190402507781982\n",
      "Batch loss: 0.16066263616085052\n",
      "Batch loss: 0.21477438509464264\n",
      "Batch loss: 0.20724323391914368\n",
      "Batch loss: 0.12454282492399216\n",
      "Batch loss: 0.1470782309770584\n",
      "Batch loss: 0.11654119193553925\n",
      "Batch loss: 0.18828840553760529\n",
      "Batch loss: 0.19603943824768066\n",
      "Batch loss: 0.19042667746543884\n",
      "Batch loss: 0.16331711411476135\n",
      "Batch loss: 0.19123230874538422\n",
      "Batch loss: 0.19574014842510223\n",
      "Batch loss: 0.21617461740970612\n",
      "Batch loss: 0.331728994846344\n",
      "Batch loss: 0.2542888820171356\n",
      "Batch loss: 0.18968120217323303\n",
      "Batch loss: 0.3459603488445282\n",
      "Batch loss: 0.20043478906154633\n",
      "Batch loss: 0.25158095359802246\n",
      "Batch loss: 0.24042704701423645\n",
      "Batch loss: 0.18347813189029694\n",
      "Batch loss: 0.17729702591896057\n",
      "Batch loss: 0.18898127973079681\n",
      "Batch loss: 0.2517063021659851\n",
      "Batch loss: 0.15369291603565216\n",
      "Batch loss: 0.17984822392463684\n",
      "Batch loss: 0.10285982489585876\n",
      "Batch loss: 0.22718022763729095\n",
      "Batch loss: 0.1545408070087433\n",
      "Batch loss: 0.17352016270160675\n",
      "Batch loss: 0.18354164063930511\n",
      "Batch loss: 0.19908501207828522\n",
      "Batch loss: 0.19427712261676788\n",
      "Batch loss: 0.13065104186534882\n",
      "Batch loss: 0.18253447115421295\n",
      "Batch loss: 0.22932659089565277\n",
      "Batch loss: 0.1288447231054306\n",
      "Batch loss: 0.15509843826293945\n",
      "Batch loss: 0.23980702459812164\n",
      "Batch loss: 0.1993936002254486\n",
      "Batch loss: 0.24115905165672302\n",
      "Batch loss: 0.21107158064842224\n",
      "Batch loss: 0.12356298416852951\n",
      "Batch loss: 0.22891327738761902\n",
      "Batch loss: 0.35343751311302185\n",
      "Batch loss: 0.19925709068775177\n",
      "Batch loss: 0.25171276926994324\n",
      "Batch loss: 0.15287694334983826\n",
      "Batch loss: 0.10689351707696915\n",
      "Batch loss: 0.1621539294719696\n",
      "Batch loss: 0.10569684207439423\n",
      "Batch loss: 0.12061495333909988\n",
      "Batch loss: 0.143922820687294\n",
      "Batch loss: 0.14825697243213654\n",
      "Batch loss: 0.1478443443775177\n",
      "Batch loss: 0.17118068039417267\n",
      "Batch loss: 0.24005746841430664\n",
      "Batch loss: 0.17272329330444336\n",
      "Batch loss: 0.11774678528308868\n",
      "Batch loss: 0.2355317622423172\n",
      "Batch loss: 0.1098780632019043\n",
      "Batch loss: 0.17280110716819763\n",
      "Batch loss: 0.11476357281208038\n",
      "Batch loss: 0.16322623193264008\n",
      "Batch loss: 0.1321742683649063\n",
      "Batch loss: 0.21734501421451569\n",
      "Batch loss: 0.21916669607162476\n",
      "Batch loss: 0.12448155879974365\n",
      "Batch loss: 0.14506962895393372\n",
      "Batch loss: 0.12084755301475525\n",
      "Batch loss: 0.2042679786682129\n",
      "Batch loss: 0.1568991243839264\n",
      "Batch loss: 0.12355289608240128\n",
      "Batch loss: 0.13324016332626343\n",
      "Batch loss: 0.20126570761203766\n",
      "Batch loss: 0.22747789323329926\n",
      "Batch loss: 0.2788979411125183\n",
      "Batch loss: 0.1768292933702469\n",
      "Batch loss: 0.17619375884532928\n",
      "Batch loss: 0.17721396684646606\n",
      "Batch loss: 0.14483629167079926\n",
      "Batch loss: 0.17814983427524567\n",
      "Batch loss: 0.15290828049182892\n",
      "Batch loss: 0.1503898799419403\n",
      "Batch loss: 0.21114778518676758\n",
      "Batch loss: 0.21177451312541962\n",
      "Batch loss: 0.21181634068489075\n",
      "Batch loss: 0.20156803727149963\n",
      "Batch loss: 0.2722260653972626\n",
      "Batch loss: 0.16511760652065277\n",
      "Batch loss: 0.21090048551559448\n",
      "Batch loss: 0.13758666813373566\n",
      "Batch loss: 0.2170991450548172\n",
      "Batch loss: 0.10475470125675201\n",
      "Batch loss: 0.1451912671327591\n",
      "Batch loss: 0.19674243032932281\n",
      "Batch loss: 0.1571483314037323\n",
      "Batch loss: 0.07282919436693192\n",
      "Batch loss: 0.12398049235343933\n",
      "Batch loss: 0.15093128383159637\n",
      "Batch loss: 0.11400540918111801\n",
      "Batch loss: 0.17726464569568634\n",
      "Batch loss: 0.09930074959993362\n",
      "Batch loss: 0.14637787640094757\n",
      "Batch loss: 0.20409482717514038\n",
      "Batch loss: 0.14528091251850128\n",
      "Batch loss: 0.19602683186531067\n",
      "Batch loss: 0.24739554524421692\n",
      "Batch loss: 0.27163007855415344\n",
      "Batch loss: 0.14184175431728363\n",
      "Batch loss: 0.10721206665039062\n",
      "Batch loss: 0.1650816798210144\n",
      "Batch loss: 0.1754765808582306\n",
      "Batch loss: 0.07406502217054367\n",
      "Batch loss: 0.11327118426561356\n",
      "Batch loss: 0.14305022358894348\n",
      "Batch loss: 0.15734359622001648\n",
      "Batch loss: 0.0985880121588707\n",
      "Batch loss: 0.2434067279100418\n",
      "Batch loss: 0.20665021240711212\n",
      "Batch loss: 0.21552032232284546\n",
      "Batch loss: 0.16114477813243866\n",
      "Batch loss: 0.2041657567024231\n",
      "Batch loss: 0.16083228588104248\n",
      "Batch loss: 0.21911707520484924\n",
      "Batch loss: 0.24196705222129822\n",
      "Batch loss: 0.1325022280216217\n",
      "Batch loss: 0.19430065155029297\n",
      "Batch loss: 0.15208858251571655\n",
      "Batch loss: 0.15503241121768951\n",
      "Batch loss: 0.2837870121002197\n",
      "Batch loss: 0.15096572041511536\n",
      "Batch loss: 0.1819455772638321\n",
      "Batch loss: 0.17312386631965637\n",
      "Batch loss: 0.14065369963645935\n",
      "Batch loss: 0.20002014935016632\n",
      "Batch loss: 0.14977562427520752\n",
      "Batch loss: 0.18477894365787506\n",
      "Batch loss: 0.10910555720329285\n",
      "Batch loss: 0.21946601569652557\n",
      "Batch loss: 0.25297707319259644\n",
      "Batch loss: 0.32717710733413696\n",
      "Batch loss: 0.1970149725675583\n",
      "Batch loss: 0.11974766850471497\n",
      "Batch loss: 0.1922362744808197\n",
      "Batch loss: 0.07181872427463531\n",
      "Batch loss: 0.18068204820156097\n",
      "Batch loss: 0.1995978057384491\n",
      "Batch loss: 0.1473025530576706\n",
      "Batch loss: 0.12934432923793793\n",
      "Batch loss: 0.15757617354393005\n",
      "Batch loss: 0.13880182802677155\n",
      "Batch loss: 0.18834751844406128\n",
      "Batch loss: 0.115494005382061\n",
      "Batch loss: 0.1319347321987152\n",
      "Batch loss: 0.3167215883731842\n",
      "Batch loss: 0.20973241329193115\n",
      "Batch loss: 0.05702197924256325\n",
      "Batch loss: 0.15099354088306427\n",
      "Batch loss: 0.09790689498186111\n",
      "Batch loss: 0.15506628155708313\n",
      "Batch loss: 0.14852482080459595\n",
      "Batch loss: 0.11574067175388336\n",
      "Batch loss: 0.32079318165779114\n",
      "Batch loss: 0.12243825942277908\n",
      "Batch loss: 0.06991427391767502\n",
      "Batch loss: 0.30840542912483215\n",
      "Batch loss: 0.21446922421455383\n",
      "Batch loss: 0.10882165282964706\n",
      "Batch loss: 0.1006903275847435\n",
      "Batch loss: 0.26286494731903076\n",
      "Batch loss: 0.14702746272087097\n",
      "Batch loss: 0.21817776560783386\n",
      "Batch loss: 0.10947562009096146\n",
      "Batch loss: 0.09494085609912872\n",
      "Batch loss: 0.16926243901252747\n",
      "Batch loss: 0.13956321775913239\n",
      "Batch loss: 0.06936196237802505\n",
      "Epoch [2/100], Loss: 0.1805\n",
      "Validation Loss: 0.5927, Accuracy: 77.75%\n",
      "Batch loss: 0.2185429185628891\n",
      "Batch loss: 0.18016381561756134\n",
      "Batch loss: 0.18583954870700836\n",
      "Batch loss: 0.15269193053245544\n",
      "Batch loss: 0.19641749560832977\n",
      "Batch loss: 0.19335955381393433\n",
      "Batch loss: 0.4640084505081177\n",
      "Batch loss: 0.2100333273410797\n",
      "Batch loss: 0.23185622692108154\n",
      "Batch loss: 0.1255301684141159\n",
      "Batch loss: 0.23564653098583221\n",
      "Batch loss: 0.26852139830589294\n",
      "Batch loss: 0.14703860878944397\n",
      "Batch loss: 0.25555071234703064\n",
      "Batch loss: 0.2597908079624176\n",
      "Batch loss: 0.2111731618642807\n",
      "Batch loss: 0.18391919136047363\n",
      "Batch loss: 0.2052805870771408\n",
      "Batch loss: 0.14413034915924072\n",
      "Batch loss: 0.21417583525180817\n",
      "Batch loss: 0.13162752985954285\n",
      "Batch loss: 0.15896818041801453\n",
      "Batch loss: 0.17778098583221436\n",
      "Batch loss: 0.12719501554965973\n",
      "Batch loss: 0.12884628772735596\n",
      "Batch loss: 0.1372852325439453\n",
      "Batch loss: 0.21540693938732147\n",
      "Batch loss: 0.19349317252635956\n",
      "Batch loss: 0.10422886162996292\n",
      "Batch loss: 0.10712876170873642\n",
      "Batch loss: 0.23159760236740112\n",
      "Batch loss: 0.14828389883041382\n",
      "Batch loss: 0.1798875629901886\n",
      "Batch loss: 0.12865115702152252\n",
      "Batch loss: 0.10832297056913376\n",
      "Batch loss: 0.276925265789032\n",
      "Batch loss: 0.13288277387619019\n",
      "Batch loss: 0.14883314073085785\n",
      "Batch loss: 0.19196201860904694\n",
      "Batch loss: 0.08485028892755508\n",
      "Batch loss: 0.14853690564632416\n",
      "Batch loss: 0.19556422531604767\n",
      "Batch loss: 0.19548317790031433\n",
      "Batch loss: 0.11840255558490753\n",
      "Batch loss: 0.16488172113895416\n",
      "Batch loss: 0.1313837766647339\n",
      "Batch loss: 0.15358176827430725\n",
      "Batch loss: 0.11821435391902924\n",
      "Batch loss: 0.08069626986980438\n",
      "Batch loss: 0.14810951054096222\n",
      "Batch loss: 0.14107006788253784\n",
      "Batch loss: 0.14521455764770508\n",
      "Batch loss: 0.1367255300283432\n",
      "Batch loss: 0.13387739658355713\n",
      "Batch loss: 0.15095625817775726\n",
      "Batch loss: 0.1842801719903946\n",
      "Batch loss: 0.15836873650550842\n",
      "Batch loss: 0.13776449859142303\n",
      "Batch loss: 0.12942561507225037\n",
      "Batch loss: 0.14112502336502075\n",
      "Batch loss: 0.1385134607553482\n",
      "Batch loss: 0.12731961905956268\n",
      "Batch loss: 0.1873510777950287\n",
      "Batch loss: 0.24161502718925476\n",
      "Batch loss: 0.07785191386938095\n",
      "Batch loss: 0.15190398693084717\n",
      "Batch loss: 0.20466525852680206\n",
      "Batch loss: 0.24835042655467987\n",
      "Batch loss: 0.1324268877506256\n",
      "Batch loss: 0.16078975796699524\n",
      "Batch loss: 0.14129161834716797\n",
      "Batch loss: 0.24122130870819092\n",
      "Batch loss: 0.09761615097522736\n",
      "Batch loss: 0.13489872217178345\n",
      "Batch loss: 0.09451697766780853\n",
      "Batch loss: 0.2064787596464157\n",
      "Batch loss: 0.15810033679008484\n",
      "Batch loss: 0.16299554705619812\n",
      "Batch loss: 0.11748471856117249\n",
      "Batch loss: 0.2024168223142624\n",
      "Batch loss: 0.1026126965880394\n",
      "Batch loss: 0.11188002675771713\n",
      "Batch loss: 0.19367164373397827\n",
      "Batch loss: 0.2600579261779785\n",
      "Batch loss: 0.16708359122276306\n",
      "Batch loss: 0.1043795570731163\n",
      "Batch loss: 0.06832664459943771\n",
      "Batch loss: 0.1484462320804596\n",
      "Batch loss: 0.14466838538646698\n",
      "Batch loss: 0.12644284963607788\n",
      "Batch loss: 0.16882051527500153\n",
      "Batch loss: 0.0839061588048935\n",
      "Batch loss: 0.2376052290201187\n",
      "Batch loss: 0.15252600610256195\n",
      "Batch loss: 0.13013502955436707\n",
      "Batch loss: 0.11428721994161606\n",
      "Batch loss: 0.11439412087202072\n",
      "Batch loss: 0.1938559114933014\n",
      "Batch loss: 0.1447547972202301\n",
      "Batch loss: 0.15719039738178253\n",
      "Batch loss: 0.09441724419593811\n",
      "Batch loss: 0.08072470128536224\n",
      "Batch loss: 0.1502971649169922\n",
      "Batch loss: 0.18538102507591248\n",
      "Batch loss: 0.13175955414772034\n",
      "Batch loss: 0.12880024313926697\n",
      "Batch loss: 0.1412012130022049\n",
      "Batch loss: 0.13872729241847992\n",
      "Batch loss: 0.16123948991298676\n",
      "Batch loss: 0.22354595363140106\n",
      "Batch loss: 0.16724225878715515\n",
      "Batch loss: 0.09656837582588196\n",
      "Batch loss: 0.0849134624004364\n",
      "Batch loss: 0.1023096963763237\n",
      "Batch loss: 0.12113360315561295\n",
      "Batch loss: 0.1685376763343811\n",
      "Batch loss: 0.10596107691526413\n",
      "Batch loss: 0.06161119416356087\n",
      "Batch loss: 0.08854156732559204\n",
      "Batch loss: 0.16915671527385712\n",
      "Batch loss: 0.11013880372047424\n",
      "Batch loss: 0.08734197169542313\n",
      "Batch loss: 0.19737252593040466\n",
      "Batch loss: 0.10312210768461227\n",
      "Batch loss: 0.18724271655082703\n",
      "Batch loss: 0.23585402965545654\n",
      "Batch loss: 0.15554094314575195\n",
      "Batch loss: 0.14826880395412445\n",
      "Batch loss: 0.11729460209608078\n",
      "Batch loss: 0.1338423639535904\n",
      "Batch loss: 0.15069130063056946\n",
      "Batch loss: 0.23839236795902252\n",
      "Batch loss: 0.12673451006412506\n",
      "Batch loss: 0.15147243440151215\n",
      "Batch loss: 0.22295328974723816\n",
      "Batch loss: 0.09130922704935074\n",
      "Batch loss: 0.12307661026716232\n",
      "Batch loss: 0.14290322363376617\n",
      "Batch loss: 0.21437740325927734\n",
      "Batch loss: 0.11366228014230728\n",
      "Batch loss: 0.12712512910366058\n",
      "Batch loss: 0.11402838677167892\n",
      "Batch loss: 0.1769692301750183\n",
      "Batch loss: 0.2321949303150177\n",
      "Batch loss: 0.1523960530757904\n",
      "Batch loss: 0.1410614252090454\n",
      "Batch loss: 0.16803358495235443\n",
      "Batch loss: 0.1560497134923935\n",
      "Batch loss: 0.14828437566757202\n",
      "Batch loss: 0.14362451434135437\n",
      "Batch loss: 0.133114293217659\n",
      "Batch loss: 0.1785936951637268\n",
      "Batch loss: 0.20008017122745514\n",
      "Batch loss: 0.22989298403263092\n",
      "Batch loss: 0.178053081035614\n",
      "Batch loss: 0.18838965892791748\n",
      "Batch loss: 0.1768842339515686\n",
      "Batch loss: 0.11306300759315491\n",
      "Batch loss: 0.10652288049459457\n",
      "Batch loss: 0.14771920442581177\n",
      "Batch loss: 0.07702585309743881\n",
      "Batch loss: 0.1721297949552536\n",
      "Batch loss: 0.1607910692691803\n",
      "Batch loss: 0.18128642439842224\n",
      "Batch loss: 0.10497976839542389\n",
      "Batch loss: 0.17167794704437256\n",
      "Batch loss: 0.15193447470664978\n",
      "Batch loss: 0.15672986209392548\n",
      "Batch loss: 0.1293317973613739\n",
      "Batch loss: 0.12004978954792023\n",
      "Batch loss: 0.17431879043579102\n",
      "Batch loss: 0.2180546522140503\n",
      "Batch loss: 0.13064159452915192\n",
      "Batch loss: 0.12947283685207367\n",
      "Batch loss: 0.12385760992765427\n",
      "Batch loss: 0.20068244636058807\n",
      "Batch loss: 0.21090789139270782\n",
      "Batch loss: 0.09862322360277176\n",
      "Batch loss: 0.19078747928142548\n",
      "Batch loss: 0.15399077534675598\n",
      "Batch loss: 0.21467597782611847\n",
      "Batch loss: 0.08104796707630157\n",
      "Batch loss: 0.07189875841140747\n",
      "Batch loss: 0.2597552239894867\n",
      "Batch loss: 0.17544014751911163\n",
      "Batch loss: 0.11069775372743607\n",
      "Batch loss: 0.17636863887310028\n",
      "Batch loss: 0.11756409704685211\n",
      "Batch loss: 0.1560482680797577\n",
      "Batch loss: 0.134713813662529\n",
      "Batch loss: 0.15098091959953308\n",
      "Batch loss: 0.08344694972038269\n",
      "Batch loss: 0.15599489212036133\n",
      "Batch loss: 0.12418985366821289\n",
      "Batch loss: 0.08662989735603333\n",
      "Batch loss: 0.1324460804462433\n",
      "Batch loss: 0.07781939208507538\n",
      "Batch loss: 0.18230602145195007\n",
      "Batch loss: 0.16816847026348114\n",
      "Batch loss: 0.128801167011261\n",
      "Batch loss: 0.17518863081932068\n",
      "Batch loss: 0.2207869589328766\n",
      "Batch loss: 0.11390305310487747\n",
      "Batch loss: 0.07823635637760162\n",
      "Batch loss: 0.21445117890834808\n",
      "Batch loss: 0.2095343917608261\n",
      "Batch loss: 0.24355962872505188\n",
      "Batch loss: 0.15462757647037506\n",
      "Batch loss: 0.06454362720251083\n",
      "Batch loss: 0.11065066605806351\n",
      "Batch loss: 0.09076254814863205\n",
      "Batch loss: 0.13750572502613068\n",
      "Batch loss: 0.1629476100206375\n",
      "Batch loss: 0.15713340044021606\n",
      "Batch loss: 0.15288256108760834\n",
      "Batch loss: 0.09138906747102737\n",
      "Batch loss: 0.1032777726650238\n",
      "Batch loss: 0.11961197108030319\n",
      "Batch loss: 0.044453829526901245\n",
      "Batch loss: 0.11898215115070343\n",
      "Batch loss: 0.06898343563079834\n",
      "Batch loss: 0.14239653944969177\n",
      "Batch loss: 0.1255670189857483\n",
      "Batch loss: 0.2599146366119385\n",
      "Batch loss: 0.10306434333324432\n",
      "Batch loss: 0.21137472987174988\n",
      "Batch loss: 0.10443486273288727\n",
      "Batch loss: 0.17504025995731354\n",
      "Batch loss: 0.21253708004951477\n",
      "Batch loss: 0.12965421378612518\n",
      "Batch loss: 0.18453101813793182\n",
      "Batch loss: 0.12173838168382645\n",
      "Batch loss: 0.08749351650476456\n",
      "Batch loss: 0.10928085446357727\n",
      "Batch loss: 0.09064041078090668\n",
      "Batch loss: 0.21136491000652313\n",
      "Batch loss: 0.0840073972940445\n",
      "Batch loss: 0.14371861517429352\n",
      "Batch loss: 0.1137671172618866\n",
      "Batch loss: 0.1182362511754036\n",
      "Batch loss: 0.19751274585723877\n",
      "Batch loss: 0.1991644650697708\n",
      "Batch loss: 0.12499872595071793\n",
      "Batch loss: 0.1491997241973877\n",
      "Batch loss: 0.12504923343658447\n",
      "Batch loss: 0.15581707656383514\n",
      "Batch loss: 0.16096259653568268\n",
      "Batch loss: 0.22408442199230194\n",
      "Batch loss: 0.12522758543491364\n",
      "Batch loss: 0.14689655601978302\n",
      "Batch loss: 0.19861051440238953\n",
      "Batch loss: 0.1828581690788269\n",
      "Batch loss: 0.28361764550209045\n",
      "Batch loss: 0.2824432849884033\n",
      "Batch loss: 0.18382562696933746\n",
      "Batch loss: 0.265960156917572\n",
      "Batch loss: 0.20615430176258087\n",
      "Batch loss: 0.1925419270992279\n",
      "Batch loss: 0.13755887746810913\n",
      "Batch loss: 0.17154675722122192\n",
      "Batch loss: 0.14446383714675903\n",
      "Batch loss: 0.1920175403356552\n",
      "Batch loss: 0.18236570060253143\n",
      "Batch loss: 0.1230359822511673\n",
      "Batch loss: 0.17291302978992462\n",
      "Batch loss: 0.113392174243927\n",
      "Batch loss: 0.17664211988449097\n",
      "Batch loss: 0.12134740501642227\n",
      "Batch loss: 0.1510491967201233\n",
      "Batch loss: 0.13946476578712463\n",
      "Batch loss: 0.15508873760700226\n",
      "Batch loss: 0.210158571600914\n",
      "Batch loss: 0.13078121840953827\n",
      "Batch loss: 0.19612251222133636\n",
      "Batch loss: 0.1997883915901184\n",
      "Batch loss: 0.08962514996528625\n",
      "Batch loss: 0.162006676197052\n",
      "Batch loss: 0.19569571316242218\n",
      "Batch loss: 0.1483478546142578\n",
      "Batch loss: 0.22398032248020172\n",
      "Batch loss: 0.1759151667356491\n",
      "Batch loss: 0.11093727499246597\n",
      "Batch loss: 0.2428257167339325\n",
      "Batch loss: 0.3682098686695099\n",
      "Batch loss: 0.1961563676595688\n",
      "Batch loss: 0.19481293857097626\n",
      "Batch loss: 0.14543737471103668\n",
      "Batch loss: 0.12613986432552338\n",
      "Batch loss: 0.15096381306648254\n",
      "Batch loss: 0.09735468029975891\n",
      "Batch loss: 0.11340752243995667\n",
      "Batch loss: 0.12116555124521255\n",
      "Batch loss: 0.1412838250398636\n",
      "Batch loss: 0.13904061913490295\n",
      "Batch loss: 0.17932140827178955\n",
      "Batch loss: 0.20839236676692963\n",
      "Batch loss: 0.17663031816482544\n",
      "Batch loss: 0.11243560165166855\n",
      "Batch loss: 0.2052108198404312\n",
      "Batch loss: 0.1352275311946869\n",
      "Batch loss: 0.15779010951519012\n",
      "Batch loss: 0.097111776471138\n",
      "Batch loss: 0.139009490609169\n",
      "Batch loss: 0.1444106251001358\n",
      "Batch loss: 0.20482191443443298\n",
      "Batch loss: 0.11529724299907684\n",
      "Batch loss: 0.08606908470392227\n",
      "Batch loss: 0.13837169110774994\n",
      "Batch loss: 0.10441222786903381\n",
      "Batch loss: 0.13687965273857117\n",
      "Batch loss: 0.14122414588928223\n",
      "Batch loss: 0.12589359283447266\n",
      "Batch loss: 0.15755623579025269\n",
      "Batch loss: 0.1304181069135666\n",
      "Batch loss: 0.18238653242588043\n",
      "Batch loss: 0.24069303274154663\n",
      "Batch loss: 0.1445343941450119\n",
      "Batch loss: 0.12558172643184662\n",
      "Batch loss: 0.17359596490859985\n",
      "Batch loss: 0.14475961029529572\n",
      "Batch loss: 0.11915689706802368\n",
      "Batch loss: 0.10614290833473206\n",
      "Batch loss: 0.08541329205036163\n",
      "Batch loss: 0.193984255194664\n",
      "Batch loss: 0.13412879407405853\n",
      "Batch loss: 0.16065441071987152\n",
      "Batch loss: 0.17115043103694916\n",
      "Batch loss: 0.2350640445947647\n",
      "Batch loss: 0.14880897104740143\n",
      "Batch loss: 0.19691689312458038\n",
      "Batch loss: 0.09478895366191864\n",
      "Batch loss: 0.16736887395381927\n",
      "Batch loss: 0.09383434802293777\n",
      "Batch loss: 0.17855563759803772\n",
      "Batch loss: 0.19119499623775482\n",
      "Batch loss: 0.16562996804714203\n",
      "Batch loss: 0.052713554352521896\n",
      "Batch loss: 0.07969043403863907\n",
      "Batch loss: 0.19597764313220978\n",
      "Batch loss: 0.10521235316991806\n",
      "Batch loss: 0.14579956233501434\n",
      "Batch loss: 0.09694293886423111\n",
      "Batch loss: 0.15170826017856598\n",
      "Batch loss: 0.18715012073516846\n",
      "Batch loss: 0.07073163241147995\n",
      "Batch loss: 0.18317002058029175\n",
      "Batch loss: 0.19546973705291748\n",
      "Batch loss: 0.2541579604148865\n",
      "Batch loss: 0.12261519581079483\n",
      "Batch loss: 0.1037759780883789\n",
      "Batch loss: 0.13717305660247803\n",
      "Batch loss: 0.14991827309131622\n",
      "Batch loss: 0.059090882539749146\n",
      "Batch loss: 0.10173600167036057\n",
      "Batch loss: 0.1323850154876709\n",
      "Batch loss: 0.13925153017044067\n",
      "Batch loss: 0.07814355194568634\n",
      "Batch loss: 0.1975247859954834\n",
      "Batch loss: 0.19139382243156433\n",
      "Batch loss: 0.22967250645160675\n",
      "Batch loss: 0.14928463101387024\n",
      "Batch loss: 0.14246158301830292\n",
      "Batch loss: 0.11285513639450073\n",
      "Batch loss: 0.16742593050003052\n",
      "Batch loss: 0.19383558630943298\n",
      "Batch loss: 0.13111452758312225\n",
      "Batch loss: 0.18024009466171265\n",
      "Batch loss: 0.14925485849380493\n",
      "Batch loss: 0.1154617965221405\n",
      "Batch loss: 0.22975824773311615\n",
      "Batch loss: 0.10162673145532608\n",
      "Batch loss: 0.12914477288722992\n",
      "Batch loss: 0.15162132680416107\n",
      "Batch loss: 0.10456995666027069\n",
      "Batch loss: 0.1262156218290329\n",
      "Batch loss: 0.13511742651462555\n",
      "Batch loss: 0.13766825199127197\n",
      "Batch loss: 0.09507302194833755\n",
      "Batch loss: 0.23007939755916595\n",
      "Batch loss: 0.21019405126571655\n",
      "Batch loss: 0.25286760926246643\n",
      "Batch loss: 0.1349429339170456\n",
      "Batch loss: 0.10311515629291534\n",
      "Batch loss: 0.14298038184642792\n",
      "Batch loss: 0.06254827231168747\n",
      "Batch loss: 0.15051676332950592\n",
      "Batch loss: 0.20425473153591156\n",
      "Batch loss: 0.13724558055400848\n",
      "Batch loss: 0.14514492452144623\n",
      "Batch loss: 0.18409378826618195\n",
      "Batch loss: 0.09978586435317993\n",
      "Batch loss: 0.1556028574705124\n",
      "Batch loss: 0.07991982251405716\n",
      "Batch loss: 0.12236303836107254\n",
      "Batch loss: 0.2212468832731247\n",
      "Batch loss: 0.18988348543643951\n",
      "Batch loss: 0.05195252224802971\n",
      "Batch loss: 0.11078091710805893\n",
      "Batch loss: 0.09069019556045532\n",
      "Batch loss: 0.07947668433189392\n",
      "Batch loss: 0.11297746002674103\n",
      "Batch loss: 0.08129355311393738\n",
      "Batch loss: 0.27795302867889404\n",
      "Batch loss: 0.12409963458776474\n",
      "Batch loss: 0.10794998705387115\n",
      "Batch loss: 0.22951430082321167\n",
      "Batch loss: 0.1878238469362259\n",
      "Batch loss: 0.10091263055801392\n",
      "Batch loss: 0.11795524507761002\n",
      "Batch loss: 0.15413214266300201\n",
      "Batch loss: 0.13099025189876556\n",
      "Batch loss: 0.10494095087051392\n",
      "Batch loss: 0.05062217265367508\n",
      "Batch loss: 0.06491240113973618\n",
      "Batch loss: 0.10307164490222931\n",
      "Batch loss: 0.10260587185621262\n",
      "Batch loss: 0.04533499479293823\n",
      "Epoch [3/100], Loss: 0.1523\n",
      "Validation Loss: 0.5534, Accuracy: 84.01%\n",
      "Batch loss: 0.15950751304626465\n",
      "Batch loss: 0.19664165377616882\n",
      "Batch loss: 0.15547236800193787\n",
      "Batch loss: 0.1803303062915802\n",
      "Batch loss: 0.1495332270860672\n",
      "Batch loss: 0.1764388531446457\n",
      "Batch loss: 0.1963517814874649\n",
      "Batch loss: 0.17164741456508636\n",
      "Batch loss: 0.20450003445148468\n",
      "Batch loss: 0.09908000379800797\n",
      "Batch loss: 0.13077357411384583\n",
      "Batch loss: 0.13144680857658386\n",
      "Batch loss: 0.12160494923591614\n",
      "Batch loss: 0.15555524826049805\n",
      "Batch loss: 0.20477834343910217\n",
      "Batch loss: 0.1312965601682663\n",
      "Batch loss: 0.12902045249938965\n",
      "Batch loss: 0.14935341477394104\n",
      "Batch loss: 0.0705738514661789\n",
      "Batch loss: 0.16401329636573792\n",
      "Batch loss: 0.153438001871109\n",
      "Batch loss: 0.1595812290906906\n",
      "Batch loss: 0.11183800548315048\n",
      "Batch loss: 0.1291966438293457\n",
      "Batch loss: 0.14939475059509277\n",
      "Batch loss: 0.13595210015773773\n",
      "Batch loss: 0.19398485124111176\n",
      "Batch loss: 0.14546723663806915\n",
      "Batch loss: 0.09440556913614273\n",
      "Batch loss: 0.11878430843353271\n",
      "Batch loss: 0.24574275314807892\n",
      "Batch loss: 0.16385820508003235\n",
      "Batch loss: 0.1390625387430191\n",
      "Batch loss: 0.1278974860906601\n",
      "Batch loss: 0.14754964411258698\n",
      "Batch loss: 0.22781483829021454\n",
      "Batch loss: 0.16305434703826904\n",
      "Batch loss: 0.12484509497880936\n",
      "Batch loss: 0.16693413257598877\n",
      "Batch loss: 0.10340581089258194\n",
      "Batch loss: 0.11423294246196747\n",
      "Batch loss: 0.12075136601924896\n",
      "Batch loss: 0.16050074994564056\n",
      "Batch loss: 0.11822988092899323\n",
      "Batch loss: 0.17347893118858337\n",
      "Batch loss: 0.12513688206672668\n",
      "Batch loss: 0.1843450963497162\n",
      "Batch loss: 0.10399173945188522\n",
      "Batch loss: 0.07507973909378052\n",
      "Batch loss: 0.14678485691547394\n",
      "Batch loss: 0.11889161914587021\n",
      "Batch loss: 0.16189247369766235\n",
      "Batch loss: 0.10962533205747604\n",
      "Batch loss: 0.11129366606473923\n",
      "Batch loss: 0.1255176067352295\n",
      "Batch loss: 0.15775936841964722\n",
      "Batch loss: 0.093589648604393\n",
      "Batch loss: 0.1395791620016098\n",
      "Batch loss: 0.13671952486038208\n",
      "Batch loss: 0.12612947821617126\n",
      "Batch loss: 0.10424551367759705\n",
      "Batch loss: 0.13091503083705902\n",
      "Batch loss: 0.14912554621696472\n",
      "Batch loss: 0.19014513492584229\n",
      "Batch loss: 0.0676170065999031\n",
      "Batch loss: 0.10836675763130188\n",
      "Batch loss: 0.19123217463493347\n",
      "Batch loss: 0.2431419938802719\n",
      "Batch loss: 0.10384901612997055\n",
      "Batch loss: 0.15208137035369873\n",
      "Batch loss: 0.12367354333400726\n",
      "Batch loss: 0.2147177755832672\n",
      "Batch loss: 0.09890342503786087\n",
      "Batch loss: 0.16801650822162628\n",
      "Batch loss: 0.05267748981714249\n",
      "Batch loss: 0.1682378351688385\n",
      "Batch loss: 0.1322316825389862\n",
      "Batch loss: 0.1781070977449417\n",
      "Batch loss: 0.1108301505446434\n",
      "Batch loss: 0.15255041420459747\n",
      "Batch loss: 0.08205704391002655\n",
      "Batch loss: 0.07897648215293884\n",
      "Batch loss: 0.16079942882061005\n",
      "Batch loss: 0.2251078337430954\n",
      "Batch loss: 0.11813699454069138\n",
      "Batch loss: 0.10417423397302628\n",
      "Batch loss: 0.057240601629018784\n",
      "Batch loss: 0.15424230694770813\n",
      "Batch loss: 0.14785003662109375\n",
      "Batch loss: 0.09455371648073196\n",
      "Batch loss: 0.1856769472360611\n",
      "Batch loss: 0.07419861108064651\n",
      "Batch loss: 0.21378804743289948\n",
      "Batch loss: 0.11401044577360153\n",
      "Batch loss: 0.136294424533844\n",
      "Batch loss: 0.11456621438264847\n",
      "Batch loss: 0.09873676300048828\n",
      "Batch loss: 0.15333202481269836\n",
      "Batch loss: 0.11845416575670242\n",
      "Batch loss: 0.12495095282793045\n",
      "Batch loss: 0.10747190564870834\n",
      "Batch loss: 0.0844193547964096\n",
      "Batch loss: 0.173789381980896\n",
      "Batch loss: 0.12923423945903778\n",
      "Batch loss: 0.0928458422422409\n",
      "Batch loss: 0.1143951267004013\n",
      "Batch loss: 0.15712571144104004\n",
      "Batch loss: 0.12537048757076263\n",
      "Batch loss: 0.138143390417099\n",
      "Batch loss: 0.17084366083145142\n",
      "Batch loss: 0.14763040840625763\n",
      "Batch loss: 0.07455392926931381\n",
      "Batch loss: 0.10412681847810745\n",
      "Batch loss: 0.07163199037313461\n",
      "Batch loss: 0.09817131608724594\n",
      "Batch loss: 0.13071660697460175\n",
      "Batch loss: 0.09319416433572769\n",
      "Batch loss: 0.04876438528299332\n",
      "Batch loss: 0.11008986085653305\n",
      "Batch loss: 0.11661384999752045\n",
      "Batch loss: 0.0653662234544754\n",
      "Batch loss: 0.11868098378181458\n",
      "Batch loss: 0.17622125148773193\n",
      "Batch loss: 0.09776423126459122\n",
      "Batch loss: 0.15031477808952332\n",
      "Batch loss: 0.11967965215444565\n",
      "Batch loss: 0.11395411938428879\n",
      "Batch loss: 0.14684143662452698\n",
      "Batch loss: 0.1285703033208847\n",
      "Batch loss: 0.16555753350257874\n",
      "Batch loss: 0.14801624417304993\n",
      "Batch loss: 0.1671973317861557\n",
      "Batch loss: 0.1108437329530716\n",
      "Batch loss: 0.26171875\n",
      "Batch loss: 0.16358454525470734\n",
      "Batch loss: 0.06439753621816635\n",
      "Batch loss: 0.07441078126430511\n",
      "Batch loss: 0.1557290256023407\n",
      "Batch loss: 0.1619008332490921\n",
      "Batch loss: 0.07556037604808807\n",
      "Batch loss: 0.10869103670120239\n",
      "Batch loss: 0.07429257780313492\n",
      "Batch loss: 0.16179248690605164\n",
      "Batch loss: 0.16777092218399048\n",
      "Batch loss: 0.13872133195400238\n",
      "Batch loss: 0.12614011764526367\n",
      "Batch loss: 0.13933831453323364\n",
      "Batch loss: 0.10428915917873383\n",
      "Batch loss: 0.12452691048383713\n",
      "Batch loss: 0.08974714577198029\n",
      "Batch loss: 0.11380431056022644\n",
      "Batch loss: 0.10833299160003662\n",
      "Batch loss: 0.1719386726617813\n",
      "Batch loss: 0.1320606917142868\n",
      "Batch loss: 0.16388297080993652\n",
      "Batch loss: 0.12551261484622955\n",
      "Batch loss: 0.17277537286281586\n",
      "Batch loss: 0.12025096267461777\n",
      "Batch loss: 0.1351262331008911\n",
      "Batch loss: 0.11672396957874298\n",
      "Batch loss: 0.07300611585378647\n",
      "Batch loss: 0.16761915385723114\n",
      "Batch loss: 0.1130075454711914\n",
      "Batch loss: 0.1403561681509018\n",
      "Batch loss: 0.11313579976558685\n",
      "Batch loss: 0.18340156972408295\n",
      "Batch loss: 0.10060102492570877\n",
      "Batch loss: 0.10883034020662308\n",
      "Batch loss: 0.11809643357992172\n",
      "Batch loss: 0.10214128345251083\n",
      "Batch loss: 0.144158273935318\n",
      "Batch loss: 0.1794939637184143\n",
      "Batch loss: 0.08147308975458145\n",
      "Batch loss: 0.10235705226659775\n",
      "Batch loss: 0.09091132134199142\n",
      "Batch loss: 0.19491422176361084\n",
      "Batch loss: 0.12691010534763336\n",
      "Batch loss: 0.09058160334825516\n",
      "Batch loss: 0.1807040572166443\n",
      "Batch loss: 0.09212557971477509\n",
      "Batch loss: 0.1454271376132965\n",
      "Batch loss: 0.142482191324234\n",
      "Batch loss: 0.07279769331216812\n",
      "Batch loss: 0.23197612166404724\n",
      "Batch loss: 0.16207846999168396\n",
      "Batch loss: 0.0876220166683197\n",
      "Batch loss: 0.15826594829559326\n",
      "Batch loss: 0.1312931627035141\n",
      "Batch loss: 0.12973926961421967\n",
      "Batch loss: 0.16162849962711334\n",
      "Batch loss: 0.1455613076686859\n",
      "Batch loss: 0.1037917509675026\n",
      "Batch loss: 0.14461691677570343\n",
      "Batch loss: 0.11104824393987656\n",
      "Batch loss: 0.09980981051921844\n",
      "Batch loss: 0.0869452953338623\n",
      "Batch loss: 0.11343543976545334\n",
      "Batch loss: 0.1560637652873993\n",
      "Batch loss: 0.12218376994132996\n",
      "Batch loss: 0.14657069742679596\n",
      "Batch loss: 0.2198343276977539\n",
      "Batch loss: 0.2159736454486847\n",
      "Batch loss: 0.1346047967672348\n",
      "Batch loss: 0.06700136512517929\n",
      "Batch loss: 0.1599625200033188\n",
      "Batch loss: 0.20294111967086792\n",
      "Batch loss: 0.1998801976442337\n",
      "Batch loss: 0.11720071732997894\n",
      "Batch loss: 0.06967097520828247\n",
      "Batch loss: 0.11997058987617493\n",
      "Batch loss: 0.10008420795202255\n",
      "Batch loss: 0.19607864320278168\n",
      "Batch loss: 0.1425231546163559\n",
      "Batch loss: 0.12494251877069473\n",
      "Batch loss: 0.12424540519714355\n",
      "Batch loss: 0.0917566642165184\n",
      "Batch loss: 0.0834638699889183\n",
      "Batch loss: 0.10430976748466492\n",
      "Batch loss: 0.0639171227812767\n",
      "Batch loss: 0.11499426513910294\n",
      "Batch loss: 0.060166582465171814\n",
      "Batch loss: 0.06916678696870804\n",
      "Batch loss: 0.06144396960735321\n",
      "Batch loss: 0.15837229788303375\n",
      "Batch loss: 0.07686113566160202\n",
      "Batch loss: 0.124913290143013\n",
      "Batch loss: 0.05464168265461922\n",
      "Batch loss: 0.1599365472793579\n",
      "Batch loss: 0.18191741406917572\n",
      "Batch loss: 0.13580816984176636\n",
      "Batch loss: 0.13175109028816223\n",
      "Batch loss: 0.04864777997136116\n",
      "Batch loss: 0.05026858672499657\n",
      "Batch loss: 0.06954687088727951\n",
      "Batch loss: 0.06742801517248154\n",
      "Batch loss: 0.13675016164779663\n",
      "Batch loss: 0.06972609460353851\n",
      "Batch loss: 0.14745710790157318\n",
      "Batch loss: 0.07634738832712173\n",
      "Batch loss: 0.09399107843637466\n",
      "Batch loss: 0.15174369513988495\n",
      "Batch loss: 0.18020442128181458\n",
      "Batch loss: 0.0456654354929924\n",
      "Batch loss: 0.13366754353046417\n",
      "Batch loss: 0.11234875023365021\n",
      "Batch loss: 0.12457215785980225\n",
      "Batch loss: 0.10026255249977112\n",
      "Batch loss: 0.12348110228776932\n",
      "Batch loss: 0.1124693751335144\n",
      "Batch loss: 0.13977929949760437\n",
      "Batch loss: 0.13169921934604645\n",
      "Batch loss: 0.26963767409324646\n",
      "Batch loss: 0.28790149092674255\n",
      "Batch loss: 0.22292831540107727\n",
      "Batch loss: 0.08312307298183441\n",
      "Batch loss: 0.3211594223976135\n",
      "Batch loss: 0.13433964550495148\n",
      "Batch loss: 0.1974765658378601\n",
      "Batch loss: 0.1823655515909195\n",
      "Batch loss: 0.17561806738376617\n",
      "Batch loss: 0.18195998668670654\n",
      "Batch loss: 0.1394338607788086\n",
      "Batch loss: 0.17855052649974823\n",
      "Batch loss: 0.12824277579784393\n",
      "Batch loss: 0.19356390833854675\n",
      "Batch loss: 0.1190192922949791\n",
      "Batch loss: 0.235218346118927\n",
      "Batch loss: 0.17593145370483398\n",
      "Batch loss: 0.1535688042640686\n",
      "Batch loss: 0.1389956921339035\n",
      "Batch loss: 0.153631329536438\n",
      "Batch loss: 0.14460133016109467\n",
      "Batch loss: 0.11234038323163986\n",
      "Batch loss: 0.14197060465812683\n",
      "Batch loss: 0.1712552011013031\n",
      "Batch loss: 0.0955754965543747\n",
      "Batch loss: 0.12440647929906845\n",
      "Batch loss: 0.19953708350658417\n",
      "Batch loss: 0.11167755722999573\n",
      "Batch loss: 0.18567213416099548\n",
      "Batch loss: 0.1012578085064888\n",
      "Batch loss: 0.09143514186143875\n",
      "Batch loss: 0.21384774148464203\n",
      "Batch loss: 0.29415199160575867\n",
      "Batch loss: 0.1394527554512024\n",
      "Batch loss: 0.21111980080604553\n",
      "Batch loss: 0.1348719298839569\n",
      "Batch loss: 0.0919606164097786\n",
      "Batch loss: 0.12874113023281097\n",
      "Batch loss: 0.09844803810119629\n",
      "Batch loss: 0.09819258749485016\n",
      "Batch loss: 0.11637488752603531\n",
      "Batch loss: 0.13469292223453522\n",
      "Batch loss: 0.11206688731908798\n",
      "Batch loss: 0.12785865366458893\n",
      "Batch loss: 0.20088331401348114\n",
      "Batch loss: 0.12162316590547562\n",
      "Batch loss: 0.11184284090995789\n",
      "Batch loss: 0.1506507247686386\n",
      "Batch loss: 0.08423423767089844\n",
      "Batch loss: 0.15063084661960602\n",
      "Batch loss: 0.09648369252681732\n",
      "Batch loss: 0.13830219209194183\n",
      "Batch loss: 0.09706383943557739\n",
      "Batch loss: 0.18091972172260284\n",
      "Batch loss: 0.10616078972816467\n",
      "Batch loss: 0.05509753152728081\n",
      "Batch loss: 0.08663993328809738\n",
      "Batch loss: 0.0915672555565834\n",
      "Batch loss: 0.13906672596931458\n",
      "Batch loss: 0.1182992234826088\n",
      "Batch loss: 0.09952378273010254\n",
      "Batch loss: 0.12818363308906555\n",
      "Batch loss: 0.12051326036453247\n",
      "Batch loss: 0.14890074729919434\n",
      "Batch loss: 0.19005997478961945\n",
      "Batch loss: 0.12518636882305145\n",
      "Batch loss: 0.11327874660491943\n",
      "Batch loss: 0.11698879301548004\n",
      "Batch loss: 0.07850410044193268\n",
      "Batch loss: 0.11336769908666611\n",
      "Batch loss: 0.08115167915821075\n",
      "Batch loss: 0.07742268592119217\n",
      "Batch loss: 0.2287645936012268\n",
      "Batch loss: 0.11628670990467072\n",
      "Batch loss: 0.18672464787960052\n",
      "Batch loss: 0.16119825839996338\n",
      "Batch loss: 0.24655704200267792\n",
      "Batch loss: 0.1301683783531189\n",
      "Batch loss: 0.1550799161195755\n",
      "Batch loss: 0.1021488830447197\n",
      "Batch loss: 0.20595164597034454\n",
      "Batch loss: 0.09514851868152618\n",
      "Batch loss: 0.15683883428573608\n",
      "Batch loss: 0.17852436006069183\n",
      "Batch loss: 0.11975914239883423\n",
      "Batch loss: 0.05362856015563011\n",
      "Batch loss: 0.1514904797077179\n",
      "Batch loss: 0.15920555591583252\n",
      "Batch loss: 0.0794360563158989\n",
      "Batch loss: 0.17266890406608582\n",
      "Batch loss: 0.08878083527088165\n",
      "Batch loss: 0.10626548528671265\n",
      "Batch loss: 0.1586841344833374\n",
      "Batch loss: 0.08195266872644424\n",
      "Batch loss: 0.19412149488925934\n",
      "Batch loss: 0.14522245526313782\n",
      "Batch loss: 0.17277061939239502\n",
      "Batch loss: 0.11119808256626129\n",
      "Batch loss: 0.08052108436822891\n",
      "Batch loss: 0.09251207858324051\n",
      "Batch loss: 0.17831480503082275\n",
      "Batch loss: 0.06307618319988251\n",
      "Batch loss: 0.12540310621261597\n",
      "Batch loss: 0.1262490451335907\n",
      "Batch loss: 0.12548109889030457\n",
      "Batch loss: 0.10499272495508194\n",
      "Batch loss: 0.17369148135185242\n",
      "Batch loss: 0.1601475030183792\n",
      "Batch loss: 0.21435008943080902\n",
      "Batch loss: 0.10554200410842896\n",
      "Batch loss: 0.13269172608852386\n",
      "Batch loss: 0.08547201007604599\n",
      "Batch loss: 0.1623196303844452\n",
      "Batch loss: 0.1318839192390442\n",
      "Batch loss: 0.12796197831630707\n",
      "Batch loss: 0.16887429356575012\n",
      "Batch loss: 0.10489852726459503\n",
      "Batch loss: 0.12507814168930054\n",
      "Batch loss: 0.2054155319929123\n",
      "Batch loss: 0.08692371100187302\n",
      "Batch loss: 0.1245909258723259\n",
      "Batch loss: 0.11692342907190323\n",
      "Batch loss: 0.0766046941280365\n",
      "Batch loss: 0.1473270058631897\n",
      "Batch loss: 0.10105215758085251\n",
      "Batch loss: 0.1203688383102417\n",
      "Batch loss: 0.06483831256628036\n",
      "Batch loss: 0.1795598417520523\n",
      "Batch loss: 0.17838561534881592\n",
      "Batch loss: 0.23421639204025269\n",
      "Batch loss: 0.1008061021566391\n",
      "Batch loss: 0.08647282421588898\n",
      "Batch loss: 0.0955594927072525\n",
      "Batch loss: 0.05311030521988869\n",
      "Batch loss: 0.07681488990783691\n",
      "Batch loss: 0.18655207753181458\n",
      "Batch loss: 0.14003600180149078\n",
      "Batch loss: 0.12021556496620178\n",
      "Batch loss: 0.15290680527687073\n",
      "Batch loss: 0.08889175951480865\n",
      "Batch loss: 0.1015605628490448\n",
      "Batch loss: 0.07730139791965485\n",
      "Batch loss: 0.09283381700515747\n",
      "Batch loss: 0.1884005069732666\n",
      "Batch loss: 0.1493832767009735\n",
      "Batch loss: 0.04859236627817154\n",
      "Batch loss: 0.11873280256986618\n",
      "Batch loss: 0.12963511049747467\n",
      "Batch loss: 0.08705894649028778\n",
      "Batch loss: 0.09627855569124222\n",
      "Batch loss: 0.10296425968408585\n",
      "Batch loss: 0.277580589056015\n",
      "Batch loss: 0.11325632780790329\n",
      "Batch loss: 0.07808452099561691\n",
      "Batch loss: 0.233345165848732\n",
      "Batch loss: 0.1268043965101242\n",
      "Batch loss: 0.15134260058403015\n",
      "Batch loss: 0.09605490416288376\n",
      "Batch loss: 0.1522901952266693\n",
      "Batch loss: 0.14313828945159912\n",
      "Batch loss: 0.12818147242069244\n",
      "Batch loss: 0.0677676722407341\n",
      "Batch loss: 0.1142120361328125\n",
      "Batch loss: 0.10265965014696121\n",
      "Batch loss: 0.09484793245792389\n",
      "Batch loss: 0.04338333383202553\n",
      "Epoch [4/100], Loss: 0.1327\n",
      "Validation Loss: 0.2544, Accuracy: 92.13%\n",
      "Batch loss: 0.13623125851154327\n",
      "Batch loss: 0.13216552138328552\n",
      "Batch loss: 0.11347898840904236\n",
      "Batch loss: 0.13309375941753387\n",
      "Batch loss: 0.12935353815555573\n",
      "Batch loss: 0.08037011325359344\n",
      "Batch loss: 0.17882496118545532\n",
      "Batch loss: 0.1563226580619812\n",
      "Batch loss: 0.14766640961170197\n",
      "Batch loss: 0.08251432329416275\n",
      "Batch loss: 0.09448463469743729\n",
      "Batch loss: 0.13424208760261536\n",
      "Batch loss: 0.12942439317703247\n",
      "Batch loss: 0.1641286015510559\n",
      "Batch loss: 0.14082778990268707\n",
      "Batch loss: 0.1370648890733719\n",
      "Batch loss: 0.10508500039577484\n",
      "Batch loss: 0.10191924124956131\n",
      "Batch loss: 0.05966823175549507\n",
      "Batch loss: 0.15623554587364197\n",
      "Batch loss: 0.21882352232933044\n",
      "Batch loss: 0.1516389101743698\n",
      "Batch loss: 0.07691220194101334\n",
      "Batch loss: 0.07831434160470963\n",
      "Batch loss: 0.08978381007909775\n",
      "Batch loss: 0.12289255112409592\n",
      "Batch loss: 0.13782520592212677\n",
      "Batch loss: 0.08412636816501617\n",
      "Batch loss: 0.09417475014925003\n",
      "Batch loss: 0.14062286913394928\n",
      "Batch loss: 0.23505547642707825\n",
      "Batch loss: 0.12129347771406174\n",
      "Batch loss: 0.13174282014369965\n",
      "Batch loss: 0.08282288908958435\n",
      "Batch loss: 0.07336940616369247\n",
      "Batch loss: 0.1521647870540619\n",
      "Batch loss: 0.1318850964307785\n",
      "Batch loss: 0.12412513792514801\n",
      "Batch loss: 0.12349710613489151\n",
      "Batch loss: 0.08271951228380203\n",
      "Batch loss: 0.10778176784515381\n",
      "Batch loss: 0.12509635090827942\n",
      "Batch loss: 0.1478027105331421\n",
      "Batch loss: 0.1016259416937828\n",
      "Batch loss: 0.11475308239459991\n",
      "Batch loss: 0.13030187785625458\n",
      "Batch loss: 0.12063496559858322\n",
      "Batch loss: 0.1094183400273323\n",
      "Batch loss: 0.07996097207069397\n",
      "Batch loss: 0.09186578541994095\n",
      "Batch loss: 0.1410941481590271\n",
      "Batch loss: 0.10264289379119873\n",
      "Batch loss: 0.08382830023765564\n",
      "Batch loss: 0.08366405963897705\n",
      "Batch loss: 0.11199364066123962\n",
      "Batch loss: 0.13117966055870056\n",
      "Batch loss: 0.06668214499950409\n",
      "Batch loss: 0.0869857594370842\n",
      "Batch loss: 0.12235615402460098\n",
      "Batch loss: 0.16261932253837585\n",
      "Batch loss: 0.11239049583673477\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     images, labels = \u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels.to(device)\n\u001b[32m     23\u001b[39m     optimizer.zero_grad()\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_loss_curves = {key : [] for key in denoise_methods.keys()}\n",
    "val_loss_curves = {key : [] for key in denoise_methods.keys()}\n",
    "\n",
    "for denoise, model in denoising_cnn_models.items():\n",
    "    print(f\"Start training {denoise} model.\")\n",
    "    best_val_loss = float('inf')\n",
    "    epoch_no_improvement = 0\n",
    "    best_model_parameters = None\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loader, val_loader, _ = split_dataset(denoising_datasets.get(denoise))\n",
    "    try:\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.amp.autocast(\"cuda\"):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                # outputs = model(images)\n",
    "                # loss = criterion(outputs, labels)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # loss.backward()\n",
    "                # optimizer.step()\n",
    "\n",
    "                print(f\"Batch loss: {loss}\")\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            training_loss = running_loss/len(train_loader)\n",
    "            training_loss_curves[denoise].append(training_loss)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {training_loss:.4f}\")\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            avg_val_loss = val_loss/len(val_loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "            print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "            val_loss_curves[denoise].append(avg_val_loss)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_parameters = model.state_dict()\n",
    "                epoch_no_improvement = 0\n",
    "            else:\n",
    "                epoch_no_improvement += 1\n",
    "                if epoch_no_improvement == patience:\n",
    "                    print(f\"No improvement for {patience} epoches. Early stopping.\")\n",
    "                    break\n",
    "\n",
    "        if best_model_parameters is not None:\n",
    "            model.load_state_dict(best_model_parameters)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        # torch.save(model.state_dict(), f'CNN_{denoise}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2676a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training None model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m optimizer.zero_grad()\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# outputs = model(images)\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# loss = criterion(outputs, labels)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_rocm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_rocm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_rocm/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_rocm/lib/python3.12/site-packages/torchvision/models/resnet.py:268\u001b[39m, in \u001b[36mResNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bn1(x)\n\u001b[32m    270\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.relu(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_rocm/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_rocm/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_rocm/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_rocm/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_loss_curves = {key : [] for key in denoise_methods.keys()}\n",
    "val_loss_curves = {key : [] for key in denoise_methods.keys()}\n",
    "\n",
    "for denoise, model in denoising_cnn_models.items():\n",
    "    print(f\"Start training {denoise} model.\")\n",
    "    best_val_loss = float('inf')\n",
    "    epoch_no_improvement = 0\n",
    "    best_model_parameters = None\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loader, val_loader, _ = split_dataset(denoising_datasets.get(denoise))\n",
    "    try:\n",
    "        scaler = torch.amp.GradScaler()\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.amp.autocast(\"cuda\"):\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                # outputs = model(images)\n",
    "                # loss = criterion(outputs, labels)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # loss.backward()\n",
    "                # optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            training_loss = running_loss/len(train_loader)\n",
    "            training_loss_curves[denoise].append(training_loss)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {training_loss:.4f}\")\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            avg_val_loss = val_loss/len(val_loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "            print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "            val_loss_curves[denoise].append(avg_val_loss)\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_parameters = model.state_dict()\n",
    "                epoch_no_improvement = 0\n",
    "            else:\n",
    "                epoch_no_improvement += 1\n",
    "                if epoch_no_improvement == patience:\n",
    "                    print(f\"No improvement for {patience} epoches. Early stopping.\")\n",
    "                    break\n",
    "\n",
    "        if best_model_parameters is not None:\n",
    "            model.load_state_dict(best_model_parameters)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a355b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for denoise, model in denoising_cnn_models.items():\n",
    "#    torch.save(model.state_dict(), f'denoised_models/CNN_{denoise}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e0dc9",
   "metadata": {},
   "source": [
    "### CNN models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for denoise, model in denoising_cnn_models.items():\n",
    "    _, _, test_loader = split_dataset(denoising_datasets.get(denoise))\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    end = time.time()\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    elapsed_time = end - start\n",
    "    \n",
    "    models_denoising_accuracies[\"CNN\"][denoise] = accuracy\n",
    "    models_denoising_f1_scores[\"CNN\"][denoise] = f1\n",
    "    models_denoising_confusion_metrics[\"CNN\"][denoise] = cm\n",
    "    models_denoising_classification_times[\"CNN\"][denoise] = elapsed_time\n",
    "    \n",
    "    print(f\"{denoise} accuracy: {accuracy}\")\n",
    "    print(f\"{denoise} f1 score: {f1}\")\n",
    "    print(f\"{denoise} classification time: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c113a538",
   "metadata": {},
   "source": [
    "### Feature extractor for traditional machine learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc7d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_feature_extractor(model, dataset):\n",
    "    model.eval()\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1]) # remove the last layer\n",
    "    feature_extractor.eval()\n",
    "    feature_extractor.to(device)\n",
    "\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    train_loader, _, test_loader = split_dataset(dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            output = feature_extractor(images).squeeze()\n",
    "            train_features.append(output.cpu().numpy())\n",
    "            train_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    X_train = np.vstack(train_features)\n",
    "    y_train = np.hstack(train_labels)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            output = feature_extractor(images).squeeze()\n",
    "            test_features.append(output.cpu().numpy())\n",
    "            test_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    X_test = np.vstack(test_features)\n",
    "    y_test = np.hstack(test_labels)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32219358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5332\n",
      "2286\n",
      "1143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM None accuracy: 0.8626543209876543\n",
      "SVM None f1 score: 0.8616103834590064\n",
      "SVM None classification time: 0.8644464015960693\n",
      "SVM None accuracy: 0.8580246913580247\n",
      "SVM None f1 score: 0.859822498593685\n",
      "SVM None classification time: 0.8489458560943604\n",
      "SVM None accuracy: 0.8441358024691358\n",
      "SVM None f1 score: 0.8394558048813368\n",
      "SVM None classification time: 0.8303875923156738\n",
      "SVM None accuracy: 0.8302469135802469\n",
      "SVM None f1 score: 0.8285276305013146\n",
      "SVM None classification time: 0.8441228866577148\n",
      "SVM None accuracy: 0.845679012345679\n",
      "SVM None f1 score: 0.8493069047716347\n",
      "SVM None classification time: 0.8485069274902344\n",
      "SVM None accuracy: 0.848531684698609\n",
      "SVM None f1 score: 0.8487342117670412\n",
      "SVM None classification time: 0.839282751083374\n",
      "SVM None accuracy: 0.8438948995363215\n",
      "SVM None f1 score: 0.840786569338059\n",
      "SVM None classification time: 0.8272037506103516\n",
      "SVM None accuracy: 0.874806800618238\n",
      "SVM None f1 score: 0.8753099275992996\n",
      "SVM None classification time: 0.6879076957702637\n",
      "SVM None accuracy: 0.8531684698608965\n",
      "SVM None f1 score: 0.8491596960100324\n",
      "SVM None classification time: 0.828066349029541\n",
      "SVM None accuracy: 0.8423493044822257\n",
      "SVM None f1 score: 0.8387804429739912\n",
      "SVM None classification time: 0.8336968421936035\n",
      "Random Forest None accuracy: 0.8194444444444444\n",
      "Random Forest None f1 score: 0.8158539990707275\n",
      "Random Forest None classification time: 8.02333950996399\n",
      "Random Forest None accuracy: 0.8317901234567902\n",
      "Random Forest None f1 score: 0.8325888481066889\n",
      "Random Forest None classification time: 8.234793663024902\n",
      "Random Forest None accuracy: 0.8101851851851852\n",
      "Random Forest None f1 score: 0.8014426589153959\n",
      "Random Forest None classification time: 7.9224653244018555\n",
      "Random Forest None accuracy: 0.7962962962962963\n",
      "Random Forest None f1 score: 0.7948863773172722\n",
      "Random Forest None classification time: 7.868056774139404\n",
      "Random Forest None accuracy: 0.8225308641975309\n",
      "Random Forest None f1 score: 0.8242358639311326\n",
      "Random Forest None classification time: 7.946845293045044\n",
      "Random Forest None accuracy: 0.8068006182380216\n",
      "Random Forest None f1 score: 0.8051574663428708\n",
      "Random Forest None classification time: 7.8946311473846436\n",
      "Random Forest None accuracy: 0.8129829984544049\n",
      "Random Forest None f1 score: 0.8095856926045794\n",
      "Random Forest None classification time: 8.139342308044434\n",
      "Random Forest None accuracy: 0.8222565687789799\n",
      "Random Forest None f1 score: 0.8186035990482835\n",
      "Random Forest None classification time: 7.938580751419067\n",
      "Random Forest None accuracy: 0.8253477588871716\n",
      "Random Forest None f1 score: 0.8190598973278465\n",
      "Random Forest None classification time: 7.8842244148254395\n",
      "Random Forest None accuracy: 0.8098918083462133\n",
      "Random Forest None f1 score: 0.805399285150064\n",
      "Random Forest None classification time: 7.963507890701294\n",
      "Best 1NN None accuracy: 0.8580246913580247\n",
      "Best 1NN None f1 score: 0.8549660747273289\n",
      "Best 1NN None classification time: 0.0006351470947265625\n",
      "Best 1NN None accuracy: 0.8595679012345679\n",
      "Best 1NN None f1 score: 0.8592745677521894\n",
      "Best 1NN None classification time: 0.0006775856018066406\n",
      "Best 1NN None accuracy: 0.816358024691358\n",
      "Best 1NN None f1 score: 0.8136560014175886\n",
      "Best 1NN None classification time: 0.0006842613220214844\n",
      "Best 1NN None accuracy: 0.8364197530864198\n",
      "Best 1NN None f1 score: 0.8393813589709112\n",
      "Best 1NN None classification time: 0.0007047653198242188\n",
      "Best 1NN None accuracy: 0.8472222222222222\n",
      "Best 1NN None f1 score: 0.8477814819274289\n",
      "Best 1NN None classification time: 0.0007569789886474609\n",
      "Best 1NN None accuracy: 0.8315301391035549\n",
      "Best 1NN None f1 score: 0.8242753984732349\n",
      "Best 1NN None classification time: 0.000762939453125\n",
      "Best 1NN None accuracy: 0.8423493044822257\n",
      "Best 1NN None f1 score: 0.8377697159107104\n",
      "Best 1NN None classification time: 0.0007746219635009766\n",
      "Best 1NN None accuracy: 0.8408037094281299\n",
      "Best 1NN None f1 score: 0.8425079172068924\n",
      "Best 1NN None classification time: 0.0007765293121337891\n",
      "Best 1NN None accuracy: 0.8377125193199382\n",
      "Best 1NN None f1 score: 0.8413379721482701\n",
      "Best 1NN None classification time: 0.0007507801055908203\n",
      "Best 1NN None accuracy: 0.8578052550231839\n",
      "Best 1NN None f1 score: 0.8589921992006619\n",
      "Best 1NN None classification time: 0.0007653236389160156\n",
      "Best 3NN None accuracy: 0.8487654320987654\n",
      "Best 3NN None f1 score: 0.8440873015873015\n",
      "Best 3NN None classification time: 0.0007317066192626953\n",
      "Best 3NN None accuracy: 0.8503086419753086\n",
      "Best 3NN None f1 score: 0.8466022464315396\n",
      "Best 3NN None classification time: 0.0007832050323486328\n",
      "Best 3NN None accuracy: 0.8194444444444444\n",
      "Best 3NN None f1 score: 0.812574308613756\n",
      "Best 3NN None classification time: 0.0007824897766113281\n",
      "Best 3NN None accuracy: 0.8333333333333334\n",
      "Best 3NN None f1 score: 0.8313957478576074\n",
      "Best 3NN None classification time: 0.0007753372192382812\n",
      "Best 3NN None accuracy: 0.8348765432098766\n",
      "Best 3NN None f1 score: 0.8350098481733603\n",
      "Best 3NN None classification time: 0.0007531642913818359\n",
      "Best 3NN None accuracy: 0.8454404945904173\n",
      "Best 3NN None f1 score: 0.8397153892565546\n",
      "Best 3NN None classification time: 0.0007417201995849609\n",
      "Best 3NN None accuracy: 0.8408037094281299\n",
      "Best 3NN None f1 score: 0.838608157967654\n",
      "Best 3NN None classification time: 0.0006737709045410156\n",
      "Best 3NN None accuracy: 0.8516228748068007\n",
      "Best 3NN None f1 score: 0.851392350238363\n",
      "Best 3NN None classification time: 0.0006773471832275391\n",
      "Best 3NN None accuracy: 0.8423493044822257\n",
      "Best 3NN None f1 score: 0.8440990249699999\n",
      "Best 3NN None classification time: 0.0006530284881591797\n",
      "Best 3NN None accuracy: 0.8562596599690881\n",
      "Best 3NN None f1 score: 0.8520381888935162\n",
      "Best 3NN None classification time: 0.0007100105285644531\n",
      "Best 5NN None accuracy: 0.8518518518518519\n",
      "Best 5NN None f1 score: 0.8485210337015904\n",
      "Best 5NN None classification time: 0.0006303787231445312\n",
      "Best 5NN None accuracy: 0.8441358024691358\n",
      "Best 5NN None f1 score: 0.8425215211235134\n",
      "Best 5NN None classification time: 0.0006494522094726562\n",
      "Best 5NN None accuracy: 0.8179012345679012\n",
      "Best 5NN None f1 score: 0.8110352269562796\n",
      "Best 5NN None classification time: 0.0007059574127197266\n",
      "Best 5NN None accuracy: 0.8333333333333334\n",
      "Best 5NN None f1 score: 0.8300100853611129\n",
      "Best 5NN None classification time: 0.0007085800170898438\n",
      "Best 5NN None accuracy: 0.8364197530864198\n",
      "Best 5NN None f1 score: 0.836690327578879\n",
      "Best 5NN None classification time: 0.0006859302520751953\n",
      "Best 5NN None accuracy: 0.8454404945904173\n",
      "Best 5NN None f1 score: 0.8452845268542198\n",
      "Best 5NN None classification time: 0.0006780624389648438\n",
      "Best 5NN None accuracy: 0.8408037094281299\n",
      "Best 5NN None f1 score: 0.8379584003342796\n",
      "Best 5NN None classification time: 0.0006587505340576172\n",
      "Best 5NN None accuracy: 0.8469860896445132\n",
      "Best 5NN None f1 score: 0.8494758351101636\n",
      "Best 5NN None classification time: 0.0006692409515380859\n",
      "Best 5NN None accuracy: 0.8438948995363215\n",
      "Best 5NN None f1 score: 0.8452967562728494\n",
      "Best 5NN None classification time: 0.0006413459777832031\n",
      "Best 5NN None accuracy: 0.8438948995363215\n",
      "Best 5NN None f1 score: 0.8398140479061461\n",
      "Best 5NN None classification time: 0.0006837844848632812\n",
      "Best 7NN None accuracy: 0.8302469135802469\n",
      "Best 7NN None f1 score: 0.827544569280675\n",
      "Best 7NN None classification time: 0.0006680488586425781\n",
      "Best 7NN None accuracy: 0.8425925925925926\n",
      "Best 7NN None f1 score: 0.8443489804643948\n",
      "Best 7NN None classification time: 0.0006651878356933594\n",
      "Best 7NN None accuracy: 0.8287037037037037\n",
      "Best 7NN None f1 score: 0.8214749567834941\n",
      "Best 7NN None classification time: 0.0006606578826904297\n",
      "Best 7NN None accuracy: 0.816358024691358\n",
      "Best 7NN None f1 score: 0.8116493386904077\n",
      "Best 7NN None classification time: 0.0006513595581054688\n",
      "Best 7NN None accuracy: 0.8333333333333334\n",
      "Best 7NN None f1 score: 0.8332956087525277\n",
      "Best 7NN None classification time: 0.0007166862487792969\n",
      "Best 7NN None accuracy: 0.8346213292117465\n",
      "Best 7NN None f1 score: 0.8329928877142088\n",
      "Best 7NN None classification time: 0.0006475448608398438\n",
      "Best 7NN None accuracy: 0.8207109737248841\n",
      "Best 7NN None f1 score: 0.8153184384827331\n",
      "Best 7NN None classification time: 0.0006775856018066406\n",
      "Best 7NN None accuracy: 0.8469860896445132\n",
      "Best 7NN None f1 score: 0.8466927970238105\n",
      "Best 7NN None classification time: 0.0006589889526367188\n",
      "Best 7NN None accuracy: 0.8377125193199382\n",
      "Best 7NN None f1 score: 0.8358367606314246\n",
      "Best 7NN None classification time: 0.0006842613220214844\n",
      "Best 7NN None accuracy: 0.8268933539412674\n",
      "Best 7NN None f1 score: 0.8233327726158789\n",
      "Best 7NN None classification time: 0.0006937980651855469\n",
      "Best 9NN None accuracy: 0.8333333333333334\n",
      "Best 9NN None f1 score: 0.827165971634699\n",
      "Best 9NN None classification time: 0.0006415843963623047\n",
      "Best 9NN None accuracy: 0.8518518518518519\n",
      "Best 9NN None f1 score: 0.8519139514122913\n",
      "Best 9NN None classification time: 0.0006561279296875\n",
      "Best 9NN None accuracy: 0.8333333333333334\n",
      "Best 9NN None f1 score: 0.8241686116076918\n",
      "Best 9NN None classification time: 0.0006127357482910156\n",
      "Best 9NN None accuracy: 0.8117283950617284\n",
      "Best 9NN None f1 score: 0.8100646893773383\n",
      "Best 9NN None classification time: 0.0006368160247802734\n",
      "Best 9NN None accuracy: 0.8410493827160493\n",
      "Best 9NN None f1 score: 0.8406391136870175\n",
      "Best 9NN None classification time: 0.0006113052368164062\n",
      "Best 9NN None accuracy: 0.8408037094281299\n",
      "Best 9NN None f1 score: 0.8336930478587813\n",
      "Best 9NN None classification time: 0.0008175373077392578\n",
      "Best 9NN None accuracy: 0.8222565687789799\n",
      "Best 9NN None f1 score: 0.8176732592350441\n",
      "Best 9NN None classification time: 0.0007610321044921875\n",
      "Best 9NN None accuracy: 0.8531684698608965\n",
      "Best 9NN None f1 score: 0.851958868664965\n",
      "Best 9NN None classification time: 0.0007011890411376953\n",
      "Best 9NN None accuracy: 0.8361669242658424\n",
      "Best 9NN None f1 score: 0.8333052833500729\n",
      "Best 9NN None classification time: 0.0006375312805175781\n",
      "Best 9NN None accuracy: 0.8299845440494591\n",
      "Best 9NN None f1 score: 0.8245512378784827\n",
      "Best 9NN None classification time: 0.0006656646728515625\n",
      "Best 11NN None accuracy: 0.8271604938271605\n",
      "Best 11NN None f1 score: 0.8190956002839783\n",
      "Best 11NN None classification time: 0.000820159912109375\n",
      "Best 11NN None accuracy: 0.8425925925925926\n",
      "Best 11NN None f1 score: 0.8434747720737225\n",
      "Best 11NN None classification time: 0.0006685256958007812\n",
      "Best 11NN None accuracy: 0.8287037037037037\n",
      "Best 11NN None f1 score: 0.8194335449146334\n",
      "Best 11NN None classification time: 0.0006580352783203125\n",
      "Best 11NN None accuracy: 0.8117283950617284\n",
      "Best 11NN None f1 score: 0.8063536127966926\n",
      "Best 11NN None classification time: 0.0006387233734130859\n",
      "Best 11NN None accuracy: 0.8487654320987654\n",
      "Best 11NN None f1 score: 0.8464445591498021\n",
      "Best 11NN None classification time: 0.0006902217864990234\n",
      "Best 11NN None accuracy: 0.8268933539412674\n",
      "Best 11NN None f1 score: 0.822005177520097\n",
      "Best 11NN None classification time: 0.0007033348083496094\n",
      "Best 11NN None accuracy: 0.8083462132921174\n",
      "Best 11NN None f1 score: 0.8025735937666774\n",
      "Best 11NN None classification time: 0.0006489753723144531\n",
      "Best 11NN None accuracy: 0.8377125193199382\n",
      "Best 11NN None f1 score: 0.8350218926441771\n",
      "Best 11NN None classification time: 0.0006344318389892578\n",
      "Best 11NN None accuracy: 0.8469860896445132\n",
      "Best 11NN None f1 score: 0.8404321350765128\n",
      "Best 11NN None classification time: 0.0006186962127685547\n",
      "Best 11NN None accuracy: 0.8253477588871716\n",
      "Best 11NN None f1 score: 0.8232737870128622\n",
      "Best 11NN None classification time: 0.0006585121154785156\n",
      "Best 13NN None accuracy: 0.8256172839506173\n",
      "Best 13NN None f1 score: 0.8183489690399942\n",
      "Best 13NN None classification time: 0.0006301403045654297\n",
      "Best 13NN None accuracy: 0.8425925925925926\n",
      "Best 13NN None f1 score: 0.8450067225046372\n",
      "Best 13NN None classification time: 0.0006766319274902344\n",
      "Best 13NN None accuracy: 0.8302469135802469\n",
      "Best 13NN None f1 score: 0.8204194298072923\n",
      "Best 13NN None classification time: 0.0006616115570068359\n",
      "Best 13NN None accuracy: 0.8148148148148148\n",
      "Best 13NN None f1 score: 0.8085193580816273\n",
      "Best 13NN None classification time: 0.0006592273712158203\n",
      "Best 13NN None accuracy: 0.8410493827160493\n",
      "Best 13NN None f1 score: 0.8385204400932725\n",
      "Best 13NN None classification time: 0.0006725788116455078\n",
      "Best 13NN None accuracy: 0.8176197836166924\n",
      "Best 13NN None f1 score: 0.8094613070823079\n",
      "Best 13NN None classification time: 0.0006382465362548828\n",
      "Best 13NN None accuracy: 0.80370942812983\n",
      "Best 13NN None f1 score: 0.7970231989737618\n",
      "Best 13NN None classification time: 0.0007259845733642578\n",
      "Best 13NN None accuracy: 0.8238021638330757\n",
      "Best 13NN None f1 score: 0.8217134659859119\n",
      "Best 13NN None classification time: 0.0007050037384033203\n",
      "Best 13NN None accuracy: 0.8253477588871716\n",
      "Best 13NN None f1 score: 0.8177603546723248\n",
      "Best 13NN None classification time: 0.0006222724914550781\n",
      "Best 13NN None accuracy: 0.8299845440494591\n",
      "Best 13NN None f1 score: 0.8276963607307475\n",
      "Best 13NN None classification time: 0.0006351470947265625\n",
      "Best 15NN None accuracy: 0.8179012345679012\n",
      "Best 15NN None f1 score: 0.8101704272733078\n",
      "Best 15NN None classification time: 0.0006344318389892578\n",
      "Best 15NN None accuracy: 0.8580246913580247\n",
      "Best 15NN None f1 score: 0.8576950311643318\n",
      "Best 15NN None classification time: 0.0006489753723144531\n",
      "Best 15NN None accuracy: 0.8256172839506173\n",
      "Best 15NN None f1 score: 0.8156133704221157\n",
      "Best 15NN None classification time: 0.0006284713745117188\n",
      "Best 15NN None accuracy: 0.8148148148148148\n",
      "Best 15NN None f1 score: 0.8069472196113007\n",
      "Best 15NN None classification time: 0.0006256103515625\n",
      "Best 15NN None accuracy: 0.8472222222222222\n",
      "Best 15NN None f1 score: 0.8451948892123564\n",
      "Best 15NN None classification time: 0.0006318092346191406\n",
      "Best 15NN None accuracy: 0.8268933539412674\n",
      "Best 15NN None f1 score: 0.8210581757840685\n",
      "Best 15NN None classification time: 0.00067138671875\n",
      "Best 15NN None accuracy: 0.8068006182380216\n",
      "Best 15NN None f1 score: 0.8009925041362767\n",
      "Best 15NN None classification time: 0.0007293224334716797\n",
      "Best 15NN None accuracy: 0.8299845440494591\n",
      "Best 15NN None f1 score: 0.8269506871089986\n",
      "Best 15NN None classification time: 0.0007131099700927734\n",
      "Best 15NN None accuracy: 0.8330757341576507\n",
      "Best 15NN None f1 score: 0.826658210999493\n",
      "Best 15NN None classification time: 0.0007345676422119141\n",
      "Best 15NN None accuracy: 0.8268933539412674\n",
      "Best 15NN None f1 score: 0.8234936252246826\n",
      "Best 15NN None classification time: 0.0007302761077880859\n",
      "Best 17NN None accuracy: 0.8209876543209876\n",
      "Best 17NN None f1 score: 0.8167213042742939\n",
      "Best 17NN None classification time: 0.0007143020629882812\n",
      "Best 17NN None accuracy: 0.8503086419753086\n",
      "Best 17NN None f1 score: 0.8501738319392692\n",
      "Best 17NN None classification time: 0.0007259845733642578\n",
      "Best 17NN None accuracy: 0.8148148148148148\n",
      "Best 17NN None f1 score: 0.8052570549814885\n",
      "Best 17NN None classification time: 0.0007336139678955078\n",
      "Best 17NN None accuracy: 0.8209876543209876\n",
      "Best 17NN None f1 score: 0.8157886196857701\n",
      "Best 17NN None classification time: 0.0006456375122070312\n",
      "Best 17NN None accuracy: 0.8364197530864198\n",
      "Best 17NN None f1 score: 0.8342911877394635\n",
      "Best 17NN None classification time: 0.000614166259765625\n",
      "Best 17NN None accuracy: 0.8160741885625966\n",
      "Best 17NN None f1 score: 0.8085907870284227\n",
      "Best 17NN None classification time: 0.0006225109100341797\n",
      "Best 17NN None accuracy: 0.80370942812983\n",
      "Best 17NN None f1 score: 0.7971102644885798\n",
      "Best 17NN None classification time: 0.0006520748138427734\n",
      "Best 17NN None accuracy: 0.8299845440494591\n",
      "Best 17NN None f1 score: 0.8259142994373542\n",
      "Best 17NN None classification time: 0.0006959438323974609\n",
      "Best 17NN None accuracy: 0.8222565687789799\n",
      "Best 17NN None f1 score: 0.8174415327253755\n",
      "Best 17NN None classification time: 0.0006825923919677734\n",
      "Best 17NN None accuracy: 0.8268933539412674\n",
      "Best 17NN None f1 score: 0.8248860175523959\n",
      "Best 17NN None classification time: 0.0007593631744384766\n",
      "Best 19NN None accuracy: 0.816358024691358\n",
      "Best 19NN None f1 score: 0.812213541095368\n",
      "Best 19NN None classification time: 0.0006673336029052734\n",
      "Best 19NN None accuracy: 0.8472222222222222\n",
      "Best 19NN None f1 score: 0.8476419768004201\n",
      "Best 19NN None classification time: 0.0006513595581054688\n",
      "Best 19NN None accuracy: 0.8132716049382716\n",
      "Best 19NN None f1 score: 0.8036446440699767\n",
      "Best 19NN None classification time: 0.0006272792816162109\n",
      "Best 19NN None accuracy: 0.8209876543209876\n",
      "Best 19NN None f1 score: 0.8167042695132581\n",
      "Best 19NN None classification time: 0.0006740093231201172\n",
      "Best 19NN None accuracy: 0.8441358024691358\n",
      "Best 19NN None f1 score: 0.8423915791959189\n",
      "Best 19NN None classification time: 0.0006208419799804688\n",
      "Best 19NN None accuracy: 0.8176197836166924\n",
      "Best 19NN None f1 score: 0.8097301579108627\n",
      "Best 19NN None classification time: 0.0006377696990966797\n",
      "Best 19NN None accuracy: 0.7990726429675425\n",
      "Best 19NN None f1 score: 0.7922080278495945\n",
      "Best 19NN None classification time: 0.0006577968597412109\n",
      "Best 19NN None accuracy: 0.8129829984544049\n",
      "Best 19NN None f1 score: 0.8082464580020622\n",
      "Best 19NN None classification time: 0.0006372928619384766\n",
      "Best 19NN None accuracy: 0.8222565687789799\n",
      "Best 19NN None f1 score: 0.8159269652431865\n",
      "Best 19NN None classification time: 0.0006601810455322266\n",
      "Best 19NN None accuracy: 0.8253477588871716\n",
      "Best 19NN None f1 score: 0.8228409609423143\n",
      "Best 19NN None classification time: 0.0006556510925292969\n",
      "Best 21NN None accuracy: 0.8055555555555556\n",
      "Best 21NN None f1 score: 0.8012034243800494\n",
      "Best 21NN None classification time: 0.0006365776062011719\n",
      "Best 21NN None accuracy: 0.8395061728395061\n",
      "Best 21NN None f1 score: 0.8412686001238665\n",
      "Best 21NN None classification time: 0.0008797645568847656\n",
      "Best 21NN None accuracy: 0.8179012345679012\n",
      "Best 21NN None f1 score: 0.8073330430692615\n",
      "Best 21NN None classification time: 0.0006511211395263672\n",
      "Best 21NN None accuracy: 0.8117283950617284\n",
      "Best 21NN None f1 score: 0.8060303408618016\n",
      "Best 21NN None classification time: 0.0006730556488037109\n",
      "Best 21NN None accuracy: 0.8348765432098766\n",
      "Best 21NN None f1 score: 0.8328432487053177\n",
      "Best 21NN None classification time: 0.0006515979766845703\n",
      "Best 21NN None accuracy: 0.8222565687789799\n",
      "Best 21NN None f1 score: 0.8150541087521148\n",
      "Best 21NN None classification time: 0.0008492469787597656\n",
      "Best 21NN None accuracy: 0.80370942812983\n",
      "Best 21NN None f1 score: 0.7958028307309567\n",
      "Best 21NN None classification time: 0.0006277561187744141\n",
      "Best 21NN None accuracy: 0.8238021638330757\n",
      "Best 21NN None f1 score: 0.8185906632821984\n",
      "Best 21NN None classification time: 0.0006492137908935547\n",
      "Best 21NN None accuracy: 0.8238021638330757\n",
      "Best 21NN None f1 score: 0.8167272397124408\n",
      "Best 21NN None classification time: 0.0006196498870849609\n",
      "Best 21NN None accuracy: 0.8222565687789799\n",
      "Best 21NN None f1 score: 0.8175555751595723\n",
      "Best 21NN None classification time: 0.0006630420684814453\n",
      "Best 23NN None accuracy: 0.808641975308642\n",
      "Best 23NN None f1 score: 0.8035667646604326\n",
      "Best 23NN None classification time: 0.0006456375122070312\n",
      "Best 23NN None accuracy: 0.8472222222222222\n",
      "Best 23NN None f1 score: 0.8465444149017136\n",
      "Best 23NN None classification time: 0.0006382465362548828\n",
      "Best 23NN None accuracy: 0.8132716049382716\n",
      "Best 23NN None f1 score: 0.8030157209789751\n",
      "Best 23NN None classification time: 0.0006225109100341797\n",
      "Best 23NN None accuracy: 0.8070987654320988\n",
      "Best 23NN None f1 score: 0.8008243336328712\n",
      "Best 23NN None classification time: 0.0007312297821044922\n",
      "Best 23NN None accuracy: 0.8348765432098766\n",
      "Best 23NN None f1 score: 0.8318743464988524\n",
      "Best 23NN None classification time: 0.0006556510925292969\n",
      "Best 23NN None accuracy: 0.8145285935085008\n",
      "Best 23NN None f1 score: 0.8073828471539929\n",
      "Best 23NN None classification time: 0.0006673336029052734\n",
      "Best 23NN None accuracy: 0.7897990726429676\n",
      "Best 23NN None f1 score: 0.7831672151630095\n",
      "Best 23NN None classification time: 0.0006978511810302734\n",
      "Best 23NN None accuracy: 0.8160741885625966\n",
      "Best 23NN None f1 score: 0.8100550767730731\n",
      "Best 23NN None classification time: 0.0006616115570068359\n",
      "Best 23NN None accuracy: 0.8160741885625966\n",
      "Best 23NN None f1 score: 0.8101135537351452\n",
      "Best 23NN None classification time: 0.000652313232421875\n",
      "Best 23NN None accuracy: 0.8145285935085008\n",
      "Best 23NN None f1 score: 0.8086104504509155\n",
      "Best 23NN None classification time: 0.0006949901580810547\n",
      "Best 25NN None accuracy: 0.808641975308642\n",
      "Best 25NN None f1 score: 0.8029935237678979\n",
      "Best 25NN None classification time: 0.0006003379821777344\n",
      "Best 25NN None accuracy: 0.8472222222222222\n",
      "Best 25NN None f1 score: 0.8498329595170078\n",
      "Best 25NN None classification time: 0.0006210803985595703\n",
      "Best 25NN None accuracy: 0.8117283950617284\n",
      "Best 25NN None f1 score: 0.802499572224935\n",
      "Best 25NN None classification time: 0.0006046295166015625\n",
      "Best 25NN None accuracy: 0.8024691358024691\n",
      "Best 25NN None f1 score: 0.7953107551793224\n",
      "Best 25NN None classification time: 0.0006539821624755859\n",
      "Best 25NN None accuracy: 0.8302469135802469\n",
      "Best 25NN None f1 score: 0.8256118844967203\n",
      "Best 25NN None classification time: 0.0006277561187744141\n",
      "Best 25NN None accuracy: 0.8129829984544049\n",
      "Best 25NN None f1 score: 0.8061846222072027\n",
      "Best 25NN None classification time: 0.0006380081176757812\n",
      "Best 25NN None accuracy: 0.7897990726429676\n",
      "Best 25NN None f1 score: 0.7853601655827683\n",
      "Best 25NN None classification time: 0.0006356239318847656\n",
      "Best 25NN None accuracy: 0.8268933539412674\n",
      "Best 25NN None f1 score: 0.8200890639415229\n",
      "Best 25NN None classification time: 0.0007634162902832031\n",
      "Best 25NN None accuracy: 0.8160741885625966\n",
      "Best 25NN None f1 score: 0.8100105045350002\n",
      "Best 25NN None classification time: 0.0007238388061523438\n",
      "Best 25NN None accuracy: 0.8129829984544049\n",
      "Best 25NN None f1 score: 0.8089359991694266\n",
      "Best 25NN None classification time: 0.0006465911865234375\n",
      "Best 27NN None accuracy: 0.7978395061728395\n",
      "Best 27NN None f1 score: 0.7901504340072832\n",
      "Best 27NN None classification time: 0.0006365776062011719\n",
      "Best 27NN None accuracy: 0.8348765432098766\n",
      "Best 27NN None f1 score: 0.8370759761580165\n",
      "Best 27NN None classification time: 0.0006906986236572266\n",
      "Best 27NN None accuracy: 0.8132716049382716\n",
      "Best 27NN None f1 score: 0.8032922719826127\n",
      "Best 27NN None classification time: 0.0006055831909179688\n",
      "Best 27NN None accuracy: 0.7993827160493827\n",
      "Best 27NN None f1 score: 0.7910913431191465\n",
      "Best 27NN None classification time: 0.0006117820739746094\n",
      "Best 27NN None accuracy: 0.8287037037037037\n",
      "Best 27NN None f1 score: 0.8237981174592406\n",
      "Best 27NN None classification time: 0.0006053447723388672\n",
      "Best 27NN None accuracy: 0.8068006182380216\n",
      "Best 27NN None f1 score: 0.8007247331809925\n",
      "Best 27NN None classification time: 0.0007073879241943359\n",
      "Best 27NN None accuracy: 0.7851622874806801\n",
      "Best 27NN None f1 score: 0.7775912132978874\n",
      "Best 27NN None classification time: 0.0006506443023681641\n",
      "Best 27NN None accuracy: 0.8222565687789799\n",
      "Best 27NN None f1 score: 0.8157734914620662\n",
      "Best 27NN None classification time: 0.0006096363067626953\n",
      "Best 27NN None accuracy: 0.8238021638330757\n",
      "Best 27NN None f1 score: 0.8166613572716277\n",
      "Best 27NN None classification time: 0.0006155967712402344\n",
      "Best 27NN None accuracy: 0.8129829984544049\n",
      "Best 27NN None f1 score: 0.8082071220612937\n",
      "Best 27NN None classification time: 0.0006535053253173828\n",
      "Best 29NN None accuracy: 0.8055555555555556\n",
      "Best 29NN None f1 score: 0.7991163609376363\n",
      "Best 29NN None classification time: 0.0006241798400878906\n",
      "Best 29NN None accuracy: 0.8333333333333334\n",
      "Best 29NN None f1 score: 0.8350704344701043\n",
      "Best 29NN None classification time: 0.0006475448608398438\n",
      "Best 29NN None accuracy: 0.8117283950617284\n",
      "Best 29NN None f1 score: 0.8004735425812278\n",
      "Best 29NN None classification time: 0.0006310939788818359\n",
      "Best 29NN None accuracy: 0.7993827160493827\n",
      "Best 29NN None f1 score: 0.7920518807610843\n",
      "Best 29NN None classification time: 0.0006070137023925781\n",
      "Best 29NN None accuracy: 0.8317901234567902\n",
      "Best 29NN None f1 score: 0.8269334873145908\n",
      "Best 29NN None classification time: 0.0006077289581298828\n",
      "Best 29NN None accuracy: 0.8068006182380216\n",
      "Best 29NN None f1 score: 0.8004739983563186\n",
      "Best 29NN None classification time: 0.0006201267242431641\n",
      "Best 29NN None accuracy: 0.7820710973724884\n",
      "Best 29NN None f1 score: 0.7761625307399954\n",
      "Best 29NN None classification time: 0.0007271766662597656\n",
      "Best 29NN None accuracy: 0.8207109737248841\n",
      "Best 29NN None f1 score: 0.8151106960011069\n",
      "Best 29NN None classification time: 0.0006093978881835938\n",
      "Best 29NN None accuracy: 0.8238021638330757\n",
      "Best 29NN None f1 score: 0.8155683103366211\n",
      "Best 29NN None classification time: 0.0006310939788818359\n",
      "Best 29NN None accuracy: 0.8160741885625966\n",
      "Best 29NN None f1 score: 0.8124289735946855\n",
      "Best 29NN None classification time: 0.0006682872772216797\n",
      "Best 31NN None accuracy: 0.808641975308642\n",
      "Best 31NN None f1 score: 0.8034797130731558\n",
      "Best 31NN None classification time: 0.0006246566772460938\n",
      "Best 31NN None accuracy: 0.8287037037037037\n",
      "Best 31NN None f1 score: 0.8287835811823818\n",
      "Best 31NN None classification time: 0.0006248950958251953\n",
      "Best 31NN None accuracy: 0.8101851851851852\n",
      "Best 31NN None f1 score: 0.7977314466399191\n",
      "Best 31NN None classification time: 0.0006263256072998047\n",
      "Best 31NN None accuracy: 0.7947530864197531\n",
      "Best 31NN None f1 score: 0.7866189910001019\n",
      "Best 31NN None classification time: 0.0006392002105712891\n",
      "Best 31NN None accuracy: 0.8333333333333334\n",
      "Best 31NN None f1 score: 0.8293152214270604\n",
      "Best 31NN None classification time: 0.0007107257843017578\n",
      "Best 31NN None accuracy: 0.7975270479134466\n",
      "Best 31NN None f1 score: 0.7909381291403763\n",
      "Best 31NN None classification time: 0.0006504058837890625\n",
      "Best 31NN None accuracy: 0.7836166924265843\n",
      "Best 31NN None f1 score: 0.7764952799067147\n",
      "Best 31NN None classification time: 0.0006282329559326172\n",
      "Best 31NN None accuracy: 0.8238021638330757\n",
      "Best 31NN None f1 score: 0.8182093978279265\n",
      "Best 31NN None classification time: 0.0006079673767089844\n",
      "Best 31NN None accuracy: 0.8253477588871716\n",
      "Best 31NN None f1 score: 0.8202981430089263\n",
      "Best 31NN None classification time: 0.0006213188171386719\n",
      "Best 31NN None accuracy: 0.8114374034003091\n",
      "Best 31NN None f1 score: 0.8064389746047357\n",
      "Best 31NN None classification time: 0.0006213188171386719\n",
      "Best 3NN None accuracy: 0.8595679012345679\n",
      "Best 3NN None f1 score: 0.8592745677521894\n",
      "Best 3NN None classification time: 0.0006775856018066406\n",
      "{'SVM': {'None': [0.8626543209876543, 0.8580246913580247, 0.8441358024691358, 0.8302469135802469, 0.845679012345679, 0.848531684698609, 0.8438948995363215, 0.874806800618238, 0.8531684698608965, 0.8423493044822257]}, 'RF': {'None': [0.8194444444444444, 0.8317901234567902, 0.8101851851851852, 0.7962962962962963, 0.8225308641975309, 0.8068006182380216, 0.8129829984544049, 0.8222565687789799, 0.8253477588871716, 0.8098918083462133]}, 'KNN': {'None': [0.8580246913580247, 0.8595679012345679, 0.816358024691358, 0.8364197530864198, 0.8472222222222222, 0.8315301391035549, 0.8423493044822257, 0.8408037094281299, 0.8377125193199382, 0.8578052550231839, 0.8487654320987654, 0.8503086419753086, 0.8194444444444444, 0.8333333333333334, 0.8348765432098766, 0.8454404945904173, 0.8408037094281299, 0.8516228748068007, 0.8423493044822257, 0.8562596599690881, 0.8518518518518519, 0.8441358024691358, 0.8179012345679012, 0.8333333333333334, 0.8364197530864198, 0.8454404945904173, 0.8408037094281299, 0.8469860896445132, 0.8438948995363215, 0.8438948995363215, 0.8302469135802469, 0.8425925925925926, 0.8287037037037037, 0.816358024691358, 0.8333333333333334, 0.8346213292117465, 0.8207109737248841, 0.8469860896445132, 0.8377125193199382, 0.8268933539412674, 0.8333333333333334, 0.8518518518518519, 0.8333333333333334, 0.8117283950617284, 0.8410493827160493, 0.8408037094281299, 0.8222565687789799, 0.8531684698608965, 0.8361669242658424, 0.8299845440494591, 0.8271604938271605, 0.8425925925925926, 0.8287037037037037, 0.8117283950617284, 0.8487654320987654, 0.8268933539412674, 0.8083462132921174, 0.8377125193199382, 0.8469860896445132, 0.8253477588871716, 0.8256172839506173, 0.8425925925925926, 0.8302469135802469, 0.8148148148148148, 0.8410493827160493, 0.8176197836166924, 0.80370942812983, 0.8238021638330757, 0.8253477588871716, 0.8299845440494591, 0.8179012345679012, 0.8580246913580247, 0.8256172839506173, 0.8148148148148148, 0.8472222222222222, 0.8268933539412674, 0.8068006182380216, 0.8299845440494591, 0.8330757341576507, 0.8268933539412674, 0.8209876543209876, 0.8503086419753086, 0.8148148148148148, 0.8209876543209876, 0.8364197530864198, 0.8160741885625966, 0.80370942812983, 0.8299845440494591, 0.8222565687789799, 0.8268933539412674, 0.816358024691358, 0.8472222222222222, 0.8132716049382716, 0.8209876543209876, 0.8441358024691358, 0.8176197836166924, 0.7990726429675425, 0.8129829984544049, 0.8222565687789799, 0.8253477588871716, 0.8055555555555556, 0.8395061728395061, 0.8179012345679012, 0.8117283950617284, 0.8348765432098766, 0.8222565687789799, 0.80370942812983, 0.8238021638330757, 0.8238021638330757, 0.8222565687789799, 0.808641975308642, 0.8472222222222222, 0.8132716049382716, 0.8070987654320988, 0.8348765432098766, 0.8145285935085008, 0.7897990726429676, 0.8160741885625966, 0.8160741885625966, 0.8145285935085008, 0.808641975308642, 0.8472222222222222, 0.8117283950617284, 0.8024691358024691, 0.8302469135802469, 0.8129829984544049, 0.7897990726429676, 0.8268933539412674, 0.8160741885625966, 0.8129829984544049, 0.7978395061728395, 0.8348765432098766, 0.8132716049382716, 0.7993827160493827, 0.8287037037037037, 0.8068006182380216, 0.7851622874806801, 0.8222565687789799, 0.8238021638330757, 0.8129829984544049, 0.8055555555555556, 0.8333333333333334, 0.8117283950617284, 0.7993827160493827, 0.8317901234567902, 0.8068006182380216, 0.7820710973724884, 0.8207109737248841, 0.8238021638330757, 0.8160741885625966, 0.808641975308642, 0.8287037037037037, 0.8101851851851852, 0.7947530864197531, 0.8333333333333334, 0.7975270479134466, 0.7836166924265843, 0.8238021638330757, 0.8253477588871716, 0.8114374034003091]}}\n",
      "{'SVM': {'None': [0.8616103834590064, 0.859822498593685, 0.8394558048813368, 0.8285276305013146, 0.8493069047716347, 0.8487342117670412, 0.840786569338059, 0.8753099275992996, 0.8491596960100324, 0.8387804429739912]}, 'RF': {'None': [0.8158539990707275, 0.8325888481066889, 0.8014426589153959, 0.7948863773172722, 0.8242358639311326, 0.8051574663428708, 0.8095856926045794, 0.8186035990482835, 0.8190598973278465, 0.805399285150064]}, 'KNN': {'None': [0.8549660747273289, 0.8592745677521894, 0.8136560014175886, 0.8393813589709112, 0.8477814819274289, 0.8242753984732349, 0.8377697159107104, 0.8425079172068924, 0.8413379721482701, 0.8589921992006619, 0.8440873015873015, 0.8466022464315396, 0.812574308613756, 0.8313957478576074, 0.8350098481733603, 0.8397153892565546, 0.838608157967654, 0.851392350238363, 0.8440990249699999, 0.8520381888935162, 0.8485210337015904, 0.8425215211235134, 0.8110352269562796, 0.8300100853611129, 0.836690327578879, 0.8452845268542198, 0.8379584003342796, 0.8494758351101636, 0.8452967562728494, 0.8398140479061461, 0.827544569280675, 0.8443489804643948, 0.8214749567834941, 0.8116493386904077, 0.8332956087525277, 0.8329928877142088, 0.8153184384827331, 0.8466927970238105, 0.8358367606314246, 0.8233327726158789, 0.827165971634699, 0.8519139514122913, 0.8241686116076918, 0.8100646893773383, 0.8406391136870175, 0.8336930478587813, 0.8176732592350441, 0.851958868664965, 0.8333052833500729, 0.8245512378784827, 0.8190956002839783, 0.8434747720737225, 0.8194335449146334, 0.8063536127966926, 0.8464445591498021, 0.822005177520097, 0.8025735937666774, 0.8350218926441771, 0.8404321350765128, 0.8232737870128622, 0.8183489690399942, 0.8450067225046372, 0.8204194298072923, 0.8085193580816273, 0.8385204400932725, 0.8094613070823079, 0.7970231989737618, 0.8217134659859119, 0.8177603546723248, 0.8276963607307475, 0.8101704272733078, 0.8576950311643318, 0.8156133704221157, 0.8069472196113007, 0.8451948892123564, 0.8210581757840685, 0.8009925041362767, 0.8269506871089986, 0.826658210999493, 0.8234936252246826, 0.8167213042742939, 0.8501738319392692, 0.8052570549814885, 0.8157886196857701, 0.8342911877394635, 0.8085907870284227, 0.7971102644885798, 0.8259142994373542, 0.8174415327253755, 0.8248860175523959, 0.812213541095368, 0.8476419768004201, 0.8036446440699767, 0.8167042695132581, 0.8423915791959189, 0.8097301579108627, 0.7922080278495945, 0.8082464580020622, 0.8159269652431865, 0.8228409609423143, 0.8012034243800494, 0.8412686001238665, 0.8073330430692615, 0.8060303408618016, 0.8328432487053177, 0.8150541087521148, 0.7958028307309567, 0.8185906632821984, 0.8167272397124408, 0.8175555751595723, 0.8035667646604326, 0.8465444149017136, 0.8030157209789751, 0.8008243336328712, 0.8318743464988524, 0.8073828471539929, 0.7831672151630095, 0.8100550767730731, 0.8101135537351452, 0.8086104504509155, 0.8029935237678979, 0.8498329595170078, 0.802499572224935, 0.7953107551793224, 0.8256118844967203, 0.8061846222072027, 0.7853601655827683, 0.8200890639415229, 0.8100105045350002, 0.8089359991694266, 0.7901504340072832, 0.8370759761580165, 0.8032922719826127, 0.7910913431191465, 0.8237981174592406, 0.8007247331809925, 0.7775912132978874, 0.8157734914620662, 0.8166613572716277, 0.8082071220612937, 0.7991163609376363, 0.8350704344701043, 0.8004735425812278, 0.7920518807610843, 0.8269334873145908, 0.8004739983563186, 0.7761625307399954, 0.8151106960011069, 0.8155683103366211, 0.8124289735946855, 0.8034797130731558, 0.8287835811823818, 0.7977314466399191, 0.7866189910001019, 0.8293152214270604, 0.7909381291403763, 0.7764952799067147, 0.8182093978279265, 0.8202981430089263, 0.8064389746047357]}}\n",
      "{'SVM': {'None': [array([[ 94,  15,   5],\n",
      "       [  2, 294,  29],\n",
      "       [  4,  34, 171]]), array([[103,   7,   4],\n",
      "       [  7, 284,  34],\n",
      "       [  7,  33, 169]]), array([[ 93,  18,   3],\n",
      "       [  7, 284,  34],\n",
      "       [  8,  31, 170]]), array([[ 95,  14,   5],\n",
      "       [  9, 283,  33],\n",
      "       [  6,  43, 160]]), array([[104,   7,   3],\n",
      "       [  7, 288,  30],\n",
      "       [  6,  47, 156]]), array([[ 99,  11,   4],\n",
      "       [  5, 289,  30],\n",
      "       [  7,  41, 161]]), array([[ 98,   9,   7],\n",
      "       [  6, 289,  29],\n",
      "       [  9,  41, 159]]), array([[103,   5,   5],\n",
      "       [  5, 292,  27],\n",
      "       [  7,  32, 171]]), array([[ 98,  11,   4],\n",
      "       [  4, 290,  31],\n",
      "       [ 12,  33, 164]]), array([[ 93,  11,   9],\n",
      "       [  4, 294,  27],\n",
      "       [  6,  45, 158]])]}, 'RF': {'None': [array([[ 93,  18,   3],\n",
      "       [  5, 296,  24],\n",
      "       [  7,  60, 142]]), array([[100,  12,   2],\n",
      "       [  5, 292,  28],\n",
      "       [  8,  54, 147]]), array([[ 87,  24,   3],\n",
      "       [  5, 291,  29],\n",
      "       [ 10,  52, 147]]), array([[ 95,  16,   3],\n",
      "       [  8, 288,  29],\n",
      "       [  7,  69, 133]]), array([[103,   7,   4],\n",
      "       [  7, 292,  26],\n",
      "       [  7,  64, 138]]), array([[ 95,  11,   8],\n",
      "       [  3, 289,  32],\n",
      "       [  9,  62, 138]]), array([[ 95,  14,   5],\n",
      "       [  6, 290,  28],\n",
      "       [  9,  59, 141]]), array([[ 94,  14,   5],\n",
      "       [  5, 292,  27],\n",
      "       [  9,  55, 146]]), array([[ 96,  11,   6],\n",
      "       [  3, 294,  28],\n",
      "       [ 14,  51, 144]]), array([[ 91,  14,   8],\n",
      "       [  3, 297,  25],\n",
      "       [  7,  66, 136]])]}, 'KNN': {'None': [array([[ 95,  14,   5],\n",
      "       [  9, 291,  25],\n",
      "       [  3,  36, 170]]), array([[ 97,   9,   8],\n",
      "       [  7, 286,  32],\n",
      "       [  3,  32, 174]]), array([[ 93,  14,   7],\n",
      "       [ 11, 278,  36],\n",
      "       [  6,  45, 158]]), array([[ 98,  10,   6],\n",
      "       [  8, 287,  30],\n",
      "       [  2,  50, 157]]), array([[104,   4,   6],\n",
      "       [ 16, 276,  33],\n",
      "       [  5,  35, 169]]), array([[ 95,  12,   7],\n",
      "       [ 10, 288,  26],\n",
      "       [ 10,  44, 155]]), array([[ 93,  15,   6],\n",
      "       [ 10, 284,  30],\n",
      "       [  5,  36, 168]]), array([[ 99,   4,  10],\n",
      "       [  7, 280,  37],\n",
      "       [  6,  39, 165]]), array([[ 98,  11,   4],\n",
      "       [  6, 277,  42],\n",
      "       [  6,  36, 167]]), array([[ 95,  12,   6],\n",
      "       [  5, 289,  31],\n",
      "       [  2,  36, 171]]), array([[ 95,  14,   5],\n",
      "       [ 10, 292,  23],\n",
      "       [  5,  41, 163]]), array([[100,   9,   5],\n",
      "       [  8, 289,  28],\n",
      "       [ 10,  37, 162]]), array([[ 96,  12,   6],\n",
      "       [ 14, 278,  33],\n",
      "       [ 12,  40, 157]]), array([[ 98,  14,   2],\n",
      "       [  7, 293,  25],\n",
      "       [  8,  52, 149]]), array([[104,   6,   4],\n",
      "       [ 13, 282,  30],\n",
      "       [  8,  46, 155]]), array([[ 99,  11,   4],\n",
      "       [  6, 295,  23],\n",
      "       [ 12,  44, 153]]), array([[ 98,  12,   4],\n",
      "       [  8, 287,  29],\n",
      "       [  8,  42, 159]]), array([[101,   6,   6],\n",
      "       [  6, 290,  28],\n",
      "       [  8,  42, 160]]), array([[ 98,  12,   3],\n",
      "       [  2, 286,  37],\n",
      "       [  9,  39, 161]]), array([[ 95,  12,   6],\n",
      "       [  5, 295,  25],\n",
      "       [  7,  38, 164]]), array([[ 94,  14,   6],\n",
      "       [  4, 298,  23],\n",
      "       [  5,  44, 160]]), array([[102,   9,   3],\n",
      "       [  9, 290,  26],\n",
      "       [  9,  45, 155]]), array([[ 91,  19,   4],\n",
      "       [ 12, 283,  30],\n",
      "       [  8,  45, 156]]), array([[ 93,  19,   2],\n",
      "       [  7, 296,  22],\n",
      "       [  5,  53, 151]]), array([[105,   8,   1],\n",
      "       [  9, 292,  24],\n",
      "       [  9,  55, 145]]), array([[102,  10,   2],\n",
      "       [  6, 295,  23],\n",
      "       [  8,  51, 150]]), array([[ 97,  12,   5],\n",
      "       [ 10, 292,  22],\n",
      "       [  5,  49, 155]]), array([[101,   6,   6],\n",
      "       [  3, 292,  29],\n",
      "       [  7,  48, 155]]), array([[100,  12,   1],\n",
      "       [  3, 288,  34],\n",
      "       [ 10,  41, 158]]), array([[ 94,  13,   6],\n",
      "       [  3, 297,  25],\n",
      "       [  8,  46, 155]]), array([[ 91,  19,   4],\n",
      "       [  6, 292,  27],\n",
      "       [  4,  50, 155]]), array([[103,   9,   2],\n",
      "       [  8, 289,  28],\n",
      "       [  7,  48, 154]]), array([[ 92,  19,   3],\n",
      "       [  8, 296,  21],\n",
      "       [  8,  52, 149]]), array([[ 93,  16,   5],\n",
      "       [  8, 292,  25],\n",
      "       [  7,  58, 144]]), array([[103,   8,   3],\n",
      "       [  9, 291,  25],\n",
      "       [  8,  55, 146]]), array([[ 99,  10,   5],\n",
      "       [  6, 296,  22],\n",
      "       [  7,  57, 145]]), array([[ 98,  12,   4],\n",
      "       [  7, 295,  22],\n",
      "       [ 11,  60, 138]]), array([[ 99,   9,   5],\n",
      "       [  3, 299,  22],\n",
      "       [  7,  53, 150]]), array([[ 97,  13,   3],\n",
      "       [  1, 295,  29],\n",
      "       [ 11,  48, 150]]), array([[ 92,  15,   6],\n",
      "       [  3, 293,  29],\n",
      "       [  8,  51, 150]]), array([[ 89,  22,   3],\n",
      "       [  6, 297,  22],\n",
      "       [  5,  50, 154]]), array([[104,   9,   1],\n",
      "       [ 11, 290,  24],\n",
      "       [  7,  44, 158]]), array([[ 91,  19,   4],\n",
      "       [  7, 298,  20],\n",
      "       [  9,  49, 151]]), array([[ 94,  16,   4],\n",
      "       [  8, 291,  26],\n",
      "       [  5,  63, 141]]), array([[106,   5,   3],\n",
      "       [  8, 296,  21],\n",
      "       [  9,  57, 143]]), array([[101,   8,   5],\n",
      "       [  5, 300,  19],\n",
      "       [ 14,  52, 143]]), array([[ 95,  14,   5],\n",
      "       [  7, 294,  23],\n",
      "       [  8,  58, 143]]), array([[ 97,  12,   4],\n",
      "       [  2, 300,  22],\n",
      "       [  7,  48, 155]]), array([[ 95,  15,   3],\n",
      "       [  1, 294,  30],\n",
      "       [ 11,  46, 152]]), array([[ 92,  16,   5],\n",
      "       [  3, 297,  25],\n",
      "       [  9,  52, 148]]), array([[ 88,  23,   3],\n",
      "       [  7, 297,  21],\n",
      "       [  6,  52, 151]]), array([[104,   8,   2],\n",
      "       [ 10, 290,  25],\n",
      "       [  7,  50, 152]]), array([[ 87,  21,   6],\n",
      "       [  4, 299,  22],\n",
      "       [  7,  51, 151]]), array([[ 91,  20,   3],\n",
      "       [  7, 294,  24],\n",
      "       [  7,  61, 141]]), array([[107,   5,   2],\n",
      "       [  8, 299,  18],\n",
      "       [ 11,  54, 144]]), array([[ 99,  11,   4],\n",
      "       [  5, 296,  23],\n",
      "       [ 12,  57, 140]]), array([[ 94,  17,   3],\n",
      "       [  6, 292,  26],\n",
      "       [ 11,  61, 137]]), array([[ 95,  13,   5],\n",
      "       [  2, 299,  23],\n",
      "       [  8,  54, 148]]), array([[ 95,  15,   3],\n",
      "       [  2, 297,  26],\n",
      "       [ 13,  40, 156]]), array([[ 92,  16,   5],\n",
      "       [  1, 297,  27],\n",
      "       [  7,  57, 145]]), array([[ 90,  21,   3],\n",
      "       [  6, 299,  20],\n",
      "       [  7,  56, 146]]), array([[103,   8,   3],\n",
      "       [  6, 294,  25],\n",
      "       [  6,  54, 149]]), array([[ 87,  22,   5],\n",
      "       [  4, 302,  19],\n",
      "       [  7,  53, 149]]), array([[ 91,  19,   4],\n",
      "       [  7, 296,  22],\n",
      "       [  7,  61, 141]]), array([[105,   4,   5],\n",
      "       [  7, 299,  19],\n",
      "       [ 10,  58, 141]]), array([[ 97,  12,   5],\n",
      "       [  5, 297,  22],\n",
      "       [ 14,  60, 135]]), array([[ 93,  18,   3],\n",
      "       [  6, 293,  25],\n",
      "       [ 11,  64, 134]]), array([[ 96,  12,   5],\n",
      "       [  4, 296,  24],\n",
      "       [  8,  61, 141]]), array([[ 95,  14,   4],\n",
      "       [  3, 295,  27],\n",
      "       [ 15,  50, 144]]), array([[ 91,  16,   6],\n",
      "       [  1, 297,  27],\n",
      "       [  6,  54, 149]]), array([[ 89,  22,   3],\n",
      "       [  5, 301,  19],\n",
      "       [  7,  62, 140]]), array([[103,   8,   3],\n",
      "       [  7, 300,  18],\n",
      "       [  6,  50, 153]]), array([[ 88,  22,   4],\n",
      "       [  2, 305,  18],\n",
      "       [  9,  58, 142]]), array([[ 90,  20,   4],\n",
      "       [  7, 299,  19],\n",
      "       [  7,  63, 139]]), array([[104,   7,   3],\n",
      "       [  7, 300,  18],\n",
      "       [  9,  55, 145]]), array([[ 97,  13,   4],\n",
      "       [  4, 300,  20],\n",
      "       [ 11,  60, 138]]), array([[ 96,  16,   2],\n",
      "       [  6, 296,  22],\n",
      "       [ 11,  68, 130]]), array([[ 97,  10,   6],\n",
      "       [  3, 299,  22],\n",
      "       [  9,  60, 141]]), array([[ 95,  14,   4],\n",
      "       [  2, 298,  25],\n",
      "       [ 13,  50, 146]]), array([[ 90,  16,   7],\n",
      "       [  1, 298,  26],\n",
      "       [  6,  56, 147]]), array([[ 92,  19,   3],\n",
      "       [  5, 299,  21],\n",
      "       [  6,  62, 141]]), array([[103,   8,   3],\n",
      "       [  8, 298,  19],\n",
      "       [  6,  53, 150]]), array([[ 88,  22,   4],\n",
      "       [  2, 304,  19],\n",
      "       [  9,  64, 136]]), array([[ 93,  19,   2],\n",
      "       [  6, 303,  16],\n",
      "       [  6,  67, 136]]), array([[103,   9,   2],\n",
      "       [  5, 301,  19],\n",
      "       [ 10,  61, 138]]), array([[ 95,  15,   4],\n",
      "       [  4, 299,  21],\n",
      "       [ 12,  63, 134]]), array([[ 94,  17,   3],\n",
      "       [  5, 296,  23],\n",
      "       [ 11,  68, 130]]), array([[ 97,  11,   5],\n",
      "       [  5, 299,  20],\n",
      "       [  9,  60, 141]]), array([[ 95,  13,   5],\n",
      "       [  1, 295,  29],\n",
      "       [ 13,  54, 142]]), array([[ 92,  15,   6],\n",
      "       [  1, 299,  25],\n",
      "       [  6,  59, 144]]), array([[ 93,  18,   3],\n",
      "       [  5, 298,  22],\n",
      "       [  7,  64, 138]]), array([[104,   8,   2],\n",
      "       [  9, 297,  19],\n",
      "       [  6,  55, 148]]), array([[ 87,  22,   5],\n",
      "       [  2, 304,  19],\n",
      "       [  8,  65, 136]]), array([[ 94,  18,   2],\n",
      "       [  6, 302,  17],\n",
      "       [  6,  67, 136]]), array([[104,   8,   2],\n",
      "       [  5, 304,  16],\n",
      "       [  9,  61, 139]]), array([[ 94,  16,   4],\n",
      "       [  4, 301,  19],\n",
      "       [ 11,  64, 134]]), array([[ 93,  18,   3],\n",
      "       [  5, 295,  24],\n",
      "       [ 11,  69, 129]]), array([[ 94,  15,   4],\n",
      "       [  6, 295,  23],\n",
      "       [  9,  64, 137]]), array([[ 93,  14,   6],\n",
      "       [  1, 295,  29],\n",
      "       [ 13,  52, 144]]), array([[ 92,  14,   7],\n",
      "       [  1, 300,  24],\n",
      "       [  6,  61, 142]]), array([[ 93,  19,   2],\n",
      "       [  6, 295,  24],\n",
      "       [  8,  67, 134]]), array([[104,   8,   2],\n",
      "       [  8, 295,  22],\n",
      "       [  6,  58, 145]]), array([[ 87,  22,   5],\n",
      "       [  2, 304,  19],\n",
      "       [  9,  61, 139]]), array([[ 94,  17,   3],\n",
      "       [  5, 304,  16],\n",
      "       [  7,  74, 128]]), array([[104,   7,   3],\n",
      "       [  4, 303,  18],\n",
      "       [ 10,  65, 134]]), array([[ 94,  15,   5],\n",
      "       [  3, 300,  21],\n",
      "       [ 11,  60, 138]]), array([[ 93,  18,   3],\n",
      "       [  5, 298,  21],\n",
      "       [ 11,  69, 129]]), array([[ 94,  15,   4],\n",
      "       [  5, 298,  21],\n",
      "       [  9,  60, 141]]), array([[ 93,  15,   5],\n",
      "       [  1, 298,  26],\n",
      "       [ 13,  54, 142]]), array([[ 90,  15,   8],\n",
      "       [  1, 302,  22],\n",
      "       [  6,  63, 140]]), array([[ 92,  19,   3],\n",
      "       [  5, 299,  21],\n",
      "       [  7,  69, 133]]), array([[103,  10,   1],\n",
      "       [  9, 297,  19],\n",
      "       [  7,  53, 149]]), array([[ 87,  22,   5],\n",
      "       [  3, 304,  18],\n",
      "       [  8,  65, 136]]), array([[ 92,  20,   2],\n",
      "       [  5, 303,  17],\n",
      "       [  7,  74, 128]]), array([[102,  10,   2],\n",
      "       [  4, 304,  17],\n",
      "       [ 10,  64, 135]]), array([[ 95,  14,   5],\n",
      "       [  3, 299,  22],\n",
      "       [ 12,  64, 133]]), array([[ 94,  17,   3],\n",
      "       [  5, 295,  24],\n",
      "       [ 11,  76, 122]]), array([[ 94,  16,   3],\n",
      "       [  6, 297,  21],\n",
      "       [ 10,  63, 137]]), array([[ 94,  14,   5],\n",
      "       [  2, 295,  28],\n",
      "       [ 13,  57, 139]]), array([[ 90,  15,   8],\n",
      "       [  1, 303,  21],\n",
      "       [  7,  68, 134]]), array([[ 91,  20,   3],\n",
      "       [  6, 297,  22],\n",
      "       [  7,  66, 136]]), array([[104,  10,   0],\n",
      "       [  5, 300,  20],\n",
      "       [  6,  58, 145]]), array([[ 88,  22,   4],\n",
      "       [  4, 302,  19],\n",
      "       [  8,  65, 136]]), array([[ 91,  19,   4],\n",
      "       [  5, 302,  18],\n",
      "       [  7,  75, 127]]), array([[100,  11,   3],\n",
      "       [  4, 305,  16],\n",
      "       [ 10,  66, 133]]), array([[ 95,  15,   4],\n",
      "       [  3, 301,  20],\n",
      "       [ 11,  68, 130]]), array([[ 95,  16,   3],\n",
      "       [  5, 291,  28],\n",
      "       [ 11,  73, 125]]), array([[ 95,  16,   2],\n",
      "       [  6, 302,  16],\n",
      "       [ 10,  62, 138]]), array([[ 94,  15,   4],\n",
      "       [  2, 296,  27],\n",
      "       [ 13,  58, 138]]), array([[ 91,  16,   6],\n",
      "       [  1, 301,  23],\n",
      "       [  7,  68, 134]]), array([[ 88,  24,   2],\n",
      "       [  6, 298,  21],\n",
      "       [  7,  71, 131]]), array([[104,   9,   1],\n",
      "       [  5, 298,  22],\n",
      "       [  7,  63, 139]]), array([[ 87,  23,   4],\n",
      "       [  3, 304,  18],\n",
      "       [  8,  65, 136]]), array([[ 91,  19,   4],\n",
      "       [  6, 303,  16],\n",
      "       [  7,  78, 124]]), array([[100,  11,   3],\n",
      "       [  5, 304,  16],\n",
      "       [ 10,  66, 133]]), array([[ 94,  16,   4],\n",
      "       [  4, 298,  22],\n",
      "       [ 10,  69, 130]]), array([[ 93,  17,   4],\n",
      "       [  5, 293,  26],\n",
      "       [ 12,  75, 122]]), array([[ 95,  16,   2],\n",
      "       [  6, 301,  17],\n",
      "       [ 10,  64, 136]]), array([[ 94,  15,   4],\n",
      "       [  2, 299,  24],\n",
      "       [ 13,  56, 140]]), array([[ 91,  16,   6],\n",
      "       [  1, 303,  21],\n",
      "       [  7,  70, 132]]), array([[ 89,  23,   2],\n",
      "       [  6, 299,  20],\n",
      "       [  6,  69, 134]]), array([[103,   9,   2],\n",
      "       [  6, 298,  21],\n",
      "       [  6,  64, 139]]), array([[ 86,  24,   4],\n",
      "       [  3, 303,  19],\n",
      "       [  9,  63, 137]]), array([[ 92,  18,   4],\n",
      "       [  6, 302,  17],\n",
      "       [  7,  78, 124]]), array([[100,  11,   3],\n",
      "       [  5, 304,  16],\n",
      "       [ 10,  64, 135]]), array([[ 94,  16,   4],\n",
      "       [  3, 298,  23],\n",
      "       [ 11,  68, 130]]), array([[ 94,  16,   4],\n",
      "       [  5, 292,  27],\n",
      "       [ 11,  78, 120]]), array([[ 96,  15,   2],\n",
      "       [  6, 300,  18],\n",
      "       [ 10,  65, 135]]), array([[ 94,  13,   6],\n",
      "       [  3, 299,  23],\n",
      "       [ 13,  56, 140]]), array([[ 92,  16,   5],\n",
      "       [  1, 305,  19],\n",
      "       [  6,  72, 131]]), array([[ 90,  22,   2],\n",
      "       [  5, 299,  21],\n",
      "       [  6,  68, 135]]), array([[102,  10,   2],\n",
      "       [  7, 297,  21],\n",
      "       [  7,  64, 138]]), array([[ 86,  24,   4],\n",
      "       [  3, 304,  18],\n",
      "       [ 10,  64, 135]]), array([[ 91,  18,   5],\n",
      "       [  7, 300,  18],\n",
      "       [  7,  78, 124]]), array([[101,  10,   3],\n",
      "       [  5, 303,  17],\n",
      "       [ 10,  63, 136]]), array([[ 94,  16,   4],\n",
      "       [  3, 298,  23],\n",
      "       [ 11,  74, 124]]), array([[ 92,  17,   5],\n",
      "       [  5, 294,  25],\n",
      "       [ 10,  78, 121]]), array([[ 96,  15,   2],\n",
      "       [  6, 300,  18],\n",
      "       [ 10,  63, 137]]), array([[ 97,  12,   4],\n",
      "       [  2, 298,  25],\n",
      "       [ 13,  57, 139]]), array([[ 92,  16,   5],\n",
      "       [  2, 304,  19],\n",
      "       [  7,  73, 129]])]}}\n",
      "{'SVM': {'None': [0.8644464015960693, 0.8489458560943604, 0.8303875923156738, 0.8441228866577148, 0.8485069274902344, 0.839282751083374, 0.8272037506103516, 0.6879076957702637, 0.828066349029541, 0.8336968421936035]}, 'RF': {'None': [8.02333950996399, 8.234793663024902, 7.9224653244018555, 7.868056774139404, 7.946845293045044, 7.8946311473846436, 8.139342308044434, 7.938580751419067, 7.8842244148254395, 7.963507890701294]}, 'KNN': {'None': [0.0006351470947265625, 0.0006775856018066406, 0.0006842613220214844, 0.0007047653198242188, 0.0007569789886474609, 0.000762939453125, 0.0007746219635009766, 0.0007765293121337891, 0.0007507801055908203, 0.0007653236389160156, 0.0007317066192626953, 0.0007832050323486328, 0.0007824897766113281, 0.0007753372192382812, 0.0007531642913818359, 0.0007417201995849609, 0.0006737709045410156, 0.0006773471832275391, 0.0006530284881591797, 0.0007100105285644531, 0.0006303787231445312, 0.0006494522094726562, 0.0007059574127197266, 0.0007085800170898438, 0.0006859302520751953, 0.0006780624389648438, 0.0006587505340576172, 0.0006692409515380859, 0.0006413459777832031, 0.0006837844848632812, 0.0006680488586425781, 0.0006651878356933594, 0.0006606578826904297, 0.0006513595581054688, 0.0007166862487792969, 0.0006475448608398438, 0.0006775856018066406, 0.0006589889526367188, 0.0006842613220214844, 0.0006937980651855469, 0.0006415843963623047, 0.0006561279296875, 0.0006127357482910156, 0.0006368160247802734, 0.0006113052368164062, 0.0008175373077392578, 0.0007610321044921875, 0.0007011890411376953, 0.0006375312805175781, 0.0006656646728515625, 0.000820159912109375, 0.0006685256958007812, 0.0006580352783203125, 0.0006387233734130859, 0.0006902217864990234, 0.0007033348083496094, 0.0006489753723144531, 0.0006344318389892578, 0.0006186962127685547, 0.0006585121154785156, 0.0006301403045654297, 0.0006766319274902344, 0.0006616115570068359, 0.0006592273712158203, 0.0006725788116455078, 0.0006382465362548828, 0.0007259845733642578, 0.0007050037384033203, 0.0006222724914550781, 0.0006351470947265625, 0.0006344318389892578, 0.0006489753723144531, 0.0006284713745117188, 0.0006256103515625, 0.0006318092346191406, 0.00067138671875, 0.0007293224334716797, 0.0007131099700927734, 0.0007345676422119141, 0.0007302761077880859, 0.0007143020629882812, 0.0007259845733642578, 0.0007336139678955078, 0.0006456375122070312, 0.000614166259765625, 0.0006225109100341797, 0.0006520748138427734, 0.0006959438323974609, 0.0006825923919677734, 0.0007593631744384766, 0.0006673336029052734, 0.0006513595581054688, 0.0006272792816162109, 0.0006740093231201172, 0.0006208419799804688, 0.0006377696990966797, 0.0006577968597412109, 0.0006372928619384766, 0.0006601810455322266, 0.0006556510925292969, 0.0006365776062011719, 0.0008797645568847656, 0.0006511211395263672, 0.0006730556488037109, 0.0006515979766845703, 0.0008492469787597656, 0.0006277561187744141, 0.0006492137908935547, 0.0006196498870849609, 0.0006630420684814453, 0.0006456375122070312, 0.0006382465362548828, 0.0006225109100341797, 0.0007312297821044922, 0.0006556510925292969, 0.0006673336029052734, 0.0006978511810302734, 0.0006616115570068359, 0.000652313232421875, 0.0006949901580810547, 0.0006003379821777344, 0.0006210803985595703, 0.0006046295166015625, 0.0006539821624755859, 0.0006277561187744141, 0.0006380081176757812, 0.0006356239318847656, 0.0007634162902832031, 0.0007238388061523438, 0.0006465911865234375, 0.0006365776062011719, 0.0006906986236572266, 0.0006055831909179688, 0.0006117820739746094, 0.0006053447723388672, 0.0007073879241943359, 0.0006506443023681641, 0.0006096363067626953, 0.0006155967712402344, 0.0006535053253173828, 0.0006241798400878906, 0.0006475448608398438, 0.0006310939788818359, 0.0006070137023925781, 0.0006077289581298828, 0.0006201267242431641, 0.0007271766662597656, 0.0006093978881835938, 0.0006310939788818359, 0.0006682872772216797, 0.0006246566772460938, 0.0006248950958251953, 0.0006263256072998047, 0.0006392002105712891, 0.0007107257843017578, 0.0006504058837890625, 0.0006282329559326172, 0.0006079673767089844, 0.0006213188171386719, 0.0006213188171386719]}}\n",
      "5332\n",
      "2286\n",
      "1143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Gaussian_Blur accuracy: 0.8487654320987654\n",
      "SVM Gaussian_Blur f1 score: 0.8505186648104418\n",
      "SVM Gaussian_Blur classification time: 0.8561840057373047\n",
      "SVM Gaussian_Blur accuracy: 0.8518518518518519\n",
      "SVM Gaussian_Blur f1 score: 0.8508848680420984\n",
      "SVM Gaussian_Blur classification time: 0.8428502082824707\n",
      "SVM Gaussian_Blur accuracy: 0.8395061728395061\n",
      "SVM Gaussian_Blur f1 score: 0.8338364173979976\n",
      "SVM Gaussian_Blur classification time: 0.8319203853607178\n",
      "SVM Gaussian_Blur accuracy: 0.8317901234567902\n",
      "SVM Gaussian_Blur f1 score: 0.836526299660307\n",
      "SVM Gaussian_Blur classification time: 0.8382830619812012\n",
      "SVM Gaussian_Blur accuracy: 0.8503086419753086\n",
      "SVM Gaussian_Blur f1 score: 0.8506410256410256\n",
      "SVM Gaussian_Blur classification time: 0.8503255844116211\n",
      "SVM Gaussian_Blur accuracy: 0.8438948995363215\n",
      "SVM Gaussian_Blur f1 score: 0.8400949722653577\n",
      "SVM Gaussian_Blur classification time: 0.8376994132995605\n",
      "SVM Gaussian_Blur accuracy: 0.8268933539412674\n",
      "SVM Gaussian_Blur f1 score: 0.8262026349075194\n",
      "SVM Gaussian_Blur classification time: 0.843454122543335\n",
      "SVM Gaussian_Blur accuracy: 0.8593508500772797\n",
      "SVM Gaussian_Blur f1 score: 0.860309724929608\n",
      "SVM Gaussian_Blur classification time: 0.8452556133270264\n",
      "SVM Gaussian_Blur accuracy: 0.8423493044822257\n",
      "SVM Gaussian_Blur f1 score: 0.8387008338782335\n",
      "SVM Gaussian_Blur classification time: 0.8406069278717041\n",
      "SVM Gaussian_Blur accuracy: 0.8315301391035549\n",
      "SVM Gaussian_Blur f1 score: 0.8268736343585669\n",
      "SVM Gaussian_Blur classification time: 0.8344025611877441\n",
      "Random Forest Gaussian_Blur accuracy: 0.7870370370370371\n",
      "Random Forest Gaussian_Blur f1 score: 0.7838863622343976\n",
      "Random Forest Gaussian_Blur classification time: 7.865200757980347\n",
      "Random Forest Gaussian_Blur accuracy: 0.8179012345679012\n",
      "Random Forest Gaussian_Blur f1 score: 0.8187221935150176\n",
      "Random Forest Gaussian_Blur classification time: 8.013236045837402\n",
      "Random Forest Gaussian_Blur accuracy: 0.808641975308642\n",
      "Random Forest Gaussian_Blur f1 score: 0.8026140857938056\n",
      "Random Forest Gaussian_Blur classification time: 7.735918760299683\n",
      "Random Forest Gaussian_Blur accuracy: 0.7854938271604939\n",
      "Random Forest Gaussian_Blur f1 score: 0.7862752562268588\n",
      "Random Forest Gaussian_Blur classification time: 7.921578645706177\n",
      "Random Forest Gaussian_Blur accuracy: 0.8194444444444444\n",
      "Random Forest Gaussian_Blur f1 score: 0.8176492588059295\n",
      "Random Forest Gaussian_Blur classification time: 7.937133312225342\n",
      "Random Forest Gaussian_Blur accuracy: 0.8222565687789799\n",
      "Random Forest Gaussian_Blur f1 score: 0.8237948841529287\n",
      "Random Forest Gaussian_Blur classification time: 7.952821254730225\n",
      "Random Forest Gaussian_Blur accuracy: 0.7975270479134466\n",
      "Random Forest Gaussian_Blur f1 score: 0.8019331703614404\n",
      "Random Forest Gaussian_Blur classification time: 8.062299489974976\n",
      "Random Forest Gaussian_Blur accuracy: 0.8315301391035549\n",
      "Random Forest Gaussian_Blur f1 score: 0.8228545335942595\n",
      "Random Forest Gaussian_Blur classification time: 7.963570833206177\n",
      "Random Forest Gaussian_Blur accuracy: 0.8068006182380216\n",
      "Random Forest Gaussian_Blur f1 score: 0.8023707110791545\n",
      "Random Forest Gaussian_Blur classification time: 7.859736680984497\n",
      "Random Forest Gaussian_Blur accuracy: 0.8006182380216383\n",
      "Random Forest Gaussian_Blur f1 score: 0.7948508860550502\n",
      "Random Forest Gaussian_Blur classification time: 7.938720703125\n",
      "Best 1NN Gaussian_Blur accuracy: 0.8271604938271605\n",
      "Best 1NN Gaussian_Blur f1 score: 0.8305679732689027\n",
      "Best 1NN Gaussian_Blur classification time: 0.0005846023559570312\n",
      "Best 1NN Gaussian_Blur accuracy: 0.845679012345679\n",
      "Best 1NN Gaussian_Blur f1 score: 0.8444328439040233\n",
      "Best 1NN Gaussian_Blur classification time: 0.0006473064422607422\n",
      "Best 1NN Gaussian_Blur accuracy: 0.7947530864197531\n",
      "Best 1NN Gaussian_Blur f1 score: 0.7926489958412882\n",
      "Best 1NN Gaussian_Blur classification time: 0.0006721019744873047\n",
      "Best 1NN Gaussian_Blur accuracy: 0.8503086419753086\n",
      "Best 1NN Gaussian_Blur f1 score: 0.8486696981942664\n",
      "Best 1NN Gaussian_Blur classification time: 0.0006544589996337891\n",
      "Best 1NN Gaussian_Blur accuracy: 0.8271604938271605\n",
      "Best 1NN Gaussian_Blur f1 score: 0.8309577663391222\n",
      "Best 1NN Gaussian_Blur classification time: 0.0007674694061279297\n",
      "Best 1NN Gaussian_Blur accuracy: 0.8315301391035549\n",
      "Best 1NN Gaussian_Blur f1 score: 0.8278147286154662\n",
      "Best 1NN Gaussian_Blur classification time: 0.0007410049438476562\n",
      "Best 1NN Gaussian_Blur accuracy: 0.8253477588871716\n",
      "Best 1NN Gaussian_Blur f1 score: 0.8212235764143055\n",
      "Best 1NN Gaussian_Blur classification time: 0.0006947517395019531\n",
      "Best 1NN Gaussian_Blur accuracy: 0.8438948995363215\n",
      "Best 1NN Gaussian_Blur f1 score: 0.8435253923650445\n",
      "Best 1NN Gaussian_Blur classification time: 0.0008375644683837891\n",
      "Best 1NN Gaussian_Blur accuracy: 0.8423493044822257\n",
      "Best 1NN Gaussian_Blur f1 score: 0.8394120044336425\n",
      "Best 1NN Gaussian_Blur classification time: 0.0006954669952392578\n",
      "Best 1NN Gaussian_Blur accuracy: 0.8423493044822257\n",
      "Best 1NN Gaussian_Blur f1 score: 0.8426221295383637\n",
      "Best 1NN Gaussian_Blur classification time: 0.0007185935974121094\n",
      "Best 3NN Gaussian_Blur accuracy: 0.8333333333333334\n",
      "Best 3NN Gaussian_Blur f1 score: 0.8272416430952475\n",
      "Best 3NN Gaussian_Blur classification time: 0.0007479190826416016\n",
      "Best 3NN Gaussian_Blur accuracy: 0.8379629629629629\n",
      "Best 3NN Gaussian_Blur f1 score: 0.8336619479833742\n",
      "Best 3NN Gaussian_Blur classification time: 0.0007445812225341797\n",
      "Best 3NN Gaussian_Blur accuracy: 0.8132716049382716\n",
      "Best 3NN Gaussian_Blur f1 score: 0.8102081434362086\n",
      "Best 3NN Gaussian_Blur classification time: 0.0007228851318359375\n",
      "Best 3NN Gaussian_Blur accuracy: 0.8364197530864198\n",
      "Best 3NN Gaussian_Blur f1 score: 0.8363668071376553\n",
      "Best 3NN Gaussian_Blur classification time: 0.0006701946258544922\n",
      "Best 3NN Gaussian_Blur accuracy: 0.8302469135802469\n",
      "Best 3NN Gaussian_Blur f1 score: 0.8341083877781941\n",
      "Best 3NN Gaussian_Blur classification time: 0.0006971359252929688\n",
      "Best 3NN Gaussian_Blur accuracy: 0.8207109737248841\n",
      "Best 3NN Gaussian_Blur f1 score: 0.8204431359016032\n",
      "Best 3NN Gaussian_Blur classification time: 0.0008003711700439453\n",
      "Best 3NN Gaussian_Blur accuracy: 0.8176197836166924\n",
      "Best 3NN Gaussian_Blur f1 score: 0.8170067097847648\n",
      "Best 3NN Gaussian_Blur classification time: 0.0007567405700683594\n",
      "Best 3NN Gaussian_Blur accuracy: 0.839258114374034\n",
      "Best 3NN Gaussian_Blur f1 score: 0.8363398085447095\n",
      "Best 3NN Gaussian_Blur classification time: 0.000820159912109375\n",
      "Best 3NN Gaussian_Blur accuracy: 0.839258114374034\n",
      "Best 3NN Gaussian_Blur f1 score: 0.838723343986502\n",
      "Best 3NN Gaussian_Blur classification time: 0.0007996559143066406\n",
      "Best 3NN Gaussian_Blur accuracy: 0.8408037094281299\n",
      "Best 3NN Gaussian_Blur f1 score: 0.835396823264416\n",
      "Best 3NN Gaussian_Blur classification time: 0.0008945465087890625\n",
      "Best 5NN Gaussian_Blur accuracy: 0.8379629629629629\n",
      "Best 5NN Gaussian_Blur f1 score: 0.8348529995533478\n",
      "Best 5NN Gaussian_Blur classification time: 0.0007476806640625\n",
      "Best 5NN Gaussian_Blur accuracy: 0.816358024691358\n",
      "Best 5NN Gaussian_Blur f1 score: 0.8141706444280059\n",
      "Best 5NN Gaussian_Blur classification time: 0.0007965564727783203\n",
      "Best 5NN Gaussian_Blur accuracy: 0.8333333333333334\n",
      "Best 5NN Gaussian_Blur f1 score: 0.8291510387890121\n",
      "Best 5NN Gaussian_Blur classification time: 0.0006556510925292969\n",
      "Best 5NN Gaussian_Blur accuracy: 0.8271604938271605\n",
      "Best 5NN Gaussian_Blur f1 score: 0.8278324308183969\n",
      "Best 5NN Gaussian_Blur classification time: 0.0009229183197021484\n",
      "Best 5NN Gaussian_Blur accuracy: 0.8287037037037037\n",
      "Best 5NN Gaussian_Blur f1 score: 0.8305259840414081\n",
      "Best 5NN Gaussian_Blur classification time: 0.0006868839263916016\n",
      "Best 5NN Gaussian_Blur accuracy: 0.8253477588871716\n",
      "Best 5NN Gaussian_Blur f1 score: 0.8240510037961313\n",
      "Best 5NN Gaussian_Blur classification time: 0.0007746219635009766\n",
      "Best 5NN Gaussian_Blur accuracy: 0.8238021638330757\n",
      "Best 5NN Gaussian_Blur f1 score: 0.8236382689101452\n",
      "Best 5NN Gaussian_Blur classification time: 0.0007603168487548828\n",
      "Best 5NN Gaussian_Blur accuracy: 0.8361669242658424\n",
      "Best 5NN Gaussian_Blur f1 score: 0.8303486593887438\n",
      "Best 5NN Gaussian_Blur classification time: 0.0007174015045166016\n",
      "Best 5NN Gaussian_Blur accuracy: 0.8299845440494591\n",
      "Best 5NN Gaussian_Blur f1 score: 0.8305656173510761\n",
      "Best 5NN Gaussian_Blur classification time: 0.0007557868957519531\n",
      "Best 5NN Gaussian_Blur accuracy: 0.8253477588871716\n",
      "Best 5NN Gaussian_Blur f1 score: 0.820632715799491\n",
      "Best 5NN Gaussian_Blur classification time: 0.0007293224334716797\n",
      "Best 7NN Gaussian_Blur accuracy: 0.8271604938271605\n",
      "Best 7NN Gaussian_Blur f1 score: 0.8231149823221312\n",
      "Best 7NN Gaussian_Blur classification time: 0.0007786750793457031\n",
      "Best 7NN Gaussian_Blur accuracy: 0.8302469135802469\n",
      "Best 7NN Gaussian_Blur f1 score: 0.8266923304577519\n",
      "Best 7NN Gaussian_Blur classification time: 0.0007319450378417969\n",
      "Best 7NN Gaussian_Blur accuracy: 0.8271604938271605\n",
      "Best 7NN Gaussian_Blur f1 score: 0.8240938318122369\n",
      "Best 7NN Gaussian_Blur classification time: 0.0006890296936035156\n",
      "Best 7NN Gaussian_Blur accuracy: 0.8132716049382716\n",
      "Best 7NN Gaussian_Blur f1 score: 0.8124237576242376\n",
      "Best 7NN Gaussian_Blur classification time: 0.0006968975067138672\n",
      "Best 7NN Gaussian_Blur accuracy: 0.8410493827160493\n",
      "Best 7NN Gaussian_Blur f1 score: 0.8433017618130269\n",
      "Best 7NN Gaussian_Blur classification time: 0.0007977485656738281\n",
      "Best 7NN Gaussian_Blur accuracy: 0.8191653786707882\n",
      "Best 7NN Gaussian_Blur f1 score: 0.8158090985248281\n",
      "Best 7NN Gaussian_Blur classification time: 0.0007009506225585938\n",
      "Best 7NN Gaussian_Blur accuracy: 0.8098918083462133\n",
      "Best 7NN Gaussian_Blur f1 score: 0.8058601234830743\n",
      "Best 7NN Gaussian_Blur classification time: 0.0007145404815673828\n",
      "Best 7NN Gaussian_Blur accuracy: 0.839258114374034\n",
      "Best 7NN Gaussian_Blur f1 score: 0.8342712915745608\n",
      "Best 7NN Gaussian_Blur classification time: 0.0007643699645996094\n",
      "Best 7NN Gaussian_Blur accuracy: 0.8330757341576507\n",
      "Best 7NN Gaussian_Blur f1 score: 0.829085571021055\n",
      "Best 7NN Gaussian_Blur classification time: 0.0008065700531005859\n",
      "Best 7NN Gaussian_Blur accuracy: 0.8207109737248841\n",
      "Best 7NN Gaussian_Blur f1 score: 0.8170093860039563\n",
      "Best 7NN Gaussian_Blur classification time: 0.0006494522094726562\n",
      "Best 9NN Gaussian_Blur accuracy: 0.8271604938271605\n",
      "Best 9NN Gaussian_Blur f1 score: 0.8260298359651799\n",
      "Best 9NN Gaussian_Blur classification time: 0.0008320808410644531\n",
      "Best 9NN Gaussian_Blur accuracy: 0.8256172839506173\n",
      "Best 9NN Gaussian_Blur f1 score: 0.8236759127953408\n",
      "Best 9NN Gaussian_Blur classification time: 0.0007302761077880859\n",
      "Best 9NN Gaussian_Blur accuracy: 0.8209876543209876\n",
      "Best 9NN Gaussian_Blur f1 score: 0.8146815524246486\n",
      "Best 9NN Gaussian_Blur classification time: 0.0008475780487060547\n",
      "Best 9NN Gaussian_Blur accuracy: 0.8132716049382716\n",
      "Best 9NN Gaussian_Blur f1 score: 0.8126559736461588\n",
      "Best 9NN Gaussian_Blur classification time: 0.0006859302520751953\n",
      "Best 9NN Gaussian_Blur accuracy: 0.8364197530864198\n",
      "Best 9NN Gaussian_Blur f1 score: 0.836858735710042\n",
      "Best 9NN Gaussian_Blur classification time: 0.0007762908935546875\n",
      "Best 9NN Gaussian_Blur accuracy: 0.8238021638330757\n",
      "Best 9NN Gaussian_Blur f1 score: 0.8204316218640275\n",
      "Best 9NN Gaussian_Blur classification time: 0.0007171630859375\n",
      "Best 9NN Gaussian_Blur accuracy: 0.8083462132921174\n",
      "Best 9NN Gaussian_Blur f1 score: 0.8048073747688638\n",
      "Best 9NN Gaussian_Blur classification time: 0.0008633136749267578\n",
      "Best 9NN Gaussian_Blur accuracy: 0.8253477588871716\n",
      "Best 9NN Gaussian_Blur f1 score: 0.8216766224754196\n",
      "Best 9NN Gaussian_Blur classification time: 0.0007491111755371094\n",
      "Best 9NN Gaussian_Blur accuracy: 0.8299845440494591\n",
      "Best 9NN Gaussian_Blur f1 score: 0.8238756685438888\n",
      "Best 9NN Gaussian_Blur classification time: 0.00069427490234375\n",
      "Best 9NN Gaussian_Blur accuracy: 0.8253477588871716\n",
      "Best 9NN Gaussian_Blur f1 score: 0.8188749416282844\n",
      "Best 9NN Gaussian_Blur classification time: 0.0007138252258300781\n",
      "Best 11NN Gaussian_Blur accuracy: 0.8302469135802469\n",
      "Best 11NN Gaussian_Blur f1 score: 0.8315752696841522\n",
      "Best 11NN Gaussian_Blur classification time: 0.0007052421569824219\n",
      "Best 11NN Gaussian_Blur accuracy: 0.8287037037037037\n",
      "Best 11NN Gaussian_Blur f1 score: 0.8285600366718859\n",
      "Best 11NN Gaussian_Blur classification time: 0.0007395744323730469\n",
      "Best 11NN Gaussian_Blur accuracy: 0.816358024691358\n",
      "Best 11NN Gaussian_Blur f1 score: 0.8073924121885753\n",
      "Best 11NN Gaussian_Blur classification time: 0.0007445812225341797\n",
      "Best 11NN Gaussian_Blur accuracy: 0.8101851851851852\n",
      "Best 11NN Gaussian_Blur f1 score: 0.8089159883016176\n",
      "Best 11NN Gaussian_Blur classification time: 0.0006840229034423828\n",
      "Best 11NN Gaussian_Blur accuracy: 0.8348765432098766\n",
      "Best 11NN Gaussian_Blur f1 score: 0.8336053311573507\n",
      "Best 11NN Gaussian_Blur classification time: 0.0007207393646240234\n",
      "Best 11NN Gaussian_Blur accuracy: 0.8145285935085008\n",
      "Best 11NN Gaussian_Blur f1 score: 0.8100444743992349\n",
      "Best 11NN Gaussian_Blur classification time: 0.0007781982421875\n",
      "Best 11NN Gaussian_Blur accuracy: 0.8083462132921174\n",
      "Best 11NN Gaussian_Blur f1 score: 0.8058631504285628\n",
      "Best 11NN Gaussian_Blur classification time: 0.0006635189056396484\n",
      "Best 11NN Gaussian_Blur accuracy: 0.8268933539412674\n",
      "Best 11NN Gaussian_Blur f1 score: 0.8252279295659304\n",
      "Best 11NN Gaussian_Blur classification time: 0.0007064342498779297\n",
      "Best 11NN Gaussian_Blur accuracy: 0.8191653786707882\n",
      "Best 11NN Gaussian_Blur f1 score: 0.8118897198936131\n",
      "Best 11NN Gaussian_Blur classification time: 0.0007581710815429688\n",
      "Best 11NN Gaussian_Blur accuracy: 0.8052550231839258\n",
      "Best 11NN Gaussian_Blur f1 score: 0.8032563526125758\n",
      "Best 11NN Gaussian_Blur classification time: 0.0006654262542724609\n",
      "Best 13NN Gaussian_Blur accuracy: 0.8302469135802469\n",
      "Best 13NN Gaussian_Blur f1 score: 0.8295363071345867\n",
      "Best 13NN Gaussian_Blur classification time: 0.0006902217864990234\n",
      "Best 13NN Gaussian_Blur accuracy: 0.8225308641975309\n",
      "Best 13NN Gaussian_Blur f1 score: 0.8221279896764345\n",
      "Best 13NN Gaussian_Blur classification time: 0.0007100105285644531\n",
      "Best 13NN Gaussian_Blur accuracy: 0.8287037037037037\n",
      "Best 13NN Gaussian_Blur f1 score: 0.8154675568301119\n",
      "Best 13NN Gaussian_Blur classification time: 0.0007987022399902344\n",
      "Best 13NN Gaussian_Blur accuracy: 0.8055555555555556\n",
      "Best 13NN Gaussian_Blur f1 score: 0.7995993129847904\n",
      "Best 13NN Gaussian_Blur classification time: 0.0007512569427490234\n",
      "Best 13NN Gaussian_Blur accuracy: 0.8410493827160493\n",
      "Best 13NN Gaussian_Blur f1 score: 0.8387955182917693\n",
      "Best 13NN Gaussian_Blur classification time: 0.0007216930389404297\n",
      "Best 13NN Gaussian_Blur accuracy: 0.8222565687789799\n",
      "Best 13NN Gaussian_Blur f1 score: 0.8174275916881157\n",
      "Best 13NN Gaussian_Blur classification time: 0.0008842945098876953\n",
      "Best 13NN Gaussian_Blur accuracy: 0.8006182380216383\n",
      "Best 13NN Gaussian_Blur f1 score: 0.7955745477679482\n",
      "Best 13NN Gaussian_Blur classification time: 0.0007443428039550781\n",
      "Best 13NN Gaussian_Blur accuracy: 0.8098918083462133\n",
      "Best 13NN Gaussian_Blur f1 score: 0.8071746145624775\n",
      "Best 13NN Gaussian_Blur classification time: 0.0006797313690185547\n",
      "Best 13NN Gaussian_Blur accuracy: 0.8191653786707882\n",
      "Best 13NN Gaussian_Blur f1 score: 0.8120039682539683\n",
      "Best 13NN Gaussian_Blur classification time: 0.0007154941558837891\n",
      "Best 13NN Gaussian_Blur accuracy: 0.8238021638330757\n",
      "Best 13NN Gaussian_Blur f1 score: 0.8199832995414355\n",
      "Best 13NN Gaussian_Blur classification time: 0.0006589889526367188\n",
      "Best 15NN Gaussian_Blur accuracy: 0.8240740740740741\n",
      "Best 15NN Gaussian_Blur f1 score: 0.8229121807735459\n",
      "Best 15NN Gaussian_Blur classification time: 0.0008232593536376953\n",
      "Best 15NN Gaussian_Blur accuracy: 0.8209876543209876\n",
      "Best 15NN Gaussian_Blur f1 score: 0.8215294231474544\n",
      "Best 15NN Gaussian_Blur classification time: 0.0007042884826660156\n",
      "Best 15NN Gaussian_Blur accuracy: 0.816358024691358\n",
      "Best 15NN Gaussian_Blur f1 score: 0.8058284491896238\n",
      "Best 15NN Gaussian_Blur classification time: 0.0008137226104736328\n",
      "Best 15NN Gaussian_Blur accuracy: 0.8055555555555556\n",
      "Best 15NN Gaussian_Blur f1 score: 0.8005041017058567\n",
      "Best 15NN Gaussian_Blur classification time: 0.0006592273712158203\n",
      "Best 15NN Gaussian_Blur accuracy: 0.8348765432098766\n",
      "Best 15NN Gaussian_Blur f1 score: 0.8329183704015319\n",
      "Best 15NN Gaussian_Blur classification time: 0.0008878707885742188\n",
      "Best 15NN Gaussian_Blur accuracy: 0.8114374034003091\n",
      "Best 15NN Gaussian_Blur f1 score: 0.8057839721254355\n",
      "Best 15NN Gaussian_Blur classification time: 0.0006327629089355469\n",
      "Best 15NN Gaussian_Blur accuracy: 0.8083462132921174\n",
      "Best 15NN Gaussian_Blur f1 score: 0.8042815939091842\n",
      "Best 15NN Gaussian_Blur classification time: 0.0007433891296386719\n",
      "Best 15NN Gaussian_Blur accuracy: 0.80370942812983\n",
      "Best 15NN Gaussian_Blur f1 score: 0.799487232843456\n",
      "Best 15NN Gaussian_Blur classification time: 0.0007469654083251953\n",
      "Best 15NN Gaussian_Blur accuracy: 0.8145285935085008\n",
      "Best 15NN Gaussian_Blur f1 score: 0.8111038885712345\n",
      "Best 15NN Gaussian_Blur classification time: 0.0007073879241943359\n",
      "Best 15NN Gaussian_Blur accuracy: 0.8145285935085008\n",
      "Best 15NN Gaussian_Blur f1 score: 0.8088835596702081\n",
      "Best 15NN Gaussian_Blur classification time: 0.0006732940673828125\n",
      "Best 17NN Gaussian_Blur accuracy: 0.8179012345679012\n",
      "Best 17NN Gaussian_Blur f1 score: 0.8155831591027303\n",
      "Best 17NN Gaussian_Blur classification time: 0.0008287429809570312\n",
      "Best 17NN Gaussian_Blur accuracy: 0.8101851851851852\n",
      "Best 17NN Gaussian_Blur f1 score: 0.8091784206892121\n",
      "Best 17NN Gaussian_Blur classification time: 0.0006995201110839844\n",
      "Best 17NN Gaussian_Blur accuracy: 0.8148148148148148\n",
      "Best 17NN Gaussian_Blur f1 score: 0.803894119785844\n",
      "Best 17NN Gaussian_Blur classification time: 0.0006945133209228516\n",
      "Best 17NN Gaussian_Blur accuracy: 0.7947530864197531\n",
      "Best 17NN Gaussian_Blur f1 score: 0.7880994640932505\n",
      "Best 17NN Gaussian_Blur classification time: 0.0006704330444335938\n",
      "Best 17NN Gaussian_Blur accuracy: 0.8395061728395061\n",
      "Best 17NN Gaussian_Blur f1 score: 0.8361240151233079\n",
      "Best 17NN Gaussian_Blur classification time: 0.0008151531219482422\n",
      "Best 17NN Gaussian_Blur accuracy: 0.8114374034003091\n",
      "Best 17NN Gaussian_Blur f1 score: 0.805896259692021\n",
      "Best 17NN Gaussian_Blur classification time: 0.0007359981536865234\n",
      "Best 17NN Gaussian_Blur accuracy: 0.8083462132921174\n",
      "Best 17NN Gaussian_Blur f1 score: 0.8029455326051208\n",
      "Best 17NN Gaussian_Blur classification time: 0.0007121562957763672\n",
      "Best 17NN Gaussian_Blur accuracy: 0.8068006182380216\n",
      "Best 17NN Gaussian_Blur f1 score: 0.8021580316973017\n",
      "Best 17NN Gaussian_Blur classification time: 0.0006945133209228516\n",
      "Best 17NN Gaussian_Blur accuracy: 0.7975270479134466\n",
      "Best 17NN Gaussian_Blur f1 score: 0.7919099875168868\n",
      "Best 17NN Gaussian_Blur classification time: 0.000782012939453125\n",
      "Best 17NN Gaussian_Blur accuracy: 0.8098918083462133\n",
      "Best 17NN Gaussian_Blur f1 score: 0.803644206021116\n",
      "Best 17NN Gaussian_Blur classification time: 0.0006794929504394531\n",
      "Best 19NN Gaussian_Blur accuracy: 0.8117283950617284\n",
      "Best 19NN Gaussian_Blur f1 score: 0.8098101514304391\n",
      "Best 19NN Gaussian_Blur classification time: 0.0006437301635742188\n",
      "Best 19NN Gaussian_Blur accuracy: 0.8101851851851852\n",
      "Best 19NN Gaussian_Blur f1 score: 0.8086937494619334\n",
      "Best 19NN Gaussian_Blur classification time: 0.000820159912109375\n",
      "Best 19NN Gaussian_Blur accuracy: 0.808641975308642\n",
      "Best 19NN Gaussian_Blur f1 score: 0.7977275126237832\n",
      "Best 19NN Gaussian_Blur classification time: 0.0007627010345458984\n",
      "Best 19NN Gaussian_Blur accuracy: 0.7993827160493827\n",
      "Best 19NN Gaussian_Blur f1 score: 0.7978966890816644\n",
      "Best 19NN Gaussian_Blur classification time: 0.0006918907165527344\n",
      "Best 19NN Gaussian_Blur accuracy: 0.8379629629629629\n",
      "Best 19NN Gaussian_Blur f1 score: 0.834073972810815\n",
      "Best 19NN Gaussian_Blur classification time: 0.0007998943328857422\n",
      "Best 19NN Gaussian_Blur accuracy: 0.8098918083462133\n",
      "Best 19NN Gaussian_Blur f1 score: 0.8039762846639062\n",
      "Best 19NN Gaussian_Blur classification time: 0.0007731914520263672\n",
      "Best 19NN Gaussian_Blur accuracy: 0.7928902627511591\n",
      "Best 19NN Gaussian_Blur f1 score: 0.7848232848232849\n",
      "Best 19NN Gaussian_Blur classification time: 0.0006923675537109375\n",
      "Best 19NN Gaussian_Blur accuracy: 0.8222565687789799\n",
      "Best 19NN Gaussian_Blur f1 score: 0.8178817297845004\n",
      "Best 19NN Gaussian_Blur classification time: 0.0007033348083496094\n",
      "Best 19NN Gaussian_Blur accuracy: 0.8006182380216383\n",
      "Best 19NN Gaussian_Blur f1 score: 0.7923471412436277\n",
      "Best 19NN Gaussian_Blur classification time: 0.0008151531219482422\n",
      "Best 19NN Gaussian_Blur accuracy: 0.8129829984544049\n",
      "Best 19NN Gaussian_Blur f1 score: 0.8081126211967332\n",
      "Best 19NN Gaussian_Blur classification time: 0.0007617473602294922\n",
      "Best 21NN Gaussian_Blur accuracy: 0.8070987654320988\n",
      "Best 21NN Gaussian_Blur f1 score: 0.8055283070384106\n",
      "Best 21NN Gaussian_Blur classification time: 0.0006926059722900391\n",
      "Best 21NN Gaussian_Blur accuracy: 0.8101851851851852\n",
      "Best 21NN Gaussian_Blur f1 score: 0.8073936802090759\n",
      "Best 21NN Gaussian_Blur classification time: 0.0006704330444335938\n",
      "Best 21NN Gaussian_Blur accuracy: 0.8194444444444444\n",
      "Best 21NN Gaussian_Blur f1 score: 0.8081403418325421\n",
      "Best 21NN Gaussian_Blur classification time: 0.0007796287536621094\n",
      "Best 21NN Gaussian_Blur accuracy: 0.7916666666666666\n",
      "Best 21NN Gaussian_Blur f1 score: 0.7867774837144378\n",
      "Best 21NN Gaussian_Blur classification time: 0.0006284713745117188\n",
      "Best 21NN Gaussian_Blur accuracy: 0.8379629629629629\n",
      "Best 21NN Gaussian_Blur f1 score: 0.8340251045648347\n",
      "Best 21NN Gaussian_Blur classification time: 0.0007121562957763672\n",
      "Best 21NN Gaussian_Blur accuracy: 0.80370942812983\n",
      "Best 21NN Gaussian_Blur f1 score: 0.7976040714498907\n",
      "Best 21NN Gaussian_Blur classification time: 0.0007770061492919922\n",
      "Best 21NN Gaussian_Blur accuracy: 0.794435857805255\n",
      "Best 21NN Gaussian_Blur f1 score: 0.7884333948163734\n",
      "Best 21NN Gaussian_Blur classification time: 0.0007302761077880859\n",
      "Best 21NN Gaussian_Blur accuracy: 0.8114374034003091\n",
      "Best 21NN Gaussian_Blur f1 score: 0.805003537204965\n",
      "Best 21NN Gaussian_Blur classification time: 0.0009093284606933594\n",
      "Best 21NN Gaussian_Blur accuracy: 0.8021638330757341\n",
      "Best 21NN Gaussian_Blur f1 score: 0.7934183299774699\n",
      "Best 21NN Gaussian_Blur classification time: 0.0007266998291015625\n",
      "Best 21NN Gaussian_Blur accuracy: 0.8068006182380216\n",
      "Best 21NN Gaussian_Blur f1 score: 0.8019632902681989\n",
      "Best 21NN Gaussian_Blur classification time: 0.0006847381591796875\n",
      "Best 23NN Gaussian_Blur accuracy: 0.8055555555555556\n",
      "Best 23NN Gaussian_Blur f1 score: 0.8033263310258616\n",
      "Best 23NN Gaussian_Blur classification time: 0.0009162425994873047\n",
      "Best 23NN Gaussian_Blur accuracy: 0.8117283950617284\n",
      "Best 23NN Gaussian_Blur f1 score: 0.8059988737793446\n",
      "Best 23NN Gaussian_Blur classification time: 0.0006670951843261719\n",
      "Best 23NN Gaussian_Blur accuracy: 0.8148148148148148\n",
      "Best 23NN Gaussian_Blur f1 score: 0.8038677468642291\n",
      "Best 23NN Gaussian_Blur classification time: 0.0006797313690185547\n",
      "Best 23NN Gaussian_Blur accuracy: 0.7870370370370371\n",
      "Best 23NN Gaussian_Blur f1 score: 0.7824952839639653\n",
      "Best 23NN Gaussian_Blur classification time: 0.0007665157318115234\n",
      "Best 23NN Gaussian_Blur accuracy: 0.8271604938271605\n",
      "Best 23NN Gaussian_Blur f1 score: 0.823155172746743\n",
      "Best 23NN Gaussian_Blur classification time: 0.0006878376007080078\n",
      "Best 23NN Gaussian_Blur accuracy: 0.8006182380216383\n",
      "Best 23NN Gaussian_Blur f1 score: 0.7937479821041465\n",
      "Best 23NN Gaussian_Blur classification time: 0.0006659030914306641\n",
      "Best 23NN Gaussian_Blur accuracy: 0.7959814528593508\n",
      "Best 23NN Gaussian_Blur f1 score: 0.7903014639014537\n",
      "Best 23NN Gaussian_Blur classification time: 0.0007593631744384766\n",
      "Best 23NN Gaussian_Blur accuracy: 0.8083462132921174\n",
      "Best 23NN Gaussian_Blur f1 score: 0.801651371439479\n",
      "Best 23NN Gaussian_Blur classification time: 0.0007011890411376953\n",
      "Best 23NN Gaussian_Blur accuracy: 0.7975270479134466\n",
      "Best 23NN Gaussian_Blur f1 score: 0.7899756784885371\n",
      "Best 23NN Gaussian_Blur classification time: 0.0008230209350585938\n",
      "Best 23NN Gaussian_Blur accuracy: 0.7990726429675425\n",
      "Best 23NN Gaussian_Blur f1 score: 0.7939566027168469\n",
      "Best 23NN Gaussian_Blur classification time: 0.0006606578826904297\n",
      "Best 25NN Gaussian_Blur accuracy: 0.8117283950617284\n",
      "Best 25NN Gaussian_Blur f1 score: 0.809728691069224\n",
      "Best 25NN Gaussian_Blur classification time: 0.0007579326629638672\n",
      "Best 25NN Gaussian_Blur accuracy: 0.816358024691358\n",
      "Best 25NN Gaussian_Blur f1 score: 0.8129724590814327\n",
      "Best 25NN Gaussian_Blur classification time: 0.0007460117340087891\n",
      "Best 25NN Gaussian_Blur accuracy: 0.7978395061728395\n",
      "Best 25NN Gaussian_Blur f1 score: 0.7830477661201193\n",
      "Best 25NN Gaussian_Blur classification time: 0.0007636547088623047\n",
      "Best 25NN Gaussian_Blur accuracy: 0.7978395061728395\n",
      "Best 25NN Gaussian_Blur f1 score: 0.7943472746510022\n",
      "Best 25NN Gaussian_Blur classification time: 0.0006480216979980469\n",
      "Best 25NN Gaussian_Blur accuracy: 0.8240740740740741\n",
      "Best 25NN Gaussian_Blur f1 score: 0.8190102118109012\n",
      "Best 25NN Gaussian_Blur classification time: 0.0009224414825439453\n",
      "Best 25NN Gaussian_Blur accuracy: 0.7959814528593508\n",
      "Best 25NN Gaussian_Blur f1 score: 0.7891780954123438\n",
      "Best 25NN Gaussian_Blur classification time: 0.0006601810455322266\n",
      "Best 25NN Gaussian_Blur accuracy: 0.7836166924265843\n",
      "Best 25NN Gaussian_Blur f1 score: 0.7771368484407449\n",
      "Best 25NN Gaussian_Blur classification time: 0.0009436607360839844\n",
      "Best 25NN Gaussian_Blur accuracy: 0.8021638330757341\n",
      "Best 25NN Gaussian_Blur f1 score: 0.7962430371821156\n",
      "Best 25NN Gaussian_Blur classification time: 0.0006723403930664062\n",
      "Best 25NN Gaussian_Blur accuracy: 0.794435857805255\n",
      "Best 25NN Gaussian_Blur f1 score: 0.7868370723237934\n",
      "Best 25NN Gaussian_Blur classification time: 0.0009012222290039062\n",
      "Best 25NN Gaussian_Blur accuracy: 0.794435857805255\n",
      "Best 25NN Gaussian_Blur f1 score: 0.7911783460498252\n",
      "Best 25NN Gaussian_Blur classification time: 0.0006861686706542969\n",
      "Best 27NN Gaussian_Blur accuracy: 0.8070987654320988\n",
      "Best 27NN Gaussian_Blur f1 score: 0.8041380856502002\n",
      "Best 27NN Gaussian_Blur classification time: 0.0007297992706298828\n",
      "Best 27NN Gaussian_Blur accuracy: 0.8225308641975309\n",
      "Best 27NN Gaussian_Blur f1 score: 0.8191570508138987\n",
      "Best 27NN Gaussian_Blur classification time: 0.0007073879241943359\n",
      "Best 27NN Gaussian_Blur accuracy: 0.8009259259259259\n",
      "Best 27NN Gaussian_Blur f1 score: 0.7859917563197824\n",
      "Best 27NN Gaussian_Blur classification time: 0.0006761550903320312\n",
      "Best 27NN Gaussian_Blur accuracy: 0.7854938271604939\n",
      "Best 27NN Gaussian_Blur f1 score: 0.7828076280590247\n",
      "Best 27NN Gaussian_Blur classification time: 0.0007176399230957031\n",
      "Best 27NN Gaussian_Blur accuracy: 0.8209876543209876\n",
      "Best 27NN Gaussian_Blur f1 score: 0.8162908859035114\n",
      "Best 27NN Gaussian_Blur classification time: 0.0007221698760986328\n",
      "Best 27NN Gaussian_Blur accuracy: 0.7975270479134466\n",
      "Best 27NN Gaussian_Blur f1 score: 0.7889361741036383\n",
      "Best 27NN Gaussian_Blur classification time: 0.0007750988006591797\n",
      "Best 27NN Gaussian_Blur accuracy: 0.7867078825347759\n",
      "Best 27NN Gaussian_Blur f1 score: 0.780013333697872\n",
      "Best 27NN Gaussian_Blur classification time: 0.0006802082061767578\n",
      "Best 27NN Gaussian_Blur accuracy: 0.7959814528593508\n",
      "Best 27NN Gaussian_Blur f1 score: 0.7880725378032182\n",
      "Best 27NN Gaussian_Blur classification time: 0.0007531642913818359\n",
      "Best 27NN Gaussian_Blur accuracy: 0.7913446676970634\n",
      "Best 27NN Gaussian_Blur f1 score: 0.7839109291482725\n",
      "Best 27NN Gaussian_Blur classification time: 0.0007777214050292969\n",
      "Best 27NN Gaussian_Blur accuracy: 0.794435857805255\n",
      "Best 27NN Gaussian_Blur f1 score: 0.788300916973245\n",
      "Best 27NN Gaussian_Blur classification time: 0.0006434917449951172\n",
      "Best 29NN Gaussian_Blur accuracy: 0.8101851851851852\n",
      "Best 29NN Gaussian_Blur f1 score: 0.8075700797686581\n",
      "Best 29NN Gaussian_Blur classification time: 0.0007200241088867188\n",
      "Best 29NN Gaussian_Blur accuracy: 0.8132716049382716\n",
      "Best 29NN Gaussian_Blur f1 score: 0.8091652024779196\n",
      "Best 29NN Gaussian_Blur classification time: 0.0006663799285888672\n",
      "Best 29NN Gaussian_Blur accuracy: 0.7978395061728395\n",
      "Best 29NN Gaussian_Blur f1 score: 0.7814653683038085\n",
      "Best 29NN Gaussian_Blur classification time: 0.0008122920989990234\n",
      "Best 29NN Gaussian_Blur accuracy: 0.7808641975308642\n",
      "Best 29NN Gaussian_Blur f1 score: 0.7762641798563833\n",
      "Best 29NN Gaussian_Blur classification time: 0.0007083415985107422\n",
      "Best 29NN Gaussian_Blur accuracy: 0.8240740740740741\n",
      "Best 29NN Gaussian_Blur f1 score: 0.820172819037455\n",
      "Best 29NN Gaussian_Blur classification time: 0.0007791519165039062\n",
      "Best 29NN Gaussian_Blur accuracy: 0.7975270479134466\n",
      "Best 29NN Gaussian_Blur f1 score: 0.7897624877125278\n",
      "Best 29NN Gaussian_Blur classification time: 0.0006842613220214844\n",
      "Best 29NN Gaussian_Blur accuracy: 0.7836166924265843\n",
      "Best 29NN Gaussian_Blur f1 score: 0.7743987255312237\n",
      "Best 29NN Gaussian_Blur classification time: 0.0007929801940917969\n",
      "Best 29NN Gaussian_Blur accuracy: 0.80370942812983\n",
      "Best 29NN Gaussian_Blur f1 score: 0.7970117958177863\n",
      "Best 29NN Gaussian_Blur classification time: 0.0006673336029052734\n",
      "Best 29NN Gaussian_Blur accuracy: 0.7851622874806801\n",
      "Best 29NN Gaussian_Blur f1 score: 0.7762302273627255\n",
      "Best 29NN Gaussian_Blur classification time: 0.0011017322540283203\n",
      "Best 29NN Gaussian_Blur accuracy: 0.7897990726429676\n",
      "Best 29NN Gaussian_Blur f1 score: 0.7810282206862937\n",
      "Best 29NN Gaussian_Blur classification time: 0.0007598400115966797\n",
      "Best 31NN Gaussian_Blur accuracy: 0.8148148148148148\n",
      "Best 31NN Gaussian_Blur f1 score: 0.8134416286774108\n",
      "Best 31NN Gaussian_Blur classification time: 0.0007319450378417969\n",
      "Best 31NN Gaussian_Blur accuracy: 0.8194444444444444\n",
      "Best 31NN Gaussian_Blur f1 score: 0.8161264286301998\n",
      "Best 31NN Gaussian_Blur classification time: 0.0007205009460449219\n",
      "Best 31NN Gaussian_Blur accuracy: 0.7993827160493827\n",
      "Best 31NN Gaussian_Blur f1 score: 0.783035817188695\n",
      "Best 31NN Gaussian_Blur classification time: 0.0007066726684570312\n",
      "Best 31NN Gaussian_Blur accuracy: 0.7870370370370371\n",
      "Best 31NN Gaussian_Blur f1 score: 0.7828988128108024\n",
      "Best 31NN Gaussian_Blur classification time: 0.0007588863372802734\n",
      "Best 31NN Gaussian_Blur accuracy: 0.8240740740740741\n",
      "Best 31NN Gaussian_Blur f1 score: 0.8192732970185869\n",
      "Best 31NN Gaussian_Blur classification time: 0.0006859302520751953\n",
      "Best 31NN Gaussian_Blur accuracy: 0.7897990726429676\n",
      "Best 31NN Gaussian_Blur f1 score: 0.7812273714038622\n",
      "Best 31NN Gaussian_Blur classification time: 0.0007078647613525391\n",
      "Best 31NN Gaussian_Blur accuracy: 0.7805255023183926\n",
      "Best 31NN Gaussian_Blur f1 score: 0.7715714568655745\n",
      "Best 31NN Gaussian_Blur classification time: 0.0007698535919189453\n",
      "Best 31NN Gaussian_Blur accuracy: 0.8114374034003091\n",
      "Best 31NN Gaussian_Blur f1 score: 0.8049417526648286\n",
      "Best 31NN Gaussian_Blur classification time: 0.0007264614105224609\n",
      "Best 31NN Gaussian_Blur accuracy: 0.7836166924265843\n",
      "Best 31NN Gaussian_Blur f1 score: 0.7740928013225657\n",
      "Best 31NN Gaussian_Blur classification time: 0.0007584095001220703\n",
      "Best 31NN Gaussian_Blur accuracy: 0.7975270479134466\n",
      "Best 31NN Gaussian_Blur f1 score: 0.78831988412994\n",
      "Best 31NN Gaussian_Blur classification time: 0.0006821155548095703\n",
      "Best 7NN Gaussian_Blur accuracy: 0.8503086419753086\n",
      "Best 7NN Gaussian_Blur f1 score: 0.8486696981942664\n",
      "Best 7NN Gaussian_Blur classification time: 0.0006544589996337891\n",
      "{'SVM': {'None': [0.8626543209876543, 0.8580246913580247, 0.8441358024691358, 0.8302469135802469, 0.845679012345679, 0.848531684698609, 0.8438948995363215, 0.874806800618238, 0.8531684698608965, 0.8423493044822257], 'Gaussian_Blur': [0.8487654320987654, 0.8518518518518519, 0.8395061728395061, 0.8317901234567902, 0.8503086419753086, 0.8438948995363215, 0.8268933539412674, 0.8593508500772797, 0.8423493044822257, 0.8315301391035549]}, 'RF': {'None': [0.8194444444444444, 0.8317901234567902, 0.8101851851851852, 0.7962962962962963, 0.8225308641975309, 0.8068006182380216, 0.8129829984544049, 0.8222565687789799, 0.8253477588871716, 0.8098918083462133], 'Gaussian_Blur': [0.7870370370370371, 0.8179012345679012, 0.808641975308642, 0.7854938271604939, 0.8194444444444444, 0.8222565687789799, 0.7975270479134466, 0.8315301391035549, 0.8068006182380216, 0.8006182380216383]}, 'KNN': {'None': [0.8580246913580247, 0.8595679012345679, 0.816358024691358, 0.8364197530864198, 0.8472222222222222, 0.8315301391035549, 0.8423493044822257, 0.8408037094281299, 0.8377125193199382, 0.8578052550231839, 0.8487654320987654, 0.8503086419753086, 0.8194444444444444, 0.8333333333333334, 0.8348765432098766, 0.8454404945904173, 0.8408037094281299, 0.8516228748068007, 0.8423493044822257, 0.8562596599690881, 0.8518518518518519, 0.8441358024691358, 0.8179012345679012, 0.8333333333333334, 0.8364197530864198, 0.8454404945904173, 0.8408037094281299, 0.8469860896445132, 0.8438948995363215, 0.8438948995363215, 0.8302469135802469, 0.8425925925925926, 0.8287037037037037, 0.816358024691358, 0.8333333333333334, 0.8346213292117465, 0.8207109737248841, 0.8469860896445132, 0.8377125193199382, 0.8268933539412674, 0.8333333333333334, 0.8518518518518519, 0.8333333333333334, 0.8117283950617284, 0.8410493827160493, 0.8408037094281299, 0.8222565687789799, 0.8531684698608965, 0.8361669242658424, 0.8299845440494591, 0.8271604938271605, 0.8425925925925926, 0.8287037037037037, 0.8117283950617284, 0.8487654320987654, 0.8268933539412674, 0.8083462132921174, 0.8377125193199382, 0.8469860896445132, 0.8253477588871716, 0.8256172839506173, 0.8425925925925926, 0.8302469135802469, 0.8148148148148148, 0.8410493827160493, 0.8176197836166924, 0.80370942812983, 0.8238021638330757, 0.8253477588871716, 0.8299845440494591, 0.8179012345679012, 0.8580246913580247, 0.8256172839506173, 0.8148148148148148, 0.8472222222222222, 0.8268933539412674, 0.8068006182380216, 0.8299845440494591, 0.8330757341576507, 0.8268933539412674, 0.8209876543209876, 0.8503086419753086, 0.8148148148148148, 0.8209876543209876, 0.8364197530864198, 0.8160741885625966, 0.80370942812983, 0.8299845440494591, 0.8222565687789799, 0.8268933539412674, 0.816358024691358, 0.8472222222222222, 0.8132716049382716, 0.8209876543209876, 0.8441358024691358, 0.8176197836166924, 0.7990726429675425, 0.8129829984544049, 0.8222565687789799, 0.8253477588871716, 0.8055555555555556, 0.8395061728395061, 0.8179012345679012, 0.8117283950617284, 0.8348765432098766, 0.8222565687789799, 0.80370942812983, 0.8238021638330757, 0.8238021638330757, 0.8222565687789799, 0.808641975308642, 0.8472222222222222, 0.8132716049382716, 0.8070987654320988, 0.8348765432098766, 0.8145285935085008, 0.7897990726429676, 0.8160741885625966, 0.8160741885625966, 0.8145285935085008, 0.808641975308642, 0.8472222222222222, 0.8117283950617284, 0.8024691358024691, 0.8302469135802469, 0.8129829984544049, 0.7897990726429676, 0.8268933539412674, 0.8160741885625966, 0.8129829984544049, 0.7978395061728395, 0.8348765432098766, 0.8132716049382716, 0.7993827160493827, 0.8287037037037037, 0.8068006182380216, 0.7851622874806801, 0.8222565687789799, 0.8238021638330757, 0.8129829984544049, 0.8055555555555556, 0.8333333333333334, 0.8117283950617284, 0.7993827160493827, 0.8317901234567902, 0.8068006182380216, 0.7820710973724884, 0.8207109737248841, 0.8238021638330757, 0.8160741885625966, 0.808641975308642, 0.8287037037037037, 0.8101851851851852, 0.7947530864197531, 0.8333333333333334, 0.7975270479134466, 0.7836166924265843, 0.8238021638330757, 0.8253477588871716, 0.8114374034003091], 'Gaussian_Blur': [0.8271604938271605, 0.845679012345679, 0.7947530864197531, 0.8503086419753086, 0.8271604938271605, 0.8315301391035549, 0.8253477588871716, 0.8438948995363215, 0.8423493044822257, 0.8423493044822257, 0.8333333333333334, 0.8379629629629629, 0.8132716049382716, 0.8364197530864198, 0.8302469135802469, 0.8207109737248841, 0.8176197836166924, 0.839258114374034, 0.839258114374034, 0.8408037094281299, 0.8379629629629629, 0.816358024691358, 0.8333333333333334, 0.8271604938271605, 0.8287037037037037, 0.8253477588871716, 0.8238021638330757, 0.8361669242658424, 0.8299845440494591, 0.8253477588871716, 0.8271604938271605, 0.8302469135802469, 0.8271604938271605, 0.8132716049382716, 0.8410493827160493, 0.8191653786707882, 0.8098918083462133, 0.839258114374034, 0.8330757341576507, 0.8207109737248841, 0.8271604938271605, 0.8256172839506173, 0.8209876543209876, 0.8132716049382716, 0.8364197530864198, 0.8238021638330757, 0.8083462132921174, 0.8253477588871716, 0.8299845440494591, 0.8253477588871716, 0.8302469135802469, 0.8287037037037037, 0.816358024691358, 0.8101851851851852, 0.8348765432098766, 0.8145285935085008, 0.8083462132921174, 0.8268933539412674, 0.8191653786707882, 0.8052550231839258, 0.8302469135802469, 0.8225308641975309, 0.8287037037037037, 0.8055555555555556, 0.8410493827160493, 0.8222565687789799, 0.8006182380216383, 0.8098918083462133, 0.8191653786707882, 0.8238021638330757, 0.8240740740740741, 0.8209876543209876, 0.816358024691358, 0.8055555555555556, 0.8348765432098766, 0.8114374034003091, 0.8083462132921174, 0.80370942812983, 0.8145285935085008, 0.8145285935085008, 0.8179012345679012, 0.8101851851851852, 0.8148148148148148, 0.7947530864197531, 0.8395061728395061, 0.8114374034003091, 0.8083462132921174, 0.8068006182380216, 0.7975270479134466, 0.8098918083462133, 0.8117283950617284, 0.8101851851851852, 0.808641975308642, 0.7993827160493827, 0.8379629629629629, 0.8098918083462133, 0.7928902627511591, 0.8222565687789799, 0.8006182380216383, 0.8129829984544049, 0.8070987654320988, 0.8101851851851852, 0.8194444444444444, 0.7916666666666666, 0.8379629629629629, 0.80370942812983, 0.794435857805255, 0.8114374034003091, 0.8021638330757341, 0.8068006182380216, 0.8055555555555556, 0.8117283950617284, 0.8148148148148148, 0.7870370370370371, 0.8271604938271605, 0.8006182380216383, 0.7959814528593508, 0.8083462132921174, 0.7975270479134466, 0.7990726429675425, 0.8117283950617284, 0.816358024691358, 0.7978395061728395, 0.7978395061728395, 0.8240740740740741, 0.7959814528593508, 0.7836166924265843, 0.8021638330757341, 0.794435857805255, 0.794435857805255, 0.8070987654320988, 0.8225308641975309, 0.8009259259259259, 0.7854938271604939, 0.8209876543209876, 0.7975270479134466, 0.7867078825347759, 0.7959814528593508, 0.7913446676970634, 0.794435857805255, 0.8101851851851852, 0.8132716049382716, 0.7978395061728395, 0.7808641975308642, 0.8240740740740741, 0.7975270479134466, 0.7836166924265843, 0.80370942812983, 0.7851622874806801, 0.7897990726429676, 0.8148148148148148, 0.8194444444444444, 0.7993827160493827, 0.7870370370370371, 0.8240740740740741, 0.7897990726429676, 0.7805255023183926, 0.8114374034003091, 0.7836166924265843, 0.7975270479134466]}}\n",
      "{'SVM': {'None': [0.8616103834590064, 0.859822498593685, 0.8394558048813368, 0.8285276305013146, 0.8493069047716347, 0.8487342117670412, 0.840786569338059, 0.8753099275992996, 0.8491596960100324, 0.8387804429739912], 'Gaussian_Blur': [0.8505186648104418, 0.8508848680420984, 0.8338364173979976, 0.836526299660307, 0.8506410256410256, 0.8400949722653577, 0.8262026349075194, 0.860309724929608, 0.8387008338782335, 0.8268736343585669]}, 'RF': {'None': [0.8158539990707275, 0.8325888481066889, 0.8014426589153959, 0.7948863773172722, 0.8242358639311326, 0.8051574663428708, 0.8095856926045794, 0.8186035990482835, 0.8190598973278465, 0.805399285150064], 'Gaussian_Blur': [0.7838863622343976, 0.8187221935150176, 0.8026140857938056, 0.7862752562268588, 0.8176492588059295, 0.8237948841529287, 0.8019331703614404, 0.8228545335942595, 0.8023707110791545, 0.7948508860550502]}, 'KNN': {'None': [0.8549660747273289, 0.8592745677521894, 0.8136560014175886, 0.8393813589709112, 0.8477814819274289, 0.8242753984732349, 0.8377697159107104, 0.8425079172068924, 0.8413379721482701, 0.8589921992006619, 0.8440873015873015, 0.8466022464315396, 0.812574308613756, 0.8313957478576074, 0.8350098481733603, 0.8397153892565546, 0.838608157967654, 0.851392350238363, 0.8440990249699999, 0.8520381888935162, 0.8485210337015904, 0.8425215211235134, 0.8110352269562796, 0.8300100853611129, 0.836690327578879, 0.8452845268542198, 0.8379584003342796, 0.8494758351101636, 0.8452967562728494, 0.8398140479061461, 0.827544569280675, 0.8443489804643948, 0.8214749567834941, 0.8116493386904077, 0.8332956087525277, 0.8329928877142088, 0.8153184384827331, 0.8466927970238105, 0.8358367606314246, 0.8233327726158789, 0.827165971634699, 0.8519139514122913, 0.8241686116076918, 0.8100646893773383, 0.8406391136870175, 0.8336930478587813, 0.8176732592350441, 0.851958868664965, 0.8333052833500729, 0.8245512378784827, 0.8190956002839783, 0.8434747720737225, 0.8194335449146334, 0.8063536127966926, 0.8464445591498021, 0.822005177520097, 0.8025735937666774, 0.8350218926441771, 0.8404321350765128, 0.8232737870128622, 0.8183489690399942, 0.8450067225046372, 0.8204194298072923, 0.8085193580816273, 0.8385204400932725, 0.8094613070823079, 0.7970231989737618, 0.8217134659859119, 0.8177603546723248, 0.8276963607307475, 0.8101704272733078, 0.8576950311643318, 0.8156133704221157, 0.8069472196113007, 0.8451948892123564, 0.8210581757840685, 0.8009925041362767, 0.8269506871089986, 0.826658210999493, 0.8234936252246826, 0.8167213042742939, 0.8501738319392692, 0.8052570549814885, 0.8157886196857701, 0.8342911877394635, 0.8085907870284227, 0.7971102644885798, 0.8259142994373542, 0.8174415327253755, 0.8248860175523959, 0.812213541095368, 0.8476419768004201, 0.8036446440699767, 0.8167042695132581, 0.8423915791959189, 0.8097301579108627, 0.7922080278495945, 0.8082464580020622, 0.8159269652431865, 0.8228409609423143, 0.8012034243800494, 0.8412686001238665, 0.8073330430692615, 0.8060303408618016, 0.8328432487053177, 0.8150541087521148, 0.7958028307309567, 0.8185906632821984, 0.8167272397124408, 0.8175555751595723, 0.8035667646604326, 0.8465444149017136, 0.8030157209789751, 0.8008243336328712, 0.8318743464988524, 0.8073828471539929, 0.7831672151630095, 0.8100550767730731, 0.8101135537351452, 0.8086104504509155, 0.8029935237678979, 0.8498329595170078, 0.802499572224935, 0.7953107551793224, 0.8256118844967203, 0.8061846222072027, 0.7853601655827683, 0.8200890639415229, 0.8100105045350002, 0.8089359991694266, 0.7901504340072832, 0.8370759761580165, 0.8032922719826127, 0.7910913431191465, 0.8237981174592406, 0.8007247331809925, 0.7775912132978874, 0.8157734914620662, 0.8166613572716277, 0.8082071220612937, 0.7991163609376363, 0.8350704344701043, 0.8004735425812278, 0.7920518807610843, 0.8269334873145908, 0.8004739983563186, 0.7761625307399954, 0.8151106960011069, 0.8155683103366211, 0.8124289735946855, 0.8034797130731558, 0.8287835811823818, 0.7977314466399191, 0.7866189910001019, 0.8293152214270604, 0.7909381291403763, 0.7764952799067147, 0.8182093978279265, 0.8202981430089263, 0.8064389746047357], 'Gaussian_Blur': [0.8305679732689027, 0.8444328439040233, 0.7926489958412882, 0.8486696981942664, 0.8309577663391222, 0.8278147286154662, 0.8212235764143055, 0.8435253923650445, 0.8394120044336425, 0.8426221295383637, 0.8272416430952475, 0.8336619479833742, 0.8102081434362086, 0.8363668071376553, 0.8341083877781941, 0.8204431359016032, 0.8170067097847648, 0.8363398085447095, 0.838723343986502, 0.835396823264416, 0.8348529995533478, 0.8141706444280059, 0.8291510387890121, 0.8278324308183969, 0.8305259840414081, 0.8240510037961313, 0.8236382689101452, 0.8303486593887438, 0.8305656173510761, 0.820632715799491, 0.8231149823221312, 0.8266923304577519, 0.8240938318122369, 0.8124237576242376, 0.8433017618130269, 0.8158090985248281, 0.8058601234830743, 0.8342712915745608, 0.829085571021055, 0.8170093860039563, 0.8260298359651799, 0.8236759127953408, 0.8146815524246486, 0.8126559736461588, 0.836858735710042, 0.8204316218640275, 0.8048073747688638, 0.8216766224754196, 0.8238756685438888, 0.8188749416282844, 0.8315752696841522, 0.8285600366718859, 0.8073924121885753, 0.8089159883016176, 0.8336053311573507, 0.8100444743992349, 0.8058631504285628, 0.8252279295659304, 0.8118897198936131, 0.8032563526125758, 0.8295363071345867, 0.8221279896764345, 0.8154675568301119, 0.7995993129847904, 0.8387955182917693, 0.8174275916881157, 0.7955745477679482, 0.8071746145624775, 0.8120039682539683, 0.8199832995414355, 0.8229121807735459, 0.8215294231474544, 0.8058284491896238, 0.8005041017058567, 0.8329183704015319, 0.8057839721254355, 0.8042815939091842, 0.799487232843456, 0.8111038885712345, 0.8088835596702081, 0.8155831591027303, 0.8091784206892121, 0.803894119785844, 0.7880994640932505, 0.8361240151233079, 0.805896259692021, 0.8029455326051208, 0.8021580316973017, 0.7919099875168868, 0.803644206021116, 0.8098101514304391, 0.8086937494619334, 0.7977275126237832, 0.7978966890816644, 0.834073972810815, 0.8039762846639062, 0.7848232848232849, 0.8178817297845004, 0.7923471412436277, 0.8081126211967332, 0.8055283070384106, 0.8073936802090759, 0.8081403418325421, 0.7867774837144378, 0.8340251045648347, 0.7976040714498907, 0.7884333948163734, 0.805003537204965, 0.7934183299774699, 0.8019632902681989, 0.8033263310258616, 0.8059988737793446, 0.8038677468642291, 0.7824952839639653, 0.823155172746743, 0.7937479821041465, 0.7903014639014537, 0.801651371439479, 0.7899756784885371, 0.7939566027168469, 0.809728691069224, 0.8129724590814327, 0.7830477661201193, 0.7943472746510022, 0.8190102118109012, 0.7891780954123438, 0.7771368484407449, 0.7962430371821156, 0.7868370723237934, 0.7911783460498252, 0.8041380856502002, 0.8191570508138987, 0.7859917563197824, 0.7828076280590247, 0.8162908859035114, 0.7889361741036383, 0.780013333697872, 0.7880725378032182, 0.7839109291482725, 0.788300916973245, 0.8075700797686581, 0.8091652024779196, 0.7814653683038085, 0.7762641798563833, 0.820172819037455, 0.7897624877125278, 0.7743987255312237, 0.7970117958177863, 0.7762302273627255, 0.7810282206862937, 0.8134416286774108, 0.8161264286301998, 0.783035817188695, 0.7828988128108024, 0.8192732970185869, 0.7812273714038622, 0.7715714568655745, 0.8049417526648286, 0.7740928013225657, 0.78831988412994]}}\n",
      "{'SVM': {'None': [array([[ 94,  15,   5],\n",
      "       [  2, 294,  29],\n",
      "       [  4,  34, 171]]), array([[103,   7,   4],\n",
      "       [  7, 284,  34],\n",
      "       [  7,  33, 169]]), array([[ 93,  18,   3],\n",
      "       [  7, 284,  34],\n",
      "       [  8,  31, 170]]), array([[ 95,  14,   5],\n",
      "       [  9, 283,  33],\n",
      "       [  6,  43, 160]]), array([[104,   7,   3],\n",
      "       [  7, 288,  30],\n",
      "       [  6,  47, 156]]), array([[ 99,  11,   4],\n",
      "       [  5, 289,  30],\n",
      "       [  7,  41, 161]]), array([[ 98,   9,   7],\n",
      "       [  6, 289,  29],\n",
      "       [  9,  41, 159]]), array([[103,   5,   5],\n",
      "       [  5, 292,  27],\n",
      "       [  7,  32, 171]]), array([[ 98,  11,   4],\n",
      "       [  4, 290,  31],\n",
      "       [ 12,  33, 164]]), array([[ 93,  11,   9],\n",
      "       [  4, 294,  27],\n",
      "       [  6,  45, 158]])], 'Gaussian_Blur': [array([[ 96,  15,   3],\n",
      "       [  4, 289,  32],\n",
      "       [  4,  40, 165]]), array([[101,  10,   3],\n",
      "       [  9, 285,  31],\n",
      "       [  8,  35, 166]]), array([[ 94,  16,   4],\n",
      "       [ 11, 282,  32],\n",
      "       [  8,  33, 168]]), array([[101,   9,   4],\n",
      "       [  9, 280,  36],\n",
      "       [  4,  47, 158]]), array([[105,   5,   4],\n",
      "       [  6, 294,  25],\n",
      "       [  9,  48, 152]]), array([[ 97,   8,   9],\n",
      "       [  5, 290,  29],\n",
      "       [  9,  41, 159]]), array([[ 97,  11,   6],\n",
      "       [  6, 285,  33],\n",
      "       [  8,  48, 153]]), array([[100,   9,   4],\n",
      "       [  5, 292,  27],\n",
      "       [  6,  40, 164]]), array([[ 97,  12,   4],\n",
      "       [  5, 286,  34],\n",
      "       [ 12,  35, 162]]), array([[ 90,  14,   9],\n",
      "       [  4, 292,  29],\n",
      "       [  6,  47, 156]])]}, 'RF': {'None': [array([[ 93,  18,   3],\n",
      "       [  5, 296,  24],\n",
      "       [  7,  60, 142]]), array([[100,  12,   2],\n",
      "       [  5, 292,  28],\n",
      "       [  8,  54, 147]]), array([[ 87,  24,   3],\n",
      "       [  5, 291,  29],\n",
      "       [ 10,  52, 147]]), array([[ 95,  16,   3],\n",
      "       [  8, 288,  29],\n",
      "       [  7,  69, 133]]), array([[103,   7,   4],\n",
      "       [  7, 292,  26],\n",
      "       [  7,  64, 138]]), array([[ 95,  11,   8],\n",
      "       [  3, 289,  32],\n",
      "       [  9,  62, 138]]), array([[ 95,  14,   5],\n",
      "       [  6, 290,  28],\n",
      "       [  9,  59, 141]]), array([[ 94,  14,   5],\n",
      "       [  5, 292,  27],\n",
      "       [  9,  55, 146]]), array([[ 96,  11,   6],\n",
      "       [  3, 294,  28],\n",
      "       [ 14,  51, 144]]), array([[ 91,  14,   8],\n",
      "       [  3, 297,  25],\n",
      "       [  7,  66, 136]])], 'Gaussian_Blur': [array([[ 89,  23,   2],\n",
      "       [  4, 288,  33],\n",
      "       [  8,  68, 133]]), array([[100,  12,   2],\n",
      "       [  7, 286,  32],\n",
      "       [  9,  56, 144]]), array([[ 88,  24,   2],\n",
      "       [  5, 288,  32],\n",
      "       [  9,  52, 148]]), array([[ 92,  18,   4],\n",
      "       [  7, 283,  35],\n",
      "       [  5,  70, 134]]), array([[ 98,  13,   3],\n",
      "       [  8, 291,  26],\n",
      "       [  8,  59, 142]]), array([[ 99,   9,   6],\n",
      "       [  4, 293,  27],\n",
      "       [  6,  63, 140]]), array([[ 97,  12,   5],\n",
      "       [  8, 277,  39],\n",
      "       [  5,  62, 142]]), array([[ 92,  16,   5],\n",
      "       [  4, 303,  17],\n",
      "       [ 10,  57, 143]]), array([[ 93,  16,   4],\n",
      "       [  6, 287,  32],\n",
      "       [ 11,  56, 142]]), array([[ 91,  15,   7],\n",
      "       [  5, 295,  25],\n",
      "       [  8,  69, 132]])]}, 'KNN': {'None': [array([[ 95,  14,   5],\n",
      "       [  9, 291,  25],\n",
      "       [  3,  36, 170]]), array([[ 97,   9,   8],\n",
      "       [  7, 286,  32],\n",
      "       [  3,  32, 174]]), array([[ 93,  14,   7],\n",
      "       [ 11, 278,  36],\n",
      "       [  6,  45, 158]]), array([[ 98,  10,   6],\n",
      "       [  8, 287,  30],\n",
      "       [  2,  50, 157]]), array([[104,   4,   6],\n",
      "       [ 16, 276,  33],\n",
      "       [  5,  35, 169]]), array([[ 95,  12,   7],\n",
      "       [ 10, 288,  26],\n",
      "       [ 10,  44, 155]]), array([[ 93,  15,   6],\n",
      "       [ 10, 284,  30],\n",
      "       [  5,  36, 168]]), array([[ 99,   4,  10],\n",
      "       [  7, 280,  37],\n",
      "       [  6,  39, 165]]), array([[ 98,  11,   4],\n",
      "       [  6, 277,  42],\n",
      "       [  6,  36, 167]]), array([[ 95,  12,   6],\n",
      "       [  5, 289,  31],\n",
      "       [  2,  36, 171]]), array([[ 95,  14,   5],\n",
      "       [ 10, 292,  23],\n",
      "       [  5,  41, 163]]), array([[100,   9,   5],\n",
      "       [  8, 289,  28],\n",
      "       [ 10,  37, 162]]), array([[ 96,  12,   6],\n",
      "       [ 14, 278,  33],\n",
      "       [ 12,  40, 157]]), array([[ 98,  14,   2],\n",
      "       [  7, 293,  25],\n",
      "       [  8,  52, 149]]), array([[104,   6,   4],\n",
      "       [ 13, 282,  30],\n",
      "       [  8,  46, 155]]), array([[ 99,  11,   4],\n",
      "       [  6, 295,  23],\n",
      "       [ 12,  44, 153]]), array([[ 98,  12,   4],\n",
      "       [  8, 287,  29],\n",
      "       [  8,  42, 159]]), array([[101,   6,   6],\n",
      "       [  6, 290,  28],\n",
      "       [  8,  42, 160]]), array([[ 98,  12,   3],\n",
      "       [  2, 286,  37],\n",
      "       [  9,  39, 161]]), array([[ 95,  12,   6],\n",
      "       [  5, 295,  25],\n",
      "       [  7,  38, 164]]), array([[ 94,  14,   6],\n",
      "       [  4, 298,  23],\n",
      "       [  5,  44, 160]]), array([[102,   9,   3],\n",
      "       [  9, 290,  26],\n",
      "       [  9,  45, 155]]), array([[ 91,  19,   4],\n",
      "       [ 12, 283,  30],\n",
      "       [  8,  45, 156]]), array([[ 93,  19,   2],\n",
      "       [  7, 296,  22],\n",
      "       [  5,  53, 151]]), array([[105,   8,   1],\n",
      "       [  9, 292,  24],\n",
      "       [  9,  55, 145]]), array([[102,  10,   2],\n",
      "       [  6, 295,  23],\n",
      "       [  8,  51, 150]]), array([[ 97,  12,   5],\n",
      "       [ 10, 292,  22],\n",
      "       [  5,  49, 155]]), array([[101,   6,   6],\n",
      "       [  3, 292,  29],\n",
      "       [  7,  48, 155]]), array([[100,  12,   1],\n",
      "       [  3, 288,  34],\n",
      "       [ 10,  41, 158]]), array([[ 94,  13,   6],\n",
      "       [  3, 297,  25],\n",
      "       [  8,  46, 155]]), array([[ 91,  19,   4],\n",
      "       [  6, 292,  27],\n",
      "       [  4,  50, 155]]), array([[103,   9,   2],\n",
      "       [  8, 289,  28],\n",
      "       [  7,  48, 154]]), array([[ 92,  19,   3],\n",
      "       [  8, 296,  21],\n",
      "       [  8,  52, 149]]), array([[ 93,  16,   5],\n",
      "       [  8, 292,  25],\n",
      "       [  7,  58, 144]]), array([[103,   8,   3],\n",
      "       [  9, 291,  25],\n",
      "       [  8,  55, 146]]), array([[ 99,  10,   5],\n",
      "       [  6, 296,  22],\n",
      "       [  7,  57, 145]]), array([[ 98,  12,   4],\n",
      "       [  7, 295,  22],\n",
      "       [ 11,  60, 138]]), array([[ 99,   9,   5],\n",
      "       [  3, 299,  22],\n",
      "       [  7,  53, 150]]), array([[ 97,  13,   3],\n",
      "       [  1, 295,  29],\n",
      "       [ 11,  48, 150]]), array([[ 92,  15,   6],\n",
      "       [  3, 293,  29],\n",
      "       [  8,  51, 150]]), array([[ 89,  22,   3],\n",
      "       [  6, 297,  22],\n",
      "       [  5,  50, 154]]), array([[104,   9,   1],\n",
      "       [ 11, 290,  24],\n",
      "       [  7,  44, 158]]), array([[ 91,  19,   4],\n",
      "       [  7, 298,  20],\n",
      "       [  9,  49, 151]]), array([[ 94,  16,   4],\n",
      "       [  8, 291,  26],\n",
      "       [  5,  63, 141]]), array([[106,   5,   3],\n",
      "       [  8, 296,  21],\n",
      "       [  9,  57, 143]]), array([[101,   8,   5],\n",
      "       [  5, 300,  19],\n",
      "       [ 14,  52, 143]]), array([[ 95,  14,   5],\n",
      "       [  7, 294,  23],\n",
      "       [  8,  58, 143]]), array([[ 97,  12,   4],\n",
      "       [  2, 300,  22],\n",
      "       [  7,  48, 155]]), array([[ 95,  15,   3],\n",
      "       [  1, 294,  30],\n",
      "       [ 11,  46, 152]]), array([[ 92,  16,   5],\n",
      "       [  3, 297,  25],\n",
      "       [  9,  52, 148]]), array([[ 88,  23,   3],\n",
      "       [  7, 297,  21],\n",
      "       [  6,  52, 151]]), array([[104,   8,   2],\n",
      "       [ 10, 290,  25],\n",
      "       [  7,  50, 152]]), array([[ 87,  21,   6],\n",
      "       [  4, 299,  22],\n",
      "       [  7,  51, 151]]), array([[ 91,  20,   3],\n",
      "       [  7, 294,  24],\n",
      "       [  7,  61, 141]]), array([[107,   5,   2],\n",
      "       [  8, 299,  18],\n",
      "       [ 11,  54, 144]]), array([[ 99,  11,   4],\n",
      "       [  5, 296,  23],\n",
      "       [ 12,  57, 140]]), array([[ 94,  17,   3],\n",
      "       [  6, 292,  26],\n",
      "       [ 11,  61, 137]]), array([[ 95,  13,   5],\n",
      "       [  2, 299,  23],\n",
      "       [  8,  54, 148]]), array([[ 95,  15,   3],\n",
      "       [  2, 297,  26],\n",
      "       [ 13,  40, 156]]), array([[ 92,  16,   5],\n",
      "       [  1, 297,  27],\n",
      "       [  7,  57, 145]]), array([[ 90,  21,   3],\n",
      "       [  6, 299,  20],\n",
      "       [  7,  56, 146]]), array([[103,   8,   3],\n",
      "       [  6, 294,  25],\n",
      "       [  6,  54, 149]]), array([[ 87,  22,   5],\n",
      "       [  4, 302,  19],\n",
      "       [  7,  53, 149]]), array([[ 91,  19,   4],\n",
      "       [  7, 296,  22],\n",
      "       [  7,  61, 141]]), array([[105,   4,   5],\n",
      "       [  7, 299,  19],\n",
      "       [ 10,  58, 141]]), array([[ 97,  12,   5],\n",
      "       [  5, 297,  22],\n",
      "       [ 14,  60, 135]]), array([[ 93,  18,   3],\n",
      "       [  6, 293,  25],\n",
      "       [ 11,  64, 134]]), array([[ 96,  12,   5],\n",
      "       [  4, 296,  24],\n",
      "       [  8,  61, 141]]), array([[ 95,  14,   4],\n",
      "       [  3, 295,  27],\n",
      "       [ 15,  50, 144]]), array([[ 91,  16,   6],\n",
      "       [  1, 297,  27],\n",
      "       [  6,  54, 149]]), array([[ 89,  22,   3],\n",
      "       [  5, 301,  19],\n",
      "       [  7,  62, 140]]), array([[103,   8,   3],\n",
      "       [  7, 300,  18],\n",
      "       [  6,  50, 153]]), array([[ 88,  22,   4],\n",
      "       [  2, 305,  18],\n",
      "       [  9,  58, 142]]), array([[ 90,  20,   4],\n",
      "       [  7, 299,  19],\n",
      "       [  7,  63, 139]]), array([[104,   7,   3],\n",
      "       [  7, 300,  18],\n",
      "       [  9,  55, 145]]), array([[ 97,  13,   4],\n",
      "       [  4, 300,  20],\n",
      "       [ 11,  60, 138]]), array([[ 96,  16,   2],\n",
      "       [  6, 296,  22],\n",
      "       [ 11,  68, 130]]), array([[ 97,  10,   6],\n",
      "       [  3, 299,  22],\n",
      "       [  9,  60, 141]]), array([[ 95,  14,   4],\n",
      "       [  2, 298,  25],\n",
      "       [ 13,  50, 146]]), array([[ 90,  16,   7],\n",
      "       [  1, 298,  26],\n",
      "       [  6,  56, 147]]), array([[ 92,  19,   3],\n",
      "       [  5, 299,  21],\n",
      "       [  6,  62, 141]]), array([[103,   8,   3],\n",
      "       [  8, 298,  19],\n",
      "       [  6,  53, 150]]), array([[ 88,  22,   4],\n",
      "       [  2, 304,  19],\n",
      "       [  9,  64, 136]]), array([[ 93,  19,   2],\n",
      "       [  6, 303,  16],\n",
      "       [  6,  67, 136]]), array([[103,   9,   2],\n",
      "       [  5, 301,  19],\n",
      "       [ 10,  61, 138]]), array([[ 95,  15,   4],\n",
      "       [  4, 299,  21],\n",
      "       [ 12,  63, 134]]), array([[ 94,  17,   3],\n",
      "       [  5, 296,  23],\n",
      "       [ 11,  68, 130]]), array([[ 97,  11,   5],\n",
      "       [  5, 299,  20],\n",
      "       [  9,  60, 141]]), array([[ 95,  13,   5],\n",
      "       [  1, 295,  29],\n",
      "       [ 13,  54, 142]]), array([[ 92,  15,   6],\n",
      "       [  1, 299,  25],\n",
      "       [  6,  59, 144]]), array([[ 93,  18,   3],\n",
      "       [  5, 298,  22],\n",
      "       [  7,  64, 138]]), array([[104,   8,   2],\n",
      "       [  9, 297,  19],\n",
      "       [  6,  55, 148]]), array([[ 87,  22,   5],\n",
      "       [  2, 304,  19],\n",
      "       [  8,  65, 136]]), array([[ 94,  18,   2],\n",
      "       [  6, 302,  17],\n",
      "       [  6,  67, 136]]), array([[104,   8,   2],\n",
      "       [  5, 304,  16],\n",
      "       [  9,  61, 139]]), array([[ 94,  16,   4],\n",
      "       [  4, 301,  19],\n",
      "       [ 11,  64, 134]]), array([[ 93,  18,   3],\n",
      "       [  5, 295,  24],\n",
      "       [ 11,  69, 129]]), array([[ 94,  15,   4],\n",
      "       [  6, 295,  23],\n",
      "       [  9,  64, 137]]), array([[ 93,  14,   6],\n",
      "       [  1, 295,  29],\n",
      "       [ 13,  52, 144]]), array([[ 92,  14,   7],\n",
      "       [  1, 300,  24],\n",
      "       [  6,  61, 142]]), array([[ 93,  19,   2],\n",
      "       [  6, 295,  24],\n",
      "       [  8,  67, 134]]), array([[104,   8,   2],\n",
      "       [  8, 295,  22],\n",
      "       [  6,  58, 145]]), array([[ 87,  22,   5],\n",
      "       [  2, 304,  19],\n",
      "       [  9,  61, 139]]), array([[ 94,  17,   3],\n",
      "       [  5, 304,  16],\n",
      "       [  7,  74, 128]]), array([[104,   7,   3],\n",
      "       [  4, 303,  18],\n",
      "       [ 10,  65, 134]]), array([[ 94,  15,   5],\n",
      "       [  3, 300,  21],\n",
      "       [ 11,  60, 138]]), array([[ 93,  18,   3],\n",
      "       [  5, 298,  21],\n",
      "       [ 11,  69, 129]]), array([[ 94,  15,   4],\n",
      "       [  5, 298,  21],\n",
      "       [  9,  60, 141]]), array([[ 93,  15,   5],\n",
      "       [  1, 298,  26],\n",
      "       [ 13,  54, 142]]), array([[ 90,  15,   8],\n",
      "       [  1, 302,  22],\n",
      "       [  6,  63, 140]]), array([[ 92,  19,   3],\n",
      "       [  5, 299,  21],\n",
      "       [  7,  69, 133]]), array([[103,  10,   1],\n",
      "       [  9, 297,  19],\n",
      "       [  7,  53, 149]]), array([[ 87,  22,   5],\n",
      "       [  3, 304,  18],\n",
      "       [  8,  65, 136]]), array([[ 92,  20,   2],\n",
      "       [  5, 303,  17],\n",
      "       [  7,  74, 128]]), array([[102,  10,   2],\n",
      "       [  4, 304,  17],\n",
      "       [ 10,  64, 135]]), array([[ 95,  14,   5],\n",
      "       [  3, 299,  22],\n",
      "       [ 12,  64, 133]]), array([[ 94,  17,   3],\n",
      "       [  5, 295,  24],\n",
      "       [ 11,  76, 122]]), array([[ 94,  16,   3],\n",
      "       [  6, 297,  21],\n",
      "       [ 10,  63, 137]]), array([[ 94,  14,   5],\n",
      "       [  2, 295,  28],\n",
      "       [ 13,  57, 139]]), array([[ 90,  15,   8],\n",
      "       [  1, 303,  21],\n",
      "       [  7,  68, 134]]), array([[ 91,  20,   3],\n",
      "       [  6, 297,  22],\n",
      "       [  7,  66, 136]]), array([[104,  10,   0],\n",
      "       [  5, 300,  20],\n",
      "       [  6,  58, 145]]), array([[ 88,  22,   4],\n",
      "       [  4, 302,  19],\n",
      "       [  8,  65, 136]]), array([[ 91,  19,   4],\n",
      "       [  5, 302,  18],\n",
      "       [  7,  75, 127]]), array([[100,  11,   3],\n",
      "       [  4, 305,  16],\n",
      "       [ 10,  66, 133]]), array([[ 95,  15,   4],\n",
      "       [  3, 301,  20],\n",
      "       [ 11,  68, 130]]), array([[ 95,  16,   3],\n",
      "       [  5, 291,  28],\n",
      "       [ 11,  73, 125]]), array([[ 95,  16,   2],\n",
      "       [  6, 302,  16],\n",
      "       [ 10,  62, 138]]), array([[ 94,  15,   4],\n",
      "       [  2, 296,  27],\n",
      "       [ 13,  58, 138]]), array([[ 91,  16,   6],\n",
      "       [  1, 301,  23],\n",
      "       [  7,  68, 134]]), array([[ 88,  24,   2],\n",
      "       [  6, 298,  21],\n",
      "       [  7,  71, 131]]), array([[104,   9,   1],\n",
      "       [  5, 298,  22],\n",
      "       [  7,  63, 139]]), array([[ 87,  23,   4],\n",
      "       [  3, 304,  18],\n",
      "       [  8,  65, 136]]), array([[ 91,  19,   4],\n",
      "       [  6, 303,  16],\n",
      "       [  7,  78, 124]]), array([[100,  11,   3],\n",
      "       [  5, 304,  16],\n",
      "       [ 10,  66, 133]]), array([[ 94,  16,   4],\n",
      "       [  4, 298,  22],\n",
      "       [ 10,  69, 130]]), array([[ 93,  17,   4],\n",
      "       [  5, 293,  26],\n",
      "       [ 12,  75, 122]]), array([[ 95,  16,   2],\n",
      "       [  6, 301,  17],\n",
      "       [ 10,  64, 136]]), array([[ 94,  15,   4],\n",
      "       [  2, 299,  24],\n",
      "       [ 13,  56, 140]]), array([[ 91,  16,   6],\n",
      "       [  1, 303,  21],\n",
      "       [  7,  70, 132]]), array([[ 89,  23,   2],\n",
      "       [  6, 299,  20],\n",
      "       [  6,  69, 134]]), array([[103,   9,   2],\n",
      "       [  6, 298,  21],\n",
      "       [  6,  64, 139]]), array([[ 86,  24,   4],\n",
      "       [  3, 303,  19],\n",
      "       [  9,  63, 137]]), array([[ 92,  18,   4],\n",
      "       [  6, 302,  17],\n",
      "       [  7,  78, 124]]), array([[100,  11,   3],\n",
      "       [  5, 304,  16],\n",
      "       [ 10,  64, 135]]), array([[ 94,  16,   4],\n",
      "       [  3, 298,  23],\n",
      "       [ 11,  68, 130]]), array([[ 94,  16,   4],\n",
      "       [  5, 292,  27],\n",
      "       [ 11,  78, 120]]), array([[ 96,  15,   2],\n",
      "       [  6, 300,  18],\n",
      "       [ 10,  65, 135]]), array([[ 94,  13,   6],\n",
      "       [  3, 299,  23],\n",
      "       [ 13,  56, 140]]), array([[ 92,  16,   5],\n",
      "       [  1, 305,  19],\n",
      "       [  6,  72, 131]]), array([[ 90,  22,   2],\n",
      "       [  5, 299,  21],\n",
      "       [  6,  68, 135]]), array([[102,  10,   2],\n",
      "       [  7, 297,  21],\n",
      "       [  7,  64, 138]]), array([[ 86,  24,   4],\n",
      "       [  3, 304,  18],\n",
      "       [ 10,  64, 135]]), array([[ 91,  18,   5],\n",
      "       [  7, 300,  18],\n",
      "       [  7,  78, 124]]), array([[101,  10,   3],\n",
      "       [  5, 303,  17],\n",
      "       [ 10,  63, 136]]), array([[ 94,  16,   4],\n",
      "       [  3, 298,  23],\n",
      "       [ 11,  74, 124]]), array([[ 92,  17,   5],\n",
      "       [  5, 294,  25],\n",
      "       [ 10,  78, 121]]), array([[ 96,  15,   2],\n",
      "       [  6, 300,  18],\n",
      "       [ 10,  63, 137]]), array([[ 97,  12,   4],\n",
      "       [  2, 298,  25],\n",
      "       [ 13,  57, 139]]), array([[ 92,  16,   5],\n",
      "       [  2, 304,  19],\n",
      "       [  7,  73, 129]])], 'Gaussian_Blur': [array([[ 94,  17,   3],\n",
      "       [  6, 280,  39],\n",
      "       [  3,  44, 162]]), array([[ 97,  10,   7],\n",
      "       [  5, 292,  28],\n",
      "       [  6,  44, 159]]), array([[ 92,  15,   7],\n",
      "       [  8, 276,  41],\n",
      "       [  9,  53, 147]]), array([[ 96,  14,   4],\n",
      "       [ 10, 289,  26],\n",
      "       [  3,  40, 166]]), array([[101,   8,   5],\n",
      "       [ 10, 274,  41],\n",
      "       [  6,  42, 161]]), array([[ 96,  10,   8],\n",
      "       [  9, 284,  31],\n",
      "       [  8,  43, 158]]), array([[ 92,  14,   8],\n",
      "       [  9, 278,  37],\n",
      "       [  7,  38, 164]]), array([[ 98,   6,   9],\n",
      "       [  8, 283,  33],\n",
      "       [  6,  39, 165]]), array([[ 92,  15,   6],\n",
      "       [  5, 282,  38],\n",
      "       [  7,  31, 171]]), array([[ 93,  11,   9],\n",
      "       [  3, 288,  34],\n",
      "       [  4,  41, 164]]), array([[ 91,  21,   2],\n",
      "       [  7, 290,  28],\n",
      "       [  8,  42, 159]]), array([[ 99,  10,   5],\n",
      "       [  6, 287,  32],\n",
      "       [ 13,  39, 157]]), array([[ 95,  15,   4],\n",
      "       [ 11, 279,  35],\n",
      "       [  9,  47, 153]]), array([[ 99,  12,   3],\n",
      "       [  9, 288,  28],\n",
      "       [  6,  48, 155]]), array([[106,   6,   2],\n",
      "       [ 10, 281,  34],\n",
      "       [  8,  50, 151]]), array([[101,  11,   2],\n",
      "       [  7, 285,  32],\n",
      "       [ 11,  53, 145]]), array([[101,   9,   4],\n",
      "       [  7, 285,  32],\n",
      "       [ 11,  55, 143]]), array([[ 98,   9,   6],\n",
      "       [  9, 285,  30],\n",
      "       [  9,  41, 160]]), array([[ 96,  15,   2],\n",
      "       [  4, 286,  35],\n",
      "       [  9,  39, 161]]), array([[ 93,  13,   7],\n",
      "       [  6, 291,  28],\n",
      "       [  8,  41, 160]]), array([[ 94,  17,   3],\n",
      "       [  4, 300,  21],\n",
      "       [  6,  54, 149]]), array([[ 97,  14,   3],\n",
      "       [  6, 289,  30],\n",
      "       [ 10,  56, 143]]), array([[ 98,  13,   3],\n",
      "       [  9, 293,  23],\n",
      "       [  9,  51, 149]]), array([[ 96,  14,   4],\n",
      "       [  8, 288,  29],\n",
      "       [  4,  53, 152]]), array([[103,   9,   2],\n",
      "       [  7, 290,  28],\n",
      "       [  8,  57, 144]]), array([[101,  12,   1],\n",
      "       [  6, 291,  27],\n",
      "       [ 11,  56, 142]]), array([[ 98,  10,   6],\n",
      "       [  7, 293,  24],\n",
      "       [  5,  62, 142]]), array([[ 95,  11,   7],\n",
      "       [  6, 293,  25],\n",
      "       [ 10,  47, 153]]), array([[ 97,  15,   1],\n",
      "       [  3, 287,  35],\n",
      "       [ 10,  46, 153]]), array([[ 95,  12,   6],\n",
      "       [  4, 293,  28],\n",
      "       [ 11,  52, 146]]), array([[ 90,  21,   3],\n",
      "       [  4, 298,  23],\n",
      "       [  5,  56, 148]]), array([[ 98,  13,   3],\n",
      "       [  9, 291,  25],\n",
      "       [  9,  51, 149]]), array([[ 95,  15,   4],\n",
      "       [  8, 290,  27],\n",
      "       [  7,  51, 151]]), array([[ 93,  15,   6],\n",
      "       [  8, 287,  30],\n",
      "       [  4,  58, 147]]), array([[101,  11,   2],\n",
      "       [  6, 295,  24],\n",
      "       [  5,  55, 149]]), array([[ 98,  12,   4],\n",
      "       [  5, 295,  24],\n",
      "       [ 10,  62, 137]]), array([[ 97,  11,   6],\n",
      "       [  4, 298,  22],\n",
      "       [  9,  71, 129]]), array([[ 96,  11,   6],\n",
      "       [  8, 291,  25],\n",
      "       [  9,  45, 156]]), array([[ 95,  15,   3],\n",
      "       [  3, 294,  28],\n",
      "       [ 11,  48, 150]]), array([[ 93,  13,   7],\n",
      "       [  3, 293,  29],\n",
      "       [  9,  55, 145]]), array([[ 93,  19,   2],\n",
      "       [  4, 296,  25],\n",
      "       [  5,  57, 147]]), array([[ 98,  12,   4],\n",
      "       [  8, 290,  27],\n",
      "       [  8,  54, 147]]), array([[ 92,  19,   3],\n",
      "       [ 10, 289,  26],\n",
      "       [  8,  50, 151]]), array([[ 93,  15,   6],\n",
      "       [  9, 287,  29],\n",
      "       [  3,  59, 147]]), array([[100,  11,   3],\n",
      "       [  6, 296,  23],\n",
      "       [  6,  57, 146]]), array([[ 99,   9,   6],\n",
      "       [  4, 297,  23],\n",
      "       [ 10,  62, 137]]), array([[ 96,  11,   7],\n",
      "       [  4, 297,  23],\n",
      "       [  8,  71, 130]]), array([[ 97,  11,   5],\n",
      "       [  7, 291,  26],\n",
      "       [ 10,  54, 146]]), array([[ 94,  16,   3],\n",
      "       [  1, 298,  26],\n",
      "       [ 13,  51, 145]]), array([[ 93,  13,   7],\n",
      "       [  2, 298,  25],\n",
      "       [ 11,  55, 143]]), array([[ 97,  15,   2],\n",
      "       [  5, 297,  23],\n",
      "       [  4,  61, 144]]), array([[ 99,  14,   1],\n",
      "       [  8, 291,  26],\n",
      "       [  7,  55, 147]]), array([[ 88,  19,   7],\n",
      "       [  7, 295,  23],\n",
      "       [  7,  56, 146]]), array([[ 92,  16,   6],\n",
      "       [  7, 292,  26],\n",
      "       [  3,  65, 141]]), array([[100,  11,   3],\n",
      "       [  6, 296,  23],\n",
      "       [  8,  56, 145]]), array([[ 97,  11,   6],\n",
      "       [  6, 296,  22],\n",
      "       [  9,  66, 134]]), array([[100,  10,   4],\n",
      "       [  5, 295,  24],\n",
      "       [ 10,  71, 128]]), array([[ 97,  12,   4],\n",
      "       [  5, 295,  24],\n",
      "       [  8,  59, 143]]), array([[ 93,  16,   4],\n",
      "       [  3, 293,  29],\n",
      "       [ 14,  51, 144]]), array([[ 93,  15,   5],\n",
      "       [  3, 293,  29],\n",
      "       [  8,  66, 135]]), array([[ 94,  18,   2],\n",
      "       [  3, 299,  23],\n",
      "       [  5,  59, 145]]), array([[ 98,  14,   2],\n",
      "       [  6, 294,  25],\n",
      "       [  7,  61, 141]]), array([[ 87,  20,   7],\n",
      "       [  7, 300,  18],\n",
      "       [  9,  50, 150]]), array([[ 89,  19,   6],\n",
      "       [  6, 297,  22],\n",
      "       [  5,  68, 136]]), array([[100,  10,   4],\n",
      "       [  6, 298,  21],\n",
      "       [  8,  54, 147]]), array([[ 97,  12,   5],\n",
      "       [  5, 297,  22],\n",
      "       [ 10,  61, 138]]), array([[ 97,  11,   6],\n",
      "       [  5, 294,  25],\n",
      "       [ 11,  71, 127]]), array([[ 95,  13,   5],\n",
      "       [  5, 292,  27],\n",
      "       [  9,  64, 137]]), array([[ 94,  14,   5],\n",
      "       [  3, 294,  28],\n",
      "       [ 14,  53, 142]]), array([[ 93,  14,   6],\n",
      "       [  2, 299,  24],\n",
      "       [  8,  60, 141]]), array([[ 93,  18,   3],\n",
      "       [  3, 298,  24],\n",
      "       [  5,  61, 143]]), array([[ 99,  12,   3],\n",
      "       [  6, 292,  27],\n",
      "       [  7,  61, 141]]), array([[ 87,  20,   7],\n",
      "       [  6, 297,  22],\n",
      "       [  8,  56, 145]]), array([[ 89,  20,   5],\n",
      "       [  6, 295,  24],\n",
      "       [  5,  66, 138]]), array([[102,  10,   2],\n",
      "       [  7, 295,  23],\n",
      "       [ 10,  55, 144]]), array([[ 96,  13,   5],\n",
      "       [  4, 297,  23],\n",
      "       [ 11,  66, 132]]), array([[ 97,  13,   4],\n",
      "       [  5, 293,  26],\n",
      "       [ 11,  65, 133]]), array([[ 93,  15,   5],\n",
      "       [  5, 293,  26],\n",
      "       [  9,  67, 134]]), array([[ 96,  14,   3],\n",
      "       [  2, 295,  28],\n",
      "       [ 12,  61, 136]]), array([[ 92,  15,   6],\n",
      "       [  5, 297,  23],\n",
      "       [  8,  63, 138]]), array([[ 93,  18,   3],\n",
      "       [  3, 302,  20],\n",
      "       [  5,  69, 135]]), array([[100,  12,   2],\n",
      "       [  9, 291,  25],\n",
      "       [  8,  67, 134]]), array([[ 85,  22,   7],\n",
      "       [  6, 296,  23],\n",
      "       [  7,  55, 147]]), array([[ 88,  22,   4],\n",
      "       [  6, 296,  23],\n",
      "       [  6,  72, 131]]), array([[100,  11,   3],\n",
      "       [  6, 297,  22],\n",
      "       [ 10,  52, 147]]), array([[ 96,  13,   5],\n",
      "       [  5, 295,  24],\n",
      "       [ 11,  64, 134]]), array([[ 97,  13,   4],\n",
      "       [  6, 295,  23],\n",
      "       [ 11,  67, 131]]), array([[ 93,  14,   6],\n",
      "       [  5, 296,  23],\n",
      "       [  8,  69, 133]]), array([[ 93,  16,   4],\n",
      "       [  4, 292,  29],\n",
      "       [ 12,  66, 131]]), array([[ 91,  15,   7],\n",
      "       [  6, 297,  22],\n",
      "       [  7,  66, 136]]), array([[ 92,  19,   3],\n",
      "       [  3, 299,  23],\n",
      "       [  5,  69, 135]]), array([[ 99,  12,   3],\n",
      "       [  8, 292,  25],\n",
      "       [  8,  67, 134]]), array([[ 85,  22,   7],\n",
      "       [  4, 298,  23],\n",
      "       [  8,  60, 141]]), array([[ 91,  20,   3],\n",
      "       [  4, 294,  27],\n",
      "       [  5,  71, 133]]), array([[ 99,  12,   3],\n",
      "       [  6, 300,  19],\n",
      "       [  9,  56, 144]]), array([[ 96,  13,   5],\n",
      "       [  5, 296,  23],\n",
      "       [ 11,  66, 132]]), array([[ 92,  17,   5],\n",
      "       [  5, 295,  24],\n",
      "       [ 11,  72, 126]]), array([[ 95,  14,   4],\n",
      "       [  4, 302,  18],\n",
      "       [  8,  67, 135]]), array([[ 91,  17,   5],\n",
      "       [  4, 293,  28],\n",
      "       [ 13,  62, 134]]), array([[ 91,  15,   7],\n",
      "       [  2, 298,  25],\n",
      "       [  8,  64, 137]]), array([[ 92,  19,   3],\n",
      "       [  3, 298,  24],\n",
      "       [  5,  71, 133]]), array([[ 99,  12,   3],\n",
      "       [  9, 294,  22],\n",
      "       [  8,  69, 132]]), array([[ 86,  20,   8],\n",
      "       [  4, 300,  21],\n",
      "       [  8,  56, 145]]), array([[ 88,  21,   5],\n",
      "       [  4, 294,  27],\n",
      "       [  6,  72, 131]]), array([[100,  12,   2],\n",
      "       [  7, 301,  17],\n",
      "       [  9,  58, 142]]), array([[ 94,  14,   6],\n",
      "       [  5, 295,  24],\n",
      "       [ 10,  68, 131]]), array([[ 95,  14,   5],\n",
      "       [  5, 294,  25],\n",
      "       [ 11,  73, 125]]), array([[ 93,  15,   5],\n",
      "       [  4, 301,  19],\n",
      "       [  9,  70, 131]]), array([[ 91,  17,   5],\n",
      "       [  3, 296,  26],\n",
      "       [ 13,  64, 132]]), array([[ 91,  15,   7],\n",
      "       [  2, 298,  25],\n",
      "       [  8,  68, 133]]), array([[ 92,  19,   3],\n",
      "       [  5, 296,  24],\n",
      "       [  5,  70, 134]]), array([[ 98,  13,   3],\n",
      "       [ 10, 297,  18],\n",
      "       [  9,  69, 131]]), array([[ 86,  22,   6],\n",
      "       [  4, 301,  20],\n",
      "       [  8,  60, 141]]), array([[ 88,  21,   5],\n",
      "       [  4, 293,  28],\n",
      "       [  6,  74, 129]]), array([[ 99,  13,   2],\n",
      "       [  7, 297,  21],\n",
      "       [ 10,  59, 140]]), array([[ 95,  14,   5],\n",
      "       [  4, 296,  24],\n",
      "       [ 12,  70, 127]]), array([[ 96,  12,   6],\n",
      "       [  5, 294,  25],\n",
      "       [ 11,  73, 125]]), array([[ 92,  16,   5],\n",
      "       [  4, 300,  20],\n",
      "       [  9,  70, 131]]), array([[ 91,  17,   5],\n",
      "       [  3, 295,  27],\n",
      "       [ 12,  67, 130]]), array([[ 91,  18,   4],\n",
      "       [  2, 298,  25],\n",
      "       [  9,  72, 128]]), array([[ 93,  19,   2],\n",
      "       [  2, 301,  22],\n",
      "       [  6,  71, 132]]), array([[ 99,  12,   3],\n",
      "       [  8, 300,  17],\n",
      "       [  7,  72, 130]]), array([[ 80,  26,   8],\n",
      "       [  4, 300,  21],\n",
      "       [  7,  65, 137]]), array([[ 89,  22,   3],\n",
      "       [  4, 296,  25],\n",
      "       [  5,  72, 132]]), array([[ 97,  14,   3],\n",
      "       [  6, 298,  21],\n",
      "       [ 10,  60, 139]]), array([[ 95,  14,   5],\n",
      "       [  5, 294,  25],\n",
      "       [ 12,  71, 126]]), array([[ 92,  16,   6],\n",
      "       [  5, 292,  27],\n",
      "       [ 10,  76, 123]]), array([[ 93,  16,   4],\n",
      "       [  4, 302,  18],\n",
      "       [  8,  78, 124]]), array([[ 91,  17,   5],\n",
      "       [  3, 295,  27],\n",
      "       [ 12,  69, 128]]), array([[ 93,  15,   5],\n",
      "       [  3, 296,  26],\n",
      "       [  8,  76, 125]]), array([[ 92,  20,   2],\n",
      "       [  3, 300,  22],\n",
      "       [  6,  72, 131]]), array([[100,  12,   2],\n",
      "       [  8, 300,  17],\n",
      "       [  8,  68, 133]]), array([[ 81,  26,   7],\n",
      "       [  4, 301,  20],\n",
      "       [  8,  64, 137]]), array([[ 88,  21,   5],\n",
      "       [  4, 293,  28],\n",
      "       [  4,  77, 128]]), array([[ 96,  14,   4],\n",
      "       [  6, 297,  22],\n",
      "       [  9,  61, 139]]), array([[ 95,  14,   5],\n",
      "       [  6, 295,  23],\n",
      "       [ 13,  70, 126]]), array([[ 93,  15,   6],\n",
      "       [  5, 294,  25],\n",
      "       [ 10,  77, 122]]), array([[ 92,  16,   5],\n",
      "       [  4, 302,  18],\n",
      "       [  9,  80, 121]]), array([[ 91,  18,   4],\n",
      "       [  3, 295,  27],\n",
      "       [ 12,  71, 126]]), array([[ 91,  16,   6],\n",
      "       [  2, 299,  24],\n",
      "       [  9,  76, 124]]), array([[ 92,  20,   2],\n",
      "       [  2, 301,  22],\n",
      "       [  6,  71, 132]]), array([[ 98,  12,   4],\n",
      "       [  7, 299,  19],\n",
      "       [  8,  71, 130]]), array([[ 81,  25,   8],\n",
      "       [  4, 302,  19],\n",
      "       [  9,  66, 134]]), array([[ 87,  21,   6],\n",
      "       [  4, 293,  28],\n",
      "       [  5,  78, 126]]), array([[ 97,  13,   4],\n",
      "       [  7, 297,  21],\n",
      "       [  8,  61, 140]]), array([[ 95,  14,   5],\n",
      "       [  6, 295,  23],\n",
      "       [ 12,  71, 126]]), array([[ 91,  15,   8],\n",
      "       [  5, 294,  25],\n",
      "       [ 11,  76, 122]]), array([[ 93,  16,   4],\n",
      "       [  4, 304,  16],\n",
      "       [  8,  79, 123]]), array([[ 91,  19,   3],\n",
      "       [  2, 294,  29],\n",
      "       [ 15,  71, 123]]), array([[ 88,  19,   6],\n",
      "       [  3, 299,  23],\n",
      "       [  9,  76, 124]]), array([[ 93,  19,   2],\n",
      "       [  2, 302,  21],\n",
      "       [  5,  71, 133]]), array([[ 99,  13,   2],\n",
      "       [  7, 300,  18],\n",
      "       [  8,  69, 132]]), array([[ 80,  26,   8],\n",
      "       [  4, 302,  19],\n",
      "       [  8,  65, 136]]), array([[ 88,  22,   4],\n",
      "       [  4, 295,  26],\n",
      "       [  5,  77, 127]]), array([[ 97,  13,   4],\n",
      "       [  8, 298,  19],\n",
      "       [  8,  62, 139]]), array([[ 94,  15,   5],\n",
      "       [  6, 295,  23],\n",
      "       [ 12,  75, 122]]), array([[ 91,  15,   8],\n",
      "       [  5, 295,  24],\n",
      "       [ 10,  80, 119]]), array([[ 94,  15,   4],\n",
      "       [  5, 304,  15],\n",
      "       [  8,  75, 127]]), array([[ 89,  20,   4],\n",
      "       [  2, 294,  29],\n",
      "       [ 14,  71, 124]]), array([[ 90,  18,   5],\n",
      "       [  3, 302,  20],\n",
      "       [ 10,  75, 124]])]}}\n",
      "{'SVM': {'None': [0.8644464015960693, 0.8489458560943604, 0.8303875923156738, 0.8441228866577148, 0.8485069274902344, 0.839282751083374, 0.8272037506103516, 0.6879076957702637, 0.828066349029541, 0.8336968421936035], 'Gaussian_Blur': [0.8561840057373047, 0.8428502082824707, 0.8319203853607178, 0.8382830619812012, 0.8503255844116211, 0.8376994132995605, 0.843454122543335, 0.8452556133270264, 0.8406069278717041, 0.8344025611877441]}, 'RF': {'None': [8.02333950996399, 8.234793663024902, 7.9224653244018555, 7.868056774139404, 7.946845293045044, 7.8946311473846436, 8.139342308044434, 7.938580751419067, 7.8842244148254395, 7.963507890701294], 'Gaussian_Blur': [7.865200757980347, 8.013236045837402, 7.735918760299683, 7.921578645706177, 7.937133312225342, 7.952821254730225, 8.062299489974976, 7.963570833206177, 7.859736680984497, 7.938720703125]}, 'KNN': {'None': [0.0006351470947265625, 0.0006775856018066406, 0.0006842613220214844, 0.0007047653198242188, 0.0007569789886474609, 0.000762939453125, 0.0007746219635009766, 0.0007765293121337891, 0.0007507801055908203, 0.0007653236389160156, 0.0007317066192626953, 0.0007832050323486328, 0.0007824897766113281, 0.0007753372192382812, 0.0007531642913818359, 0.0007417201995849609, 0.0006737709045410156, 0.0006773471832275391, 0.0006530284881591797, 0.0007100105285644531, 0.0006303787231445312, 0.0006494522094726562, 0.0007059574127197266, 0.0007085800170898438, 0.0006859302520751953, 0.0006780624389648438, 0.0006587505340576172, 0.0006692409515380859, 0.0006413459777832031, 0.0006837844848632812, 0.0006680488586425781, 0.0006651878356933594, 0.0006606578826904297, 0.0006513595581054688, 0.0007166862487792969, 0.0006475448608398438, 0.0006775856018066406, 0.0006589889526367188, 0.0006842613220214844, 0.0006937980651855469, 0.0006415843963623047, 0.0006561279296875, 0.0006127357482910156, 0.0006368160247802734, 0.0006113052368164062, 0.0008175373077392578, 0.0007610321044921875, 0.0007011890411376953, 0.0006375312805175781, 0.0006656646728515625, 0.000820159912109375, 0.0006685256958007812, 0.0006580352783203125, 0.0006387233734130859, 0.0006902217864990234, 0.0007033348083496094, 0.0006489753723144531, 0.0006344318389892578, 0.0006186962127685547, 0.0006585121154785156, 0.0006301403045654297, 0.0006766319274902344, 0.0006616115570068359, 0.0006592273712158203, 0.0006725788116455078, 0.0006382465362548828, 0.0007259845733642578, 0.0007050037384033203, 0.0006222724914550781, 0.0006351470947265625, 0.0006344318389892578, 0.0006489753723144531, 0.0006284713745117188, 0.0006256103515625, 0.0006318092346191406, 0.00067138671875, 0.0007293224334716797, 0.0007131099700927734, 0.0007345676422119141, 0.0007302761077880859, 0.0007143020629882812, 0.0007259845733642578, 0.0007336139678955078, 0.0006456375122070312, 0.000614166259765625, 0.0006225109100341797, 0.0006520748138427734, 0.0006959438323974609, 0.0006825923919677734, 0.0007593631744384766, 0.0006673336029052734, 0.0006513595581054688, 0.0006272792816162109, 0.0006740093231201172, 0.0006208419799804688, 0.0006377696990966797, 0.0006577968597412109, 0.0006372928619384766, 0.0006601810455322266, 0.0006556510925292969, 0.0006365776062011719, 0.0008797645568847656, 0.0006511211395263672, 0.0006730556488037109, 0.0006515979766845703, 0.0008492469787597656, 0.0006277561187744141, 0.0006492137908935547, 0.0006196498870849609, 0.0006630420684814453, 0.0006456375122070312, 0.0006382465362548828, 0.0006225109100341797, 0.0007312297821044922, 0.0006556510925292969, 0.0006673336029052734, 0.0006978511810302734, 0.0006616115570068359, 0.000652313232421875, 0.0006949901580810547, 0.0006003379821777344, 0.0006210803985595703, 0.0006046295166015625, 0.0006539821624755859, 0.0006277561187744141, 0.0006380081176757812, 0.0006356239318847656, 0.0007634162902832031, 0.0007238388061523438, 0.0006465911865234375, 0.0006365776062011719, 0.0006906986236572266, 0.0006055831909179688, 0.0006117820739746094, 0.0006053447723388672, 0.0007073879241943359, 0.0006506443023681641, 0.0006096363067626953, 0.0006155967712402344, 0.0006535053253173828, 0.0006241798400878906, 0.0006475448608398438, 0.0006310939788818359, 0.0006070137023925781, 0.0006077289581298828, 0.0006201267242431641, 0.0007271766662597656, 0.0006093978881835938, 0.0006310939788818359, 0.0006682872772216797, 0.0006246566772460938, 0.0006248950958251953, 0.0006263256072998047, 0.0006392002105712891, 0.0007107257843017578, 0.0006504058837890625, 0.0006282329559326172, 0.0006079673767089844, 0.0006213188171386719, 0.0006213188171386719], 'Gaussian_Blur': [0.0005846023559570312, 0.0006473064422607422, 0.0006721019744873047, 0.0006544589996337891, 0.0007674694061279297, 0.0007410049438476562, 0.0006947517395019531, 0.0008375644683837891, 0.0006954669952392578, 0.0007185935974121094, 0.0007479190826416016, 0.0007445812225341797, 0.0007228851318359375, 0.0006701946258544922, 0.0006971359252929688, 0.0008003711700439453, 0.0007567405700683594, 0.000820159912109375, 0.0007996559143066406, 0.0008945465087890625, 0.0007476806640625, 0.0007965564727783203, 0.0006556510925292969, 0.0009229183197021484, 0.0006868839263916016, 0.0007746219635009766, 0.0007603168487548828, 0.0007174015045166016, 0.0007557868957519531, 0.0007293224334716797, 0.0007786750793457031, 0.0007319450378417969, 0.0006890296936035156, 0.0006968975067138672, 0.0007977485656738281, 0.0007009506225585938, 0.0007145404815673828, 0.0007643699645996094, 0.0008065700531005859, 0.0006494522094726562, 0.0008320808410644531, 0.0007302761077880859, 0.0008475780487060547, 0.0006859302520751953, 0.0007762908935546875, 0.0007171630859375, 0.0008633136749267578, 0.0007491111755371094, 0.00069427490234375, 0.0007138252258300781, 0.0007052421569824219, 0.0007395744323730469, 0.0007445812225341797, 0.0006840229034423828, 0.0007207393646240234, 0.0007781982421875, 0.0006635189056396484, 0.0007064342498779297, 0.0007581710815429688, 0.0006654262542724609, 0.0006902217864990234, 0.0007100105285644531, 0.0007987022399902344, 0.0007512569427490234, 0.0007216930389404297, 0.0008842945098876953, 0.0007443428039550781, 0.0006797313690185547, 0.0007154941558837891, 0.0006589889526367188, 0.0008232593536376953, 0.0007042884826660156, 0.0008137226104736328, 0.0006592273712158203, 0.0008878707885742188, 0.0006327629089355469, 0.0007433891296386719, 0.0007469654083251953, 0.0007073879241943359, 0.0006732940673828125, 0.0008287429809570312, 0.0006995201110839844, 0.0006945133209228516, 0.0006704330444335938, 0.0008151531219482422, 0.0007359981536865234, 0.0007121562957763672, 0.0006945133209228516, 0.000782012939453125, 0.0006794929504394531, 0.0006437301635742188, 0.000820159912109375, 0.0007627010345458984, 0.0006918907165527344, 0.0007998943328857422, 0.0007731914520263672, 0.0006923675537109375, 0.0007033348083496094, 0.0008151531219482422, 0.0007617473602294922, 0.0006926059722900391, 0.0006704330444335938, 0.0007796287536621094, 0.0006284713745117188, 0.0007121562957763672, 0.0007770061492919922, 0.0007302761077880859, 0.0009093284606933594, 0.0007266998291015625, 0.0006847381591796875, 0.0009162425994873047, 0.0006670951843261719, 0.0006797313690185547, 0.0007665157318115234, 0.0006878376007080078, 0.0006659030914306641, 0.0007593631744384766, 0.0007011890411376953, 0.0008230209350585938, 0.0006606578826904297, 0.0007579326629638672, 0.0007460117340087891, 0.0007636547088623047, 0.0006480216979980469, 0.0009224414825439453, 0.0006601810455322266, 0.0009436607360839844, 0.0006723403930664062, 0.0009012222290039062, 0.0006861686706542969, 0.0007297992706298828, 0.0007073879241943359, 0.0006761550903320312, 0.0007176399230957031, 0.0007221698760986328, 0.0007750988006591797, 0.0006802082061767578, 0.0007531642913818359, 0.0007777214050292969, 0.0006434917449951172, 0.0007200241088867188, 0.0006663799285888672, 0.0008122920989990234, 0.0007083415985107422, 0.0007791519165039062, 0.0006842613220214844, 0.0007929801940917969, 0.0006673336029052734, 0.0011017322540283203, 0.0007598400115966797, 0.0007319450378417969, 0.0007205009460449219, 0.0007066726684570312, 0.0007588863372802734, 0.0006859302520751953, 0.0007078647613525391, 0.0007698535919189453, 0.0007264614105224609, 0.0007584095001220703, 0.0006821155548095703]}}\n",
      "5332\n",
      "2286\n",
      "1143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/falzgrandma/venvs/torch_rocm/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Median_Blur accuracy: 0.8472222222222222\n",
      "SVM Median_Blur f1 score: 0.8482163031278521\n",
      "SVM Median_Blur classification time: 1.0398097038269043\n",
      "SVM Median_Blur accuracy: 0.8503086419753086\n",
      "SVM Median_Blur f1 score: 0.8527508361132329\n",
      "SVM Median_Blur classification time: 1.018547773361206\n",
      "SVM Median_Blur accuracy: 0.8240740740740741\n",
      "SVM Median_Blur f1 score: 0.8219670263148524\n",
      "SVM Median_Blur classification time: 1.0280635356903076\n",
      "SVM Median_Blur accuracy: 0.808641975308642\n",
      "SVM Median_Blur f1 score: 0.8121904308078699\n",
      "SVM Median_Blur classification time: 1.0348823070526123\n",
      "SVM Median_Blur accuracy: 0.8364197530864198\n",
      "SVM Median_Blur f1 score: 0.8364288494394673\n",
      "SVM Median_Blur classification time: 1.0584323406219482\n",
      "SVM Median_Blur accuracy: 0.8408037094281299\n",
      "SVM Median_Blur f1 score: 0.8406810275622156\n",
      "SVM Median_Blur classification time: 1.088325023651123\n",
      "SVM Median_Blur accuracy: 0.8129829984544049\n",
      "SVM Median_Blur f1 score: 0.8111673865797621\n",
      "SVM Median_Blur classification time: 1.1364688873291016\n",
      "SVM Median_Blur accuracy: 0.839258114374034\n",
      "SVM Median_Blur f1 score: 0.8378691059458325\n",
      "SVM Median_Blur classification time: 1.1041386127471924\n",
      "SVM Median_Blur accuracy: 0.8299845440494591\n",
      "SVM Median_Blur f1 score: 0.8276127157093924\n",
      "SVM Median_Blur classification time: 1.0705926418304443\n",
      "SVM Median_Blur accuracy: 0.8191653786707882\n",
      "SVM Median_Blur f1 score: 0.8176090855888162\n",
      "SVM Median_Blur classification time: 1.1047117710113525\n",
      "Random Forest Median_Blur accuracy: 0.8101851851851852\n",
      "Random Forest Median_Blur f1 score: 0.8057012753694877\n",
      "Random Forest Median_Blur classification time: 8.109986782073975\n",
      "Random Forest Median_Blur accuracy: 0.8009259259259259\n",
      "Random Forest Median_Blur f1 score: 0.801784152333946\n",
      "Random Forest Median_Blur classification time: 7.972768068313599\n",
      "Random Forest Median_Blur accuracy: 0.8024691358024691\n",
      "Random Forest Median_Blur f1 score: 0.794519618224078\n",
      "Random Forest Median_Blur classification time: 8.068957567214966\n",
      "Random Forest Median_Blur accuracy: 0.7623456790123457\n",
      "Random Forest Median_Blur f1 score: 0.7599787546024105\n",
      "Random Forest Median_Blur classification time: 7.8763158321380615\n",
      "Random Forest Median_Blur accuracy: 0.8009259259259259\n",
      "Random Forest Median_Blur f1 score: 0.7982079410140316\n",
      "Random Forest Median_Blur classification time: 7.9858715534210205\n",
      "Random Forest Median_Blur accuracy: 0.8114374034003091\n",
      "Random Forest Median_Blur f1 score: 0.8118107820311513\n",
      "Random Forest Median_Blur classification time: 7.951327800750732\n",
      "Random Forest Median_Blur accuracy: 0.7882534775888718\n",
      "Random Forest Median_Blur f1 score: 0.7912888390924602\n",
      "Random Forest Median_Blur classification time: 8.008968830108643\n",
      "Random Forest Median_Blur accuracy: 0.8052550231839258\n",
      "Random Forest Median_Blur f1 score: 0.7995955502338482\n",
      "Random Forest Median_Blur classification time: 8.324324369430542\n",
      "Random Forest Median_Blur accuracy: 0.8098918083462133\n",
      "Random Forest Median_Blur f1 score: 0.8048251942988784\n",
      "Random Forest Median_Blur classification time: 8.24072527885437\n",
      "Random Forest Median_Blur accuracy: 0.8145285935085008\n",
      "Random Forest Median_Blur f1 score: 0.8109723401615162\n",
      "Random Forest Median_Blur classification time: 8.468615293502808\n",
      "Best 1NN Median_Blur accuracy: 0.8209876543209876\n",
      "Best 1NN Median_Blur f1 score: 0.8231944825969398\n",
      "Best 1NN Median_Blur classification time: 0.0006077289581298828\n",
      "Best 1NN Median_Blur accuracy: 0.8179012345679012\n",
      "Best 1NN Median_Blur f1 score: 0.8207751415632072\n",
      "Best 1NN Median_Blur classification time: 0.0006835460662841797\n",
      "Best 1NN Median_Blur accuracy: 0.816358024691358\n",
      "Best 1NN Median_Blur f1 score: 0.8099348367891069\n",
      "Best 1NN Median_Blur classification time: 0.0007708072662353516\n",
      "Best 1NN Median_Blur accuracy: 0.8132716049382716\n",
      "Best 1NN Median_Blur f1 score: 0.8125799891011171\n",
      "Best 1NN Median_Blur classification time: 0.0008394718170166016\n",
      "Best 1NN Median_Blur accuracy: 0.8425925925925926\n",
      "Best 1NN Median_Blur f1 score: 0.8437759487179356\n",
      "Best 1NN Median_Blur classification time: 0.0006158351898193359\n",
      "Best 1NN Median_Blur accuracy: 0.8207109737248841\n",
      "Best 1NN Median_Blur f1 score: 0.8184550205320598\n",
      "Best 1NN Median_Blur classification time: 0.0006134510040283203\n",
      "Best 1NN Median_Blur accuracy: 0.80370942812983\n",
      "Best 1NN Median_Blur f1 score: 0.8046438042170271\n",
      "Best 1NN Median_Blur classification time: 0.0009589195251464844\n",
      "Best 1NN Median_Blur accuracy: 0.8207109737248841\n",
      "Best 1NN Median_Blur f1 score: 0.8198661705795335\n",
      "Best 1NN Median_Blur classification time: 0.0006666183471679688\n",
      "Best 1NN Median_Blur accuracy: 0.8346213292117465\n",
      "Best 1NN Median_Blur f1 score: 0.8359573065455418\n",
      "Best 1NN Median_Blur classification time: 0.0008704662322998047\n",
      "Best 1NN Median_Blur accuracy: 0.8438948995363215\n",
      "Best 1NN Median_Blur f1 score: 0.8405779053453467\n",
      "Best 1NN Median_Blur classification time: 0.0006322860717773438\n",
      "Best 3NN Median_Blur accuracy: 0.8518518518518519\n",
      "Best 3NN Median_Blur f1 score: 0.8462353023913344\n",
      "Best 3NN Median_Blur classification time: 0.0006015300750732422\n",
      "Best 3NN Median_Blur accuracy: 0.8240740740740741\n",
      "Best 3NN Median_Blur f1 score: 0.8230159439147068\n",
      "Best 3NN Median_Blur classification time: 0.0006334781646728516\n",
      "Best 3NN Median_Blur accuracy: 0.8070987654320988\n",
      "Best 3NN Median_Blur f1 score: 0.7954840258704491\n",
      "Best 3NN Median_Blur classification time: 0.0007483959197998047\n",
      "Best 3NN Median_Blur accuracy: 0.8040123456790124\n",
      "Best 3NN Median_Blur f1 score: 0.8009977908488337\n",
      "Best 3NN Median_Blur classification time: 0.0007760524749755859\n",
      "Best 3NN Median_Blur accuracy: 0.8364197530864198\n",
      "Best 3NN Median_Blur f1 score: 0.8311989166414717\n",
      "Best 3NN Median_Blur classification time: 0.0006988048553466797\n",
      "Best 3NN Median_Blur accuracy: 0.839258114374034\n",
      "Best 3NN Median_Blur f1 score: 0.8329175312426993\n",
      "Best 3NN Median_Blur classification time: 0.0006771087646484375\n",
      "Best 3NN Median_Blur accuracy: 0.8268933539412674\n",
      "Best 3NN Median_Blur f1 score: 0.8231848424981472\n",
      "Best 3NN Median_Blur classification time: 0.000667572021484375\n",
      "Best 3NN Median_Blur accuracy: 0.8315301391035549\n",
      "Best 3NN Median_Blur f1 score: 0.8286767348362504\n",
      "Best 3NN Median_Blur classification time: 0.0006997585296630859\n",
      "Best 3NN Median_Blur accuracy: 0.8222565687789799\n",
      "Best 3NN Median_Blur f1 score: 0.8146677134195667\n",
      "Best 3NN Median_Blur classification time: 0.0008459091186523438\n",
      "Best 3NN Median_Blur accuracy: 0.8299845440494591\n",
      "Best 3NN Median_Blur f1 score: 0.8229390887193254\n",
      "Best 3NN Median_Blur classification time: 0.0009553432464599609\n",
      "Best 5NN Median_Blur accuracy: 0.845679012345679\n",
      "Best 5NN Median_Blur f1 score: 0.8375907270364399\n",
      "Best 5NN Median_Blur classification time: 0.0006492137908935547\n",
      "Best 5NN Median_Blur accuracy: 0.816358024691358\n",
      "Best 5NN Median_Blur f1 score: 0.8163149397628828\n",
      "Best 5NN Median_Blur classification time: 0.0007352828979492188\n",
      "Best 5NN Median_Blur accuracy: 0.8225308641975309\n",
      "Best 5NN Median_Blur f1 score: 0.8133549204914843\n",
      "Best 5NN Median_Blur classification time: 0.0006954669952392578\n",
      "Best 5NN Median_Blur accuracy: 0.8240740740740741\n",
      "Best 5NN Median_Blur f1 score: 0.8220481436148511\n",
      "Best 5NN Median_Blur classification time: 0.0007314682006835938\n",
      "Best 5NN Median_Blur accuracy: 0.8317901234567902\n",
      "Best 5NN Median_Blur f1 score: 0.8290869548403118\n",
      "Best 5NN Median_Blur classification time: 0.000705718994140625\n",
      "Best 5NN Median_Blur accuracy: 0.8330757341576507\n",
      "Best 5NN Median_Blur f1 score: 0.8299006655339051\n",
      "Best 5NN Median_Blur classification time: 0.0006945133209228516\n",
      "Best 5NN Median_Blur accuracy: 0.8253477588871716\n",
      "Best 5NN Median_Blur f1 score: 0.8213709027108496\n",
      "Best 5NN Median_Blur classification time: 0.000728607177734375\n",
      "Best 5NN Median_Blur accuracy: 0.8438948995363215\n",
      "Best 5NN Median_Blur f1 score: 0.8386356462838157\n",
      "Best 5NN Median_Blur classification time: 0.0006811618804931641\n",
      "Best 5NN Median_Blur accuracy: 0.8191653786707882\n",
      "Best 5NN Median_Blur f1 score: 0.8149781701227244\n",
      "Best 5NN Median_Blur classification time: 0.0006346702575683594\n",
      "Best 5NN Median_Blur accuracy: 0.8361669242658424\n",
      "Best 5NN Median_Blur f1 score: 0.8362624057234802\n",
      "Best 5NN Median_Blur classification time: 0.0006196498870849609\n",
      "Best 7NN Median_Blur accuracy: 0.8395061728395061\n",
      "Best 7NN Median_Blur f1 score: 0.8326848455112502\n",
      "Best 7NN Median_Blur classification time: 0.0007224082946777344\n",
      "Best 7NN Median_Blur accuracy: 0.8209876543209876\n",
      "Best 7NN Median_Blur f1 score: 0.8196145925617474\n",
      "Best 7NN Median_Blur classification time: 0.0008594989776611328\n",
      "Best 7NN Median_Blur accuracy: 0.8179012345679012\n",
      "Best 7NN Median_Blur f1 score: 0.809726682752356\n",
      "Best 7NN Median_Blur classification time: 0.0007240772247314453\n",
      "Best 7NN Median_Blur accuracy: 0.8317901234567902\n",
      "Best 7NN Median_Blur f1 score: 0.8318349292379037\n",
      "Best 7NN Median_Blur classification time: 0.0007750988006591797\n",
      "Best 7NN Median_Blur accuracy: 0.8256172839506173\n",
      "Best 7NN Median_Blur f1 score: 0.8200944599074539\n",
      "Best 7NN Median_Blur classification time: 0.0006933212280273438\n",
      "Best 7NN Median_Blur accuracy: 0.8284389489953632\n",
      "Best 7NN Median_Blur f1 score: 0.8292356206029075\n",
      "Best 7NN Median_Blur classification time: 0.0008268356323242188\n",
      "Best 7NN Median_Blur accuracy: 0.8068006182380216\n",
      "Best 7NN Median_Blur f1 score: 0.8038216510357067\n",
      "Best 7NN Median_Blur classification time: 0.0006692409515380859\n",
      "Best 7NN Median_Blur accuracy: 0.8438948995363215\n",
      "Best 7NN Median_Blur f1 score: 0.838253152691841\n",
      "Best 7NN Median_Blur classification time: 0.0007884502410888672\n",
      "Best 7NN Median_Blur accuracy: 0.80370942812983\n",
      "Best 7NN Median_Blur f1 score: 0.7995699150257952\n",
      "Best 7NN Median_Blur classification time: 0.0008747577667236328\n",
      "Best 7NN Median_Blur accuracy: 0.8268933539412674\n",
      "Best 7NN Median_Blur f1 score: 0.8243819824900633\n",
      "Best 7NN Median_Blur classification time: 0.0007510185241699219\n",
      "Best 9NN Median_Blur accuracy: 0.8395061728395061\n",
      "Best 9NN Median_Blur f1 score: 0.8353575570495549\n",
      "Best 9NN Median_Blur classification time: 0.0006444454193115234\n",
      "Best 9NN Median_Blur accuracy: 0.8209876543209876\n",
      "Best 9NN Median_Blur f1 score: 0.8214880126753744\n",
      "Best 9NN Median_Blur classification time: 0.0006616115570068359\n",
      "Best 9NN Median_Blur accuracy: 0.8209876543209876\n",
      "Best 9NN Median_Blur f1 score: 0.8119788713485453\n",
      "Best 9NN Median_Blur classification time: 0.0007417201995849609\n",
      "Best 9NN Median_Blur accuracy: 0.808641975308642\n",
      "Best 9NN Median_Blur f1 score: 0.807884065083668\n",
      "Best 9NN Median_Blur classification time: 0.0008206367492675781\n",
      "Best 9NN Median_Blur accuracy: 0.8333333333333334\n",
      "Best 9NN Median_Blur f1 score: 0.8310511094363071\n",
      "Best 9NN Median_Blur classification time: 0.0006537437438964844\n",
      "Best 9NN Median_Blur accuracy: 0.8222565687789799\n",
      "Best 9NN Median_Blur f1 score: 0.8180580508742045\n",
      "Best 9NN Median_Blur classification time: 0.0006725788116455078\n",
      "Best 9NN Median_Blur accuracy: 0.7990726429675425\n",
      "Best 9NN Median_Blur f1 score: 0.7951761112954981\n",
      "Best 9NN Median_Blur classification time: 0.0007789134979248047\n",
      "Best 9NN Median_Blur accuracy: 0.8284389489953632\n",
      "Best 9NN Median_Blur f1 score: 0.82408408931883\n",
      "Best 9NN Median_Blur classification time: 0.0007762908935546875\n",
      "Best 9NN Median_Blur accuracy: 0.7897990726429676\n",
      "Best 9NN Median_Blur f1 score: 0.7838292080499557\n",
      "Best 9NN Median_Blur classification time: 0.0006642341613769531\n",
      "Best 9NN Median_Blur accuracy: 0.8222565687789799\n",
      "Best 9NN Median_Blur f1 score: 0.8187872827469976\n",
      "Best 9NN Median_Blur classification time: 0.0006816387176513672\n",
      "Best 11NN Median_Blur accuracy: 0.8333333333333334\n",
      "Best 11NN Median_Blur f1 score: 0.8300964248627799\n",
      "Best 11NN Median_Blur classification time: 0.0007190704345703125\n",
      "Best 11NN Median_Blur accuracy: 0.8179012345679012\n",
      "Best 11NN Median_Blur f1 score: 0.8144662698780462\n",
      "Best 11NN Median_Blur classification time: 0.0007994174957275391\n",
      "Best 11NN Median_Blur accuracy: 0.8009259259259259\n",
      "Best 11NN Median_Blur f1 score: 0.7898383778446084\n",
      "Best 11NN Median_Blur classification time: 0.0006861686706542969\n",
      "Best 11NN Median_Blur accuracy: 0.808641975308642\n",
      "Best 11NN Median_Blur f1 score: 0.8054268265139096\n",
      "Best 11NN Median_Blur classification time: 0.0006678104400634766\n",
      "Best 11NN Median_Blur accuracy: 0.8302469135802469\n",
      "Best 11NN Median_Blur f1 score: 0.823522625772776\n",
      "Best 11NN Median_Blur classification time: 0.0006818771362304688\n",
      "Best 11NN Median_Blur accuracy: 0.7975270479134466\n",
      "Best 11NN Median_Blur f1 score: 0.7902869272434488\n",
      "Best 11NN Median_Blur classification time: 0.000865936279296875\n",
      "Best 11NN Median_Blur accuracy: 0.7928902627511591\n",
      "Best 11NN Median_Blur f1 score: 0.7901049273468606\n",
      "Best 11NN Median_Blur classification time: 0.0007071495056152344\n",
      "Best 11NN Median_Blur accuracy: 0.8191653786707882\n",
      "Best 11NN Median_Blur f1 score: 0.8142574859296007\n",
      "Best 11NN Median_Blur classification time: 0.0006880760192871094\n",
      "Best 11NN Median_Blur accuracy: 0.7928902627511591\n",
      "Best 11NN Median_Blur f1 score: 0.7898268060539865\n",
      "Best 11NN Median_Blur classification time: 0.0008075237274169922\n",
      "Best 11NN Median_Blur accuracy: 0.8191653786707882\n",
      "Best 11NN Median_Blur f1 score: 0.8125417547568711\n",
      "Best 11NN Median_Blur classification time: 0.0007510185241699219\n",
      "Best 13NN Median_Blur accuracy: 0.8271604938271605\n",
      "Best 13NN Median_Blur f1 score: 0.8219029070195006\n",
      "Best 13NN Median_Blur classification time: 0.0006151199340820312\n",
      "Best 13NN Median_Blur accuracy: 0.8179012345679012\n",
      "Best 13NN Median_Blur f1 score: 0.8152917735616952\n",
      "Best 13NN Median_Blur classification time: 0.0008013248443603516\n",
      "Best 13NN Median_Blur accuracy: 0.8070987654320988\n",
      "Best 13NN Median_Blur f1 score: 0.7962449832316235\n",
      "Best 13NN Median_Blur classification time: 0.0007703304290771484\n",
      "Best 13NN Median_Blur accuracy: 0.8070987654320988\n",
      "Best 13NN Median_Blur f1 score: 0.8029364024150903\n",
      "Best 13NN Median_Blur classification time: 0.0006866455078125\n",
      "Best 13NN Median_Blur accuracy: 0.8256172839506173\n",
      "Best 13NN Median_Blur f1 score: 0.8207055682684974\n",
      "Best 13NN Median_Blur classification time: 0.0007164478302001953\n",
      "Best 13NN Median_Blur accuracy: 0.80370942812983\n",
      "Best 13NN Median_Blur f1 score: 0.7993239867360789\n",
      "Best 13NN Median_Blur classification time: 0.0007112026214599609\n",
      "Best 13NN Median_Blur accuracy: 0.7867078825347759\n",
      "Best 13NN Median_Blur f1 score: 0.7817266186364898\n",
      "Best 13NN Median_Blur classification time: 0.0006592273712158203\n",
      "Best 13NN Median_Blur accuracy: 0.8207109737248841\n",
      "Best 13NN Median_Blur f1 score: 0.8148778120045731\n",
      "Best 13NN Median_Blur classification time: 0.0007193088531494141\n",
      "Best 13NN Median_Blur accuracy: 0.7867078825347759\n",
      "Best 13NN Median_Blur f1 score: 0.7818585208882481\n",
      "Best 13NN Median_Blur classification time: 0.0007326602935791016\n",
      "Best 13NN Median_Blur accuracy: 0.8191653786707882\n",
      "Best 13NN Median_Blur f1 score: 0.8161600645239727\n",
      "Best 13NN Median_Blur classification time: 0.0007991790771484375\n",
      "Best 15NN Median_Blur accuracy: 0.8225308641975309\n",
      "Best 15NN Median_Blur f1 score: 0.8169703330576349\n",
      "Best 15NN Median_Blur classification time: 0.0006592273712158203\n",
      "Best 15NN Median_Blur accuracy: 0.8256172839506173\n",
      "Best 15NN Median_Blur f1 score: 0.8242683105499288\n",
      "Best 15NN Median_Blur classification time: 0.0007922649383544922\n",
      "Best 15NN Median_Blur accuracy: 0.808641975308642\n",
      "Best 15NN Median_Blur f1 score: 0.7984827590324044\n",
      "Best 15NN Median_Blur classification time: 0.0006082057952880859\n",
      "Best 15NN Median_Blur accuracy: 0.8040123456790124\n",
      "Best 15NN Median_Blur f1 score: 0.8023521886382753\n",
      "Best 15NN Median_Blur classification time: 0.0007786750793457031\n",
      "Best 15NN Median_Blur accuracy: 0.8240740740740741\n",
      "Best 15NN Median_Blur f1 score: 0.8183012994632207\n",
      "Best 15NN Median_Blur classification time: 0.0006945133209228516\n",
      "Best 15NN Median_Blur accuracy: 0.8021638330757341\n",
      "Best 15NN Median_Blur f1 score: 0.7976496440627555\n",
      "Best 15NN Median_Blur classification time: 0.0007030963897705078\n",
      "Best 15NN Median_Blur accuracy: 0.7882534775888718\n",
      "Best 15NN Median_Blur f1 score: 0.7818686023655887\n",
      "Best 15NN Median_Blur classification time: 0.0007865428924560547\n",
      "Best 15NN Median_Blur accuracy: 0.8176197836166924\n",
      "Best 15NN Median_Blur f1 score: 0.8117743434234401\n",
      "Best 15NN Median_Blur classification time: 0.0006616115570068359\n",
      "Best 15NN Median_Blur accuracy: 0.7820710973724884\n",
      "Best 15NN Median_Blur f1 score: 0.7753269020176309\n",
      "Best 15NN Median_Blur classification time: 0.0006706714630126953\n",
      "Best 15NN Median_Blur accuracy: 0.8145285935085008\n",
      "Best 15NN Median_Blur f1 score: 0.8113704357118617\n",
      "Best 15NN Median_Blur classification time: 0.0009932518005371094\n",
      "Best 17NN Median_Blur accuracy: 0.8256172839506173\n",
      "Best 17NN Median_Blur f1 score: 0.8203370149987729\n",
      "Best 17NN Median_Blur classification time: 0.0006382465362548828\n",
      "Best 17NN Median_Blur accuracy: 0.8225308641975309\n",
      "Best 17NN Median_Blur f1 score: 0.8215544163286598\n",
      "Best 17NN Median_Blur classification time: 0.0008044242858886719\n",
      "Best 17NN Median_Blur accuracy: 0.8179012345679012\n",
      "Best 17NN Median_Blur f1 score: 0.8091316668781457\n",
      "Best 17NN Median_Blur classification time: 0.0007987022399902344\n",
      "Best 17NN Median_Blur accuracy: 0.8101851851851852\n",
      "Best 17NN Median_Blur f1 score: 0.8076548897180489\n",
      "Best 17NN Median_Blur classification time: 0.0006706714630126953\n",
      "Best 17NN Median_Blur accuracy: 0.8179012345679012\n",
      "Best 17NN Median_Blur f1 score: 0.8141017547432372\n",
      "Best 17NN Median_Blur classification time: 0.0007746219635009766\n",
      "Best 17NN Median_Blur accuracy: 0.7975270479134466\n",
      "Best 17NN Median_Blur f1 score: 0.7916853092531589\n",
      "Best 17NN Median_Blur classification time: 0.0007526874542236328\n",
      "Best 17NN Median_Blur accuracy: 0.7867078825347759\n",
      "Best 17NN Median_Blur f1 score: 0.7805076509340712\n",
      "Best 17NN Median_Blur classification time: 0.0006797313690185547\n",
      "Best 17NN Median_Blur accuracy: 0.8145285935085008\n",
      "Best 17NN Median_Blur f1 score: 0.8068681801675645\n",
      "Best 17NN Median_Blur classification time: 0.0008363723754882812\n",
      "Best 17NN Median_Blur accuracy: 0.7913446676970634\n",
      "Best 17NN Median_Blur f1 score: 0.7839861562697674\n",
      "Best 17NN Median_Blur classification time: 0.0006964206695556641\n",
      "Best 17NN Median_Blur accuracy: 0.8068006182380216\n",
      "Best 17NN Median_Blur f1 score: 0.8020725163862972\n",
      "Best 17NN Median_Blur classification time: 0.0006887912750244141\n",
      "Best 19NN Median_Blur accuracy: 0.8132716049382716\n",
      "Best 19NN Median_Blur f1 score: 0.8081135548059694\n",
      "Best 19NN Median_Blur classification time: 0.000823974609375\n",
      "Best 19NN Median_Blur accuracy: 0.8101851851851852\n",
      "Best 19NN Median_Blur f1 score: 0.8081011106733681\n",
      "Best 19NN Median_Blur classification time: 0.0006923675537109375\n",
      "Best 19NN Median_Blur accuracy: 0.8179012345679012\n",
      "Best 19NN Median_Blur f1 score: 0.8087742107896373\n",
      "Best 19NN Median_Blur classification time: 0.0007276535034179688\n",
      "Best 19NN Median_Blur accuracy: 0.8117283950617284\n",
      "Best 19NN Median_Blur f1 score: 0.8041420284839199\n",
      "Best 19NN Median_Blur classification time: 0.0008087158203125\n",
      "Best 19NN Median_Blur accuracy: 0.8194444444444444\n",
      "Best 19NN Median_Blur f1 score: 0.8148239414401527\n",
      "Best 19NN Median_Blur classification time: 0.0006916522979736328\n",
      "Best 19NN Median_Blur accuracy: 0.7959814528593508\n",
      "Best 19NN Median_Blur f1 score: 0.7899748456854431\n",
      "Best 19NN Median_Blur classification time: 0.0007140636444091797\n",
      "Best 19NN Median_Blur accuracy: 0.7805255023183926\n",
      "Best 19NN Median_Blur f1 score: 0.7735649110624171\n",
      "Best 19NN Median_Blur classification time: 0.00077056884765625\n",
      "Best 19NN Median_Blur accuracy: 0.8068006182380216\n",
      "Best 19NN Median_Blur f1 score: 0.7984785348144482\n",
      "Best 19NN Median_Blur classification time: 0.0006866455078125\n",
      "Best 19NN Median_Blur accuracy: 0.7882534775888718\n",
      "Best 19NN Median_Blur f1 score: 0.7803382904701762\n",
      "Best 19NN Median_Blur classification time: 0.0007085800170898438\n",
      "Best 19NN Median_Blur accuracy: 0.8083462132921174\n",
      "Best 19NN Median_Blur f1 score: 0.8055244982475673\n",
      "Best 19NN Median_Blur classification time: 0.0007953643798828125\n",
      "Best 21NN Median_Blur accuracy: 0.816358024691358\n",
      "Best 21NN Median_Blur f1 score: 0.8098526523870834\n",
      "Best 21NN Median_Blur classification time: 0.0006694793701171875\n",
      "Best 21NN Median_Blur accuracy: 0.8117283950617284\n",
      "Best 21NN Median_Blur f1 score: 0.8081930928487812\n",
      "Best 21NN Median_Blur classification time: 0.0007474422454833984\n",
      "Best 21NN Median_Blur accuracy: 0.8148148148148148\n",
      "Best 21NN Median_Blur f1 score: 0.807617219302475\n",
      "Best 21NN Median_Blur classification time: 0.0006811618804931641\n",
      "Best 21NN Median_Blur accuracy: 0.808641975308642\n",
      "Best 21NN Median_Blur f1 score: 0.8028324761256161\n",
      "Best 21NN Median_Blur classification time: 0.0006892681121826172\n",
      "Best 21NN Median_Blur accuracy: 0.8179012345679012\n",
      "Best 21NN Median_Blur f1 score: 0.8150944611985151\n",
      "Best 21NN Median_Blur classification time: 0.0007202625274658203\n",
      "Best 21NN Median_Blur accuracy: 0.794435857805255\n",
      "Best 21NN Median_Blur f1 score: 0.788251341646434\n",
      "Best 21NN Median_Blur classification time: 0.0008213520050048828\n",
      "Best 21NN Median_Blur accuracy: 0.7851622874806801\n",
      "Best 21NN Median_Blur f1 score: 0.778342126951889\n",
      "Best 21NN Median_Blur classification time: 0.0007011890411376953\n",
      "Best 21NN Median_Blur accuracy: 0.80370942812983\n",
      "Best 21NN Median_Blur f1 score: 0.7975081687819908\n",
      "Best 21NN Median_Blur classification time: 0.0007827281951904297\n",
      "Best 21NN Median_Blur accuracy: 0.7928902627511591\n",
      "Best 21NN Median_Blur f1 score: 0.7869013572700103\n",
      "Best 21NN Median_Blur classification time: 0.0008289813995361328\n",
      "Best 21NN Median_Blur accuracy: 0.80370942812983\n",
      "Best 21NN Median_Blur f1 score: 0.7986413984077535\n",
      "Best 21NN Median_Blur classification time: 0.0006525516510009766\n",
      "Best 23NN Median_Blur accuracy: 0.816358024691358\n",
      "Best 23NN Median_Blur f1 score: 0.810482557840878\n",
      "Best 23NN Median_Blur classification time: 0.0007581710815429688\n",
      "Best 23NN Median_Blur accuracy: 0.808641975308642\n",
      "Best 23NN Median_Blur f1 score: 0.8048294058641913\n",
      "Best 23NN Median_Blur classification time: 0.0006780624389648438\n",
      "Best 23NN Median_Blur accuracy: 0.8194444444444444\n",
      "Best 23NN Median_Blur f1 score: 0.8106103783314335\n",
      "Best 23NN Median_Blur classification time: 0.0007114410400390625\n",
      "Best 23NN Median_Blur accuracy: 0.8009259259259259\n",
      "Best 23NN Median_Blur f1 score: 0.7950534479782391\n",
      "Best 23NN Median_Blur classification time: 0.0009205341339111328\n",
      "Best 23NN Median_Blur accuracy: 0.8101851851851852\n",
      "Best 23NN Median_Blur f1 score: 0.8076993191153993\n",
      "Best 23NN Median_Blur classification time: 0.0007979869842529297\n",
      "Best 23NN Median_Blur accuracy: 0.7867078825347759\n",
      "Best 23NN Median_Blur f1 score: 0.7791295867549349\n",
      "Best 23NN Median_Blur classification time: 0.000667572021484375\n",
      "Best 23NN Median_Blur accuracy: 0.7805255023183926\n",
      "Best 23NN Median_Blur f1 score: 0.7743453242313264\n",
      "Best 23NN Median_Blur classification time: 0.0006406307220458984\n",
      "Best 23NN Median_Blur accuracy: 0.8052550231839258\n",
      "Best 23NN Median_Blur f1 score: 0.7978222875759095\n",
      "Best 23NN Median_Blur classification time: 0.0006561279296875\n",
      "Best 23NN Median_Blur accuracy: 0.7882534775888718\n",
      "Best 23NN Median_Blur f1 score: 0.7834634787732716\n",
      "Best 23NN Median_Blur classification time: 0.0006244182586669922\n",
      "Best 23NN Median_Blur accuracy: 0.7913446676970634\n",
      "Best 23NN Median_Blur f1 score: 0.7860260920154781\n",
      "Best 23NN Median_Blur classification time: 0.0007491111755371094\n",
      "Best 25NN Median_Blur accuracy: 0.8132716049382716\n",
      "Best 25NN Median_Blur f1 score: 0.806413880064281\n",
      "Best 25NN Median_Blur classification time: 0.0006430149078369141\n",
      "Best 25NN Median_Blur accuracy: 0.8148148148148148\n",
      "Best 25NN Median_Blur f1 score: 0.8139310601954347\n",
      "Best 25NN Median_Blur classification time: 0.0006520748138427734\n",
      "Best 25NN Median_Blur accuracy: 0.8101851851851852\n",
      "Best 25NN Median_Blur f1 score: 0.7985982492572014\n",
      "Best 25NN Median_Blur classification time: 0.0007426738739013672\n",
      "Best 25NN Median_Blur accuracy: 0.8070987654320988\n",
      "Best 25NN Median_Blur f1 score: 0.8006322702701533\n",
      "Best 25NN Median_Blur classification time: 0.0006811618804931641\n",
      "Best 25NN Median_Blur accuracy: 0.8117283950617284\n",
      "Best 25NN Median_Blur f1 score: 0.8088987847379937\n",
      "Best 25NN Median_Blur classification time: 0.0007174015045166016\n",
      "Best 25NN Median_Blur accuracy: 0.7820710973724884\n",
      "Best 25NN Median_Blur f1 score: 0.7738892437279073\n",
      "Best 25NN Median_Blur classification time: 0.0006542205810546875\n",
      "Best 25NN Median_Blur accuracy: 0.7743431221020093\n",
      "Best 25NN Median_Blur f1 score: 0.7678436895089614\n",
      "Best 25NN Median_Blur classification time: 0.0007486343383789062\n",
      "Best 25NN Median_Blur accuracy: 0.794435857805255\n",
      "Best 25NN Median_Blur f1 score: 0.784236479262015\n",
      "Best 25NN Median_Blur classification time: 0.0007364749908447266\n",
      "Best 25NN Median_Blur accuracy: 0.7836166924265843\n",
      "Best 25NN Median_Blur f1 score: 0.7809063335958807\n",
      "Best 25NN Median_Blur classification time: 0.0006449222564697266\n",
      "Best 25NN Median_Blur accuracy: 0.8021638330757341\n",
      "Best 25NN Median_Blur f1 score: 0.7972154818865754\n",
      "Best 25NN Median_Blur classification time: 0.0007097721099853516\n",
      "Best 27NN Median_Blur accuracy: 0.8148148148148148\n",
      "Best 27NN Median_Blur f1 score: 0.80730386815807\n",
      "Best 27NN Median_Blur classification time: 0.0006322860717773438\n",
      "Best 27NN Median_Blur accuracy: 0.816358024691358\n",
      "Best 27NN Median_Blur f1 score: 0.8146554443649641\n",
      "Best 27NN Median_Blur classification time: 0.000797271728515625\n",
      "Best 27NN Median_Blur accuracy: 0.8117283950617284\n",
      "Best 27NN Median_Blur f1 score: 0.8022108663037515\n",
      "Best 27NN Median_Blur classification time: 0.0006425380706787109\n",
      "Best 27NN Median_Blur accuracy: 0.8132716049382716\n",
      "Best 27NN Median_Blur f1 score: 0.8066623014937622\n",
      "Best 27NN Median_Blur classification time: 0.0006582736968994141\n",
      "Best 27NN Median_Blur accuracy: 0.8101851851851852\n",
      "Best 27NN Median_Blur f1 score: 0.8064378025025888\n",
      "Best 27NN Median_Blur classification time: 0.0007636547088623047\n",
      "Best 27NN Median_Blur accuracy: 0.7789799072642968\n",
      "Best 27NN Median_Blur f1 score: 0.7713012841657095\n",
      "Best 27NN Median_Blur classification time: 0.0007746219635009766\n",
      "Best 27NN Median_Blur accuracy: 0.7712519319938176\n",
      "Best 27NN Median_Blur f1 score: 0.764140181451897\n",
      "Best 27NN Median_Blur classification time: 0.0006761550903320312\n",
      "Best 27NN Median_Blur accuracy: 0.8006182380216383\n",
      "Best 27NN Median_Blur f1 score: 0.7905422716832488\n",
      "Best 27NN Median_Blur classification time: 0.0006651878356933594\n",
      "Best 27NN Median_Blur accuracy: 0.7882534775888718\n",
      "Best 27NN Median_Blur f1 score: 0.7831876184817362\n",
      "Best 27NN Median_Blur classification time: 0.0008161067962646484\n",
      "Best 27NN Median_Blur accuracy: 0.794435857805255\n",
      "Best 27NN Median_Blur f1 score: 0.7907570741795341\n",
      "Best 27NN Median_Blur classification time: 0.0006968975067138672\n",
      "Best 29NN Median_Blur accuracy: 0.8209876543209876\n",
      "Best 29NN Median_Blur f1 score: 0.8144732991439992\n",
      "Best 29NN Median_Blur classification time: 0.0007665157318115234\n",
      "Best 29NN Median_Blur accuracy: 0.8101851851851852\n",
      "Best 29NN Median_Blur f1 score: 0.808969490881695\n",
      "Best 29NN Median_Blur classification time: 0.000644683837890625\n",
      "Best 29NN Median_Blur accuracy: 0.8148148148148148\n",
      "Best 29NN Median_Blur f1 score: 0.8029883769961367\n",
      "Best 29NN Median_Blur classification time: 0.0008232593536376953\n",
      "Best 29NN Median_Blur accuracy: 0.7947530864197531\n",
      "Best 29NN Median_Blur f1 score: 0.7855107245998229\n",
      "Best 29NN Median_Blur classification time: 0.0006768703460693359\n",
      "Best 29NN Median_Blur accuracy: 0.816358024691358\n",
      "Best 29NN Median_Blur f1 score: 0.8144807366924961\n",
      "Best 29NN Median_Blur classification time: 0.0006923675537109375\n",
      "Best 29NN Median_Blur accuracy: 0.7789799072642968\n",
      "Best 29NN Median_Blur f1 score: 0.7718182384675197\n",
      "Best 29NN Median_Blur classification time: 0.0007956027984619141\n",
      "Best 29NN Median_Blur accuracy: 0.7758887171561051\n",
      "Best 29NN Median_Blur f1 score: 0.7670271657692224\n",
      "Best 29NN Median_Blur classification time: 0.0006961822509765625\n",
      "Best 29NN Median_Blur accuracy: 0.8006182380216383\n",
      "Best 29NN Median_Blur f1 score: 0.7901105712497684\n",
      "Best 29NN Median_Blur classification time: 0.0006542205810546875\n",
      "Best 29NN Median_Blur accuracy: 0.7774343122102009\n",
      "Best 29NN Median_Blur f1 score: 0.7704591607319179\n",
      "Best 29NN Median_Blur classification time: 0.0008192062377929688\n",
      "Best 29NN Median_Blur accuracy: 0.7913446676970634\n",
      "Best 29NN Median_Blur f1 score: 0.786734650795494\n",
      "Best 29NN Median_Blur classification time: 0.0006241798400878906\n",
      "Best 31NN Median_Blur accuracy: 0.8179012345679012\n",
      "Best 31NN Median_Blur f1 score: 0.8107185875155899\n",
      "Best 31NN Median_Blur classification time: 0.0008075237274169922\n",
      "Best 31NN Median_Blur accuracy: 0.8148148148148148\n",
      "Best 31NN Median_Blur f1 score: 0.8115973938342359\n",
      "Best 31NN Median_Blur classification time: 0.0007343292236328125\n",
      "Best 31NN Median_Blur accuracy: 0.8132716049382716\n",
      "Best 31NN Median_Blur f1 score: 0.8003779471637079\n",
      "Best 31NN Median_Blur classification time: 0.0007641315460205078\n",
      "Best 31NN Median_Blur accuracy: 0.7932098765432098\n",
      "Best 31NN Median_Blur f1 score: 0.7862808132629636\n",
      "Best 31NN Median_Blur classification time: 0.0006809234619140625\n",
      "Best 31NN Median_Blur accuracy: 0.816358024691358\n",
      "Best 31NN Median_Blur f1 score: 0.8144807366924961\n",
      "Best 31NN Median_Blur classification time: 0.0007023811340332031\n",
      "Best 31NN Median_Blur accuracy: 0.7758887171561051\n",
      "Best 31NN Median_Blur f1 score: 0.768104777919322\n",
      "Best 31NN Median_Blur classification time: 0.0007221698760986328\n",
      "Best 31NN Median_Blur accuracy: 0.7774343122102009\n",
      "Best 31NN Median_Blur f1 score: 0.77204982712875\n",
      "Best 31NN Median_Blur classification time: 0.0006957054138183594\n",
      "Best 31NN Median_Blur accuracy: 0.8021638330757341\n",
      "Best 31NN Median_Blur f1 score: 0.7917272922924727\n",
      "Best 31NN Median_Blur classification time: 0.0007951259613037109\n",
      "Best 31NN Median_Blur accuracy: 0.7789799072642968\n",
      "Best 31NN Median_Blur f1 score: 0.7742419464364176\n",
      "Best 31NN Median_Blur classification time: 0.0006690025329589844\n",
      "Best 31NN Median_Blur accuracy: 0.7867078825347759\n",
      "Best 31NN Median_Blur f1 score: 0.7807059732673962\n",
      "Best 31NN Median_Blur classification time: 0.0008077621459960938\n",
      "Best 21NN Median_Blur accuracy: 0.8518518518518519\n",
      "Best 21NN Median_Blur f1 score: 0.8462353023913344\n",
      "Best 21NN Median_Blur classification time: 0.0006015300750732422\n",
      "{'SVM': {'None': [0.8626543209876543, 0.8580246913580247, 0.8441358024691358, 0.8302469135802469, 0.845679012345679, 0.848531684698609, 0.8438948995363215, 0.874806800618238, 0.8531684698608965, 0.8423493044822257], 'Gaussian_Blur': [0.8487654320987654, 0.8518518518518519, 0.8395061728395061, 0.8317901234567902, 0.8503086419753086, 0.8438948995363215, 0.8268933539412674, 0.8593508500772797, 0.8423493044822257, 0.8315301391035549], 'Median_Blur': [0.8472222222222222, 0.8503086419753086, 0.8240740740740741, 0.808641975308642, 0.8364197530864198, 0.8408037094281299, 0.8129829984544049, 0.839258114374034, 0.8299845440494591, 0.8191653786707882]}, 'RF': {'None': [0.8194444444444444, 0.8317901234567902, 0.8101851851851852, 0.7962962962962963, 0.8225308641975309, 0.8068006182380216, 0.8129829984544049, 0.8222565687789799, 0.8253477588871716, 0.8098918083462133], 'Gaussian_Blur': [0.7870370370370371, 0.8179012345679012, 0.808641975308642, 0.7854938271604939, 0.8194444444444444, 0.8222565687789799, 0.7975270479134466, 0.8315301391035549, 0.8068006182380216, 0.8006182380216383], 'Median_Blur': [0.8101851851851852, 0.8009259259259259, 0.8024691358024691, 0.7623456790123457, 0.8009259259259259, 0.8114374034003091, 0.7882534775888718, 0.8052550231839258, 0.8098918083462133, 0.8145285935085008]}, 'KNN': {'None': [0.8580246913580247, 0.8595679012345679, 0.816358024691358, 0.8364197530864198, 0.8472222222222222, 0.8315301391035549, 0.8423493044822257, 0.8408037094281299, 0.8377125193199382, 0.8578052550231839, 0.8487654320987654, 0.8503086419753086, 0.8194444444444444, 0.8333333333333334, 0.8348765432098766, 0.8454404945904173, 0.8408037094281299, 0.8516228748068007, 0.8423493044822257, 0.8562596599690881, 0.8518518518518519, 0.8441358024691358, 0.8179012345679012, 0.8333333333333334, 0.8364197530864198, 0.8454404945904173, 0.8408037094281299, 0.8469860896445132, 0.8438948995363215, 0.8438948995363215, 0.8302469135802469, 0.8425925925925926, 0.8287037037037037, 0.816358024691358, 0.8333333333333334, 0.8346213292117465, 0.8207109737248841, 0.8469860896445132, 0.8377125193199382, 0.8268933539412674, 0.8333333333333334, 0.8518518518518519, 0.8333333333333334, 0.8117283950617284, 0.8410493827160493, 0.8408037094281299, 0.8222565687789799, 0.8531684698608965, 0.8361669242658424, 0.8299845440494591, 0.8271604938271605, 0.8425925925925926, 0.8287037037037037, 0.8117283950617284, 0.8487654320987654, 0.8268933539412674, 0.8083462132921174, 0.8377125193199382, 0.8469860896445132, 0.8253477588871716, 0.8256172839506173, 0.8425925925925926, 0.8302469135802469, 0.8148148148148148, 0.8410493827160493, 0.8176197836166924, 0.80370942812983, 0.8238021638330757, 0.8253477588871716, 0.8299845440494591, 0.8179012345679012, 0.8580246913580247, 0.8256172839506173, 0.8148148148148148, 0.8472222222222222, 0.8268933539412674, 0.8068006182380216, 0.8299845440494591, 0.8330757341576507, 0.8268933539412674, 0.8209876543209876, 0.8503086419753086, 0.8148148148148148, 0.8209876543209876, 0.8364197530864198, 0.8160741885625966, 0.80370942812983, 0.8299845440494591, 0.8222565687789799, 0.8268933539412674, 0.816358024691358, 0.8472222222222222, 0.8132716049382716, 0.8209876543209876, 0.8441358024691358, 0.8176197836166924, 0.7990726429675425, 0.8129829984544049, 0.8222565687789799, 0.8253477588871716, 0.8055555555555556, 0.8395061728395061, 0.8179012345679012, 0.8117283950617284, 0.8348765432098766, 0.8222565687789799, 0.80370942812983, 0.8238021638330757, 0.8238021638330757, 0.8222565687789799, 0.808641975308642, 0.8472222222222222, 0.8132716049382716, 0.8070987654320988, 0.8348765432098766, 0.8145285935085008, 0.7897990726429676, 0.8160741885625966, 0.8160741885625966, 0.8145285935085008, 0.808641975308642, 0.8472222222222222, 0.8117283950617284, 0.8024691358024691, 0.8302469135802469, 0.8129829984544049, 0.7897990726429676, 0.8268933539412674, 0.8160741885625966, 0.8129829984544049, 0.7978395061728395, 0.8348765432098766, 0.8132716049382716, 0.7993827160493827, 0.8287037037037037, 0.8068006182380216, 0.7851622874806801, 0.8222565687789799, 0.8238021638330757, 0.8129829984544049, 0.8055555555555556, 0.8333333333333334, 0.8117283950617284, 0.7993827160493827, 0.8317901234567902, 0.8068006182380216, 0.7820710973724884, 0.8207109737248841, 0.8238021638330757, 0.8160741885625966, 0.808641975308642, 0.8287037037037037, 0.8101851851851852, 0.7947530864197531, 0.8333333333333334, 0.7975270479134466, 0.7836166924265843, 0.8238021638330757, 0.8253477588871716, 0.8114374034003091], 'Gaussian_Blur': [0.8271604938271605, 0.845679012345679, 0.7947530864197531, 0.8503086419753086, 0.8271604938271605, 0.8315301391035549, 0.8253477588871716, 0.8438948995363215, 0.8423493044822257, 0.8423493044822257, 0.8333333333333334, 0.8379629629629629, 0.8132716049382716, 0.8364197530864198, 0.8302469135802469, 0.8207109737248841, 0.8176197836166924, 0.839258114374034, 0.839258114374034, 0.8408037094281299, 0.8379629629629629, 0.816358024691358, 0.8333333333333334, 0.8271604938271605, 0.8287037037037037, 0.8253477588871716, 0.8238021638330757, 0.8361669242658424, 0.8299845440494591, 0.8253477588871716, 0.8271604938271605, 0.8302469135802469, 0.8271604938271605, 0.8132716049382716, 0.8410493827160493, 0.8191653786707882, 0.8098918083462133, 0.839258114374034, 0.8330757341576507, 0.8207109737248841, 0.8271604938271605, 0.8256172839506173, 0.8209876543209876, 0.8132716049382716, 0.8364197530864198, 0.8238021638330757, 0.8083462132921174, 0.8253477588871716, 0.8299845440494591, 0.8253477588871716, 0.8302469135802469, 0.8287037037037037, 0.816358024691358, 0.8101851851851852, 0.8348765432098766, 0.8145285935085008, 0.8083462132921174, 0.8268933539412674, 0.8191653786707882, 0.8052550231839258, 0.8302469135802469, 0.8225308641975309, 0.8287037037037037, 0.8055555555555556, 0.8410493827160493, 0.8222565687789799, 0.8006182380216383, 0.8098918083462133, 0.8191653786707882, 0.8238021638330757, 0.8240740740740741, 0.8209876543209876, 0.816358024691358, 0.8055555555555556, 0.8348765432098766, 0.8114374034003091, 0.8083462132921174, 0.80370942812983, 0.8145285935085008, 0.8145285935085008, 0.8179012345679012, 0.8101851851851852, 0.8148148148148148, 0.7947530864197531, 0.8395061728395061, 0.8114374034003091, 0.8083462132921174, 0.8068006182380216, 0.7975270479134466, 0.8098918083462133, 0.8117283950617284, 0.8101851851851852, 0.808641975308642, 0.7993827160493827, 0.8379629629629629, 0.8098918083462133, 0.7928902627511591, 0.8222565687789799, 0.8006182380216383, 0.8129829984544049, 0.8070987654320988, 0.8101851851851852, 0.8194444444444444, 0.7916666666666666, 0.8379629629629629, 0.80370942812983, 0.794435857805255, 0.8114374034003091, 0.8021638330757341, 0.8068006182380216, 0.8055555555555556, 0.8117283950617284, 0.8148148148148148, 0.7870370370370371, 0.8271604938271605, 0.8006182380216383, 0.7959814528593508, 0.8083462132921174, 0.7975270479134466, 0.7990726429675425, 0.8117283950617284, 0.816358024691358, 0.7978395061728395, 0.7978395061728395, 0.8240740740740741, 0.7959814528593508, 0.7836166924265843, 0.8021638330757341, 0.794435857805255, 0.794435857805255, 0.8070987654320988, 0.8225308641975309, 0.8009259259259259, 0.7854938271604939, 0.8209876543209876, 0.7975270479134466, 0.7867078825347759, 0.7959814528593508, 0.7913446676970634, 0.794435857805255, 0.8101851851851852, 0.8132716049382716, 0.7978395061728395, 0.7808641975308642, 0.8240740740740741, 0.7975270479134466, 0.7836166924265843, 0.80370942812983, 0.7851622874806801, 0.7897990726429676, 0.8148148148148148, 0.8194444444444444, 0.7993827160493827, 0.7870370370370371, 0.8240740740740741, 0.7897990726429676, 0.7805255023183926, 0.8114374034003091, 0.7836166924265843, 0.7975270479134466], 'Median_Blur': [0.8209876543209876, 0.8179012345679012, 0.816358024691358, 0.8132716049382716, 0.8425925925925926, 0.8207109737248841, 0.80370942812983, 0.8207109737248841, 0.8346213292117465, 0.8438948995363215, 0.8518518518518519, 0.8240740740740741, 0.8070987654320988, 0.8040123456790124, 0.8364197530864198, 0.839258114374034, 0.8268933539412674, 0.8315301391035549, 0.8222565687789799, 0.8299845440494591, 0.845679012345679, 0.816358024691358, 0.8225308641975309, 0.8240740740740741, 0.8317901234567902, 0.8330757341576507, 0.8253477588871716, 0.8438948995363215, 0.8191653786707882, 0.8361669242658424, 0.8395061728395061, 0.8209876543209876, 0.8179012345679012, 0.8317901234567902, 0.8256172839506173, 0.8284389489953632, 0.8068006182380216, 0.8438948995363215, 0.80370942812983, 0.8268933539412674, 0.8395061728395061, 0.8209876543209876, 0.8209876543209876, 0.808641975308642, 0.8333333333333334, 0.8222565687789799, 0.7990726429675425, 0.8284389489953632, 0.7897990726429676, 0.8222565687789799, 0.8333333333333334, 0.8179012345679012, 0.8009259259259259, 0.808641975308642, 0.8302469135802469, 0.7975270479134466, 0.7928902627511591, 0.8191653786707882, 0.7928902627511591, 0.8191653786707882, 0.8271604938271605, 0.8179012345679012, 0.8070987654320988, 0.8070987654320988, 0.8256172839506173, 0.80370942812983, 0.7867078825347759, 0.8207109737248841, 0.7867078825347759, 0.8191653786707882, 0.8225308641975309, 0.8256172839506173, 0.808641975308642, 0.8040123456790124, 0.8240740740740741, 0.8021638330757341, 0.7882534775888718, 0.8176197836166924, 0.7820710973724884, 0.8145285935085008, 0.8256172839506173, 0.8225308641975309, 0.8179012345679012, 0.8101851851851852, 0.8179012345679012, 0.7975270479134466, 0.7867078825347759, 0.8145285935085008, 0.7913446676970634, 0.8068006182380216, 0.8132716049382716, 0.8101851851851852, 0.8179012345679012, 0.8117283950617284, 0.8194444444444444, 0.7959814528593508, 0.7805255023183926, 0.8068006182380216, 0.7882534775888718, 0.8083462132921174, 0.816358024691358, 0.8117283950617284, 0.8148148148148148, 0.808641975308642, 0.8179012345679012, 0.794435857805255, 0.7851622874806801, 0.80370942812983, 0.7928902627511591, 0.80370942812983, 0.816358024691358, 0.808641975308642, 0.8194444444444444, 0.8009259259259259, 0.8101851851851852, 0.7867078825347759, 0.7805255023183926, 0.8052550231839258, 0.7882534775888718, 0.7913446676970634, 0.8132716049382716, 0.8148148148148148, 0.8101851851851852, 0.8070987654320988, 0.8117283950617284, 0.7820710973724884, 0.7743431221020093, 0.794435857805255, 0.7836166924265843, 0.8021638330757341, 0.8148148148148148, 0.816358024691358, 0.8117283950617284, 0.8132716049382716, 0.8101851851851852, 0.7789799072642968, 0.7712519319938176, 0.8006182380216383, 0.7882534775888718, 0.794435857805255, 0.8209876543209876, 0.8101851851851852, 0.8148148148148148, 0.7947530864197531, 0.816358024691358, 0.7789799072642968, 0.7758887171561051, 0.8006182380216383, 0.7774343122102009, 0.7913446676970634, 0.8179012345679012, 0.8148148148148148, 0.8132716049382716, 0.7932098765432098, 0.816358024691358, 0.7758887171561051, 0.7774343122102009, 0.8021638330757341, 0.7789799072642968, 0.7867078825347759]}}\n",
      "{'SVM': {'None': [0.8616103834590064, 0.859822498593685, 0.8394558048813368, 0.8285276305013146, 0.8493069047716347, 0.8487342117670412, 0.840786569338059, 0.8753099275992996, 0.8491596960100324, 0.8387804429739912], 'Gaussian_Blur': [0.8505186648104418, 0.8508848680420984, 0.8338364173979976, 0.836526299660307, 0.8506410256410256, 0.8400949722653577, 0.8262026349075194, 0.860309724929608, 0.8387008338782335, 0.8268736343585669], 'Median_Blur': [0.8482163031278521, 0.8527508361132329, 0.8219670263148524, 0.8121904308078699, 0.8364288494394673, 0.8406810275622156, 0.8111673865797621, 0.8378691059458325, 0.8276127157093924, 0.8176090855888162]}, 'RF': {'None': [0.8158539990707275, 0.8325888481066889, 0.8014426589153959, 0.7948863773172722, 0.8242358639311326, 0.8051574663428708, 0.8095856926045794, 0.8186035990482835, 0.8190598973278465, 0.805399285150064], 'Gaussian_Blur': [0.7838863622343976, 0.8187221935150176, 0.8026140857938056, 0.7862752562268588, 0.8176492588059295, 0.8237948841529287, 0.8019331703614404, 0.8228545335942595, 0.8023707110791545, 0.7948508860550502], 'Median_Blur': [0.8057012753694877, 0.801784152333946, 0.794519618224078, 0.7599787546024105, 0.7982079410140316, 0.8118107820311513, 0.7912888390924602, 0.7995955502338482, 0.8048251942988784, 0.8109723401615162]}, 'KNN': {'None': [0.8549660747273289, 0.8592745677521894, 0.8136560014175886, 0.8393813589709112, 0.8477814819274289, 0.8242753984732349, 0.8377697159107104, 0.8425079172068924, 0.8413379721482701, 0.8589921992006619, 0.8440873015873015, 0.8466022464315396, 0.812574308613756, 0.8313957478576074, 0.8350098481733603, 0.8397153892565546, 0.838608157967654, 0.851392350238363, 0.8440990249699999, 0.8520381888935162, 0.8485210337015904, 0.8425215211235134, 0.8110352269562796, 0.8300100853611129, 0.836690327578879, 0.8452845268542198, 0.8379584003342796, 0.8494758351101636, 0.8452967562728494, 0.8398140479061461, 0.827544569280675, 0.8443489804643948, 0.8214749567834941, 0.8116493386904077, 0.8332956087525277, 0.8329928877142088, 0.8153184384827331, 0.8466927970238105, 0.8358367606314246, 0.8233327726158789, 0.827165971634699, 0.8519139514122913, 0.8241686116076918, 0.8100646893773383, 0.8406391136870175, 0.8336930478587813, 0.8176732592350441, 0.851958868664965, 0.8333052833500729, 0.8245512378784827, 0.8190956002839783, 0.8434747720737225, 0.8194335449146334, 0.8063536127966926, 0.8464445591498021, 0.822005177520097, 0.8025735937666774, 0.8350218926441771, 0.8404321350765128, 0.8232737870128622, 0.8183489690399942, 0.8450067225046372, 0.8204194298072923, 0.8085193580816273, 0.8385204400932725, 0.8094613070823079, 0.7970231989737618, 0.8217134659859119, 0.8177603546723248, 0.8276963607307475, 0.8101704272733078, 0.8576950311643318, 0.8156133704221157, 0.8069472196113007, 0.8451948892123564, 0.8210581757840685, 0.8009925041362767, 0.8269506871089986, 0.826658210999493, 0.8234936252246826, 0.8167213042742939, 0.8501738319392692, 0.8052570549814885, 0.8157886196857701, 0.8342911877394635, 0.8085907870284227, 0.7971102644885798, 0.8259142994373542, 0.8174415327253755, 0.8248860175523959, 0.812213541095368, 0.8476419768004201, 0.8036446440699767, 0.8167042695132581, 0.8423915791959189, 0.8097301579108627, 0.7922080278495945, 0.8082464580020622, 0.8159269652431865, 0.8228409609423143, 0.8012034243800494, 0.8412686001238665, 0.8073330430692615, 0.8060303408618016, 0.8328432487053177, 0.8150541087521148, 0.7958028307309567, 0.8185906632821984, 0.8167272397124408, 0.8175555751595723, 0.8035667646604326, 0.8465444149017136, 0.8030157209789751, 0.8008243336328712, 0.8318743464988524, 0.8073828471539929, 0.7831672151630095, 0.8100550767730731, 0.8101135537351452, 0.8086104504509155, 0.8029935237678979, 0.8498329595170078, 0.802499572224935, 0.7953107551793224, 0.8256118844967203, 0.8061846222072027, 0.7853601655827683, 0.8200890639415229, 0.8100105045350002, 0.8089359991694266, 0.7901504340072832, 0.8370759761580165, 0.8032922719826127, 0.7910913431191465, 0.8237981174592406, 0.8007247331809925, 0.7775912132978874, 0.8157734914620662, 0.8166613572716277, 0.8082071220612937, 0.7991163609376363, 0.8350704344701043, 0.8004735425812278, 0.7920518807610843, 0.8269334873145908, 0.8004739983563186, 0.7761625307399954, 0.8151106960011069, 0.8155683103366211, 0.8124289735946855, 0.8034797130731558, 0.8287835811823818, 0.7977314466399191, 0.7866189910001019, 0.8293152214270604, 0.7909381291403763, 0.7764952799067147, 0.8182093978279265, 0.8202981430089263, 0.8064389746047357], 'Gaussian_Blur': [0.8305679732689027, 0.8444328439040233, 0.7926489958412882, 0.8486696981942664, 0.8309577663391222, 0.8278147286154662, 0.8212235764143055, 0.8435253923650445, 0.8394120044336425, 0.8426221295383637, 0.8272416430952475, 0.8336619479833742, 0.8102081434362086, 0.8363668071376553, 0.8341083877781941, 0.8204431359016032, 0.8170067097847648, 0.8363398085447095, 0.838723343986502, 0.835396823264416, 0.8348529995533478, 0.8141706444280059, 0.8291510387890121, 0.8278324308183969, 0.8305259840414081, 0.8240510037961313, 0.8236382689101452, 0.8303486593887438, 0.8305656173510761, 0.820632715799491, 0.8231149823221312, 0.8266923304577519, 0.8240938318122369, 0.8124237576242376, 0.8433017618130269, 0.8158090985248281, 0.8058601234830743, 0.8342712915745608, 0.829085571021055, 0.8170093860039563, 0.8260298359651799, 0.8236759127953408, 0.8146815524246486, 0.8126559736461588, 0.836858735710042, 0.8204316218640275, 0.8048073747688638, 0.8216766224754196, 0.8238756685438888, 0.8188749416282844, 0.8315752696841522, 0.8285600366718859, 0.8073924121885753, 0.8089159883016176, 0.8336053311573507, 0.8100444743992349, 0.8058631504285628, 0.8252279295659304, 0.8118897198936131, 0.8032563526125758, 0.8295363071345867, 0.8221279896764345, 0.8154675568301119, 0.7995993129847904, 0.8387955182917693, 0.8174275916881157, 0.7955745477679482, 0.8071746145624775, 0.8120039682539683, 0.8199832995414355, 0.8229121807735459, 0.8215294231474544, 0.8058284491896238, 0.8005041017058567, 0.8329183704015319, 0.8057839721254355, 0.8042815939091842, 0.799487232843456, 0.8111038885712345, 0.8088835596702081, 0.8155831591027303, 0.8091784206892121, 0.803894119785844, 0.7880994640932505, 0.8361240151233079, 0.805896259692021, 0.8029455326051208, 0.8021580316973017, 0.7919099875168868, 0.803644206021116, 0.8098101514304391, 0.8086937494619334, 0.7977275126237832, 0.7978966890816644, 0.834073972810815, 0.8039762846639062, 0.7848232848232849, 0.8178817297845004, 0.7923471412436277, 0.8081126211967332, 0.8055283070384106, 0.8073936802090759, 0.8081403418325421, 0.7867774837144378, 0.8340251045648347, 0.7976040714498907, 0.7884333948163734, 0.805003537204965, 0.7934183299774699, 0.8019632902681989, 0.8033263310258616, 0.8059988737793446, 0.8038677468642291, 0.7824952839639653, 0.823155172746743, 0.7937479821041465, 0.7903014639014537, 0.801651371439479, 0.7899756784885371, 0.7939566027168469, 0.809728691069224, 0.8129724590814327, 0.7830477661201193, 0.7943472746510022, 0.8190102118109012, 0.7891780954123438, 0.7771368484407449, 0.7962430371821156, 0.7868370723237934, 0.7911783460498252, 0.8041380856502002, 0.8191570508138987, 0.7859917563197824, 0.7828076280590247, 0.8162908859035114, 0.7889361741036383, 0.780013333697872, 0.7880725378032182, 0.7839109291482725, 0.788300916973245, 0.8075700797686581, 0.8091652024779196, 0.7814653683038085, 0.7762641798563833, 0.820172819037455, 0.7897624877125278, 0.7743987255312237, 0.7970117958177863, 0.7762302273627255, 0.7810282206862937, 0.8134416286774108, 0.8161264286301998, 0.783035817188695, 0.7828988128108024, 0.8192732970185869, 0.7812273714038622, 0.7715714568655745, 0.8049417526648286, 0.7740928013225657, 0.78831988412994], 'Median_Blur': [0.8231944825969398, 0.8207751415632072, 0.8099348367891069, 0.8125799891011171, 0.8437759487179356, 0.8184550205320598, 0.8046438042170271, 0.8198661705795335, 0.8359573065455418, 0.8405779053453467, 0.8462353023913344, 0.8230159439147068, 0.7954840258704491, 0.8009977908488337, 0.8311989166414717, 0.8329175312426993, 0.8231848424981472, 0.8286767348362504, 0.8146677134195667, 0.8229390887193254, 0.8375907270364399, 0.8163149397628828, 0.8133549204914843, 0.8220481436148511, 0.8290869548403118, 0.8299006655339051, 0.8213709027108496, 0.8386356462838157, 0.8149781701227244, 0.8362624057234802, 0.8326848455112502, 0.8196145925617474, 0.809726682752356, 0.8318349292379037, 0.8200944599074539, 0.8292356206029075, 0.8038216510357067, 0.838253152691841, 0.7995699150257952, 0.8243819824900633, 0.8353575570495549, 0.8214880126753744, 0.8119788713485453, 0.807884065083668, 0.8310511094363071, 0.8180580508742045, 0.7951761112954981, 0.82408408931883, 0.7838292080499557, 0.8187872827469976, 0.8300964248627799, 0.8144662698780462, 0.7898383778446084, 0.8054268265139096, 0.823522625772776, 0.7902869272434488, 0.7901049273468606, 0.8142574859296007, 0.7898268060539865, 0.8125417547568711, 0.8219029070195006, 0.8152917735616952, 0.7962449832316235, 0.8029364024150903, 0.8207055682684974, 0.7993239867360789, 0.7817266186364898, 0.8148778120045731, 0.7818585208882481, 0.8161600645239727, 0.8169703330576349, 0.8242683105499288, 0.7984827590324044, 0.8023521886382753, 0.8183012994632207, 0.7976496440627555, 0.7818686023655887, 0.8117743434234401, 0.7753269020176309, 0.8113704357118617, 0.8203370149987729, 0.8215544163286598, 0.8091316668781457, 0.8076548897180489, 0.8141017547432372, 0.7916853092531589, 0.7805076509340712, 0.8068681801675645, 0.7839861562697674, 0.8020725163862972, 0.8081135548059694, 0.8081011106733681, 0.8087742107896373, 0.8041420284839199, 0.8148239414401527, 0.7899748456854431, 0.7735649110624171, 0.7984785348144482, 0.7803382904701762, 0.8055244982475673, 0.8098526523870834, 0.8081930928487812, 0.807617219302475, 0.8028324761256161, 0.8150944611985151, 0.788251341646434, 0.778342126951889, 0.7975081687819908, 0.7869013572700103, 0.7986413984077535, 0.810482557840878, 0.8048294058641913, 0.8106103783314335, 0.7950534479782391, 0.8076993191153993, 0.7791295867549349, 0.7743453242313264, 0.7978222875759095, 0.7834634787732716, 0.7860260920154781, 0.806413880064281, 0.8139310601954347, 0.7985982492572014, 0.8006322702701533, 0.8088987847379937, 0.7738892437279073, 0.7678436895089614, 0.784236479262015, 0.7809063335958807, 0.7972154818865754, 0.80730386815807, 0.8146554443649641, 0.8022108663037515, 0.8066623014937622, 0.8064378025025888, 0.7713012841657095, 0.764140181451897, 0.7905422716832488, 0.7831876184817362, 0.7907570741795341, 0.8144732991439992, 0.808969490881695, 0.8029883769961367, 0.7855107245998229, 0.8144807366924961, 0.7718182384675197, 0.7670271657692224, 0.7901105712497684, 0.7704591607319179, 0.786734650795494, 0.8107185875155899, 0.8115973938342359, 0.8003779471637079, 0.7862808132629636, 0.8144807366924961, 0.768104777919322, 0.77204982712875, 0.7917272922924727, 0.7742419464364176, 0.7807059732673962]}}\n",
      "{'SVM': {'None': [array([[ 94,  15,   5],\n",
      "       [  2, 294,  29],\n",
      "       [  4,  34, 171]]), array([[103,   7,   4],\n",
      "       [  7, 284,  34],\n",
      "       [  7,  33, 169]]), array([[ 93,  18,   3],\n",
      "       [  7, 284,  34],\n",
      "       [  8,  31, 170]]), array([[ 95,  14,   5],\n",
      "       [  9, 283,  33],\n",
      "       [  6,  43, 160]]), array([[104,   7,   3],\n",
      "       [  7, 288,  30],\n",
      "       [  6,  47, 156]]), array([[ 99,  11,   4],\n",
      "       [  5, 289,  30],\n",
      "       [  7,  41, 161]]), array([[ 98,   9,   7],\n",
      "       [  6, 289,  29],\n",
      "       [  9,  41, 159]]), array([[103,   5,   5],\n",
      "       [  5, 292,  27],\n",
      "       [  7,  32, 171]]), array([[ 98,  11,   4],\n",
      "       [  4, 290,  31],\n",
      "       [ 12,  33, 164]]), array([[ 93,  11,   9],\n",
      "       [  4, 294,  27],\n",
      "       [  6,  45, 158]])], 'Gaussian_Blur': [array([[ 96,  15,   3],\n",
      "       [  4, 289,  32],\n",
      "       [  4,  40, 165]]), array([[101,  10,   3],\n",
      "       [  9, 285,  31],\n",
      "       [  8,  35, 166]]), array([[ 94,  16,   4],\n",
      "       [ 11, 282,  32],\n",
      "       [  8,  33, 168]]), array([[101,   9,   4],\n",
      "       [  9, 280,  36],\n",
      "       [  4,  47, 158]]), array([[105,   5,   4],\n",
      "       [  6, 294,  25],\n",
      "       [  9,  48, 152]]), array([[ 97,   8,   9],\n",
      "       [  5, 290,  29],\n",
      "       [  9,  41, 159]]), array([[ 97,  11,   6],\n",
      "       [  6, 285,  33],\n",
      "       [  8,  48, 153]]), array([[100,   9,   4],\n",
      "       [  5, 292,  27],\n",
      "       [  6,  40, 164]]), array([[ 97,  12,   4],\n",
      "       [  5, 286,  34],\n",
      "       [ 12,  35, 162]]), array([[ 90,  14,   9],\n",
      "       [  4, 292,  29],\n",
      "       [  6,  47, 156]])], 'Median_Blur': [array([[ 97,  13,   4],\n",
      "       [  5, 288,  32],\n",
      "       [  5,  40, 164]]), array([[103,   7,   4],\n",
      "       [ 10, 283,  32],\n",
      "       [  5,  39, 165]]), array([[ 93,  17,   4],\n",
      "       [  7, 279,  39],\n",
      "       [  8,  39, 162]]), array([[ 99,   9,   6],\n",
      "       [  9, 277,  39],\n",
      "       [  6,  55, 148]]), array([[104,   6,   4],\n",
      "       [  6, 289,  30],\n",
      "       [ 11,  49, 149]]), array([[ 98,  10,   6],\n",
      "       [  5, 288,  31],\n",
      "       [  7,  44, 158]]), array([[ 99,   8,   7],\n",
      "       [  7, 281,  36],\n",
      "       [ 12,  51, 146]]), array([[ 97,  12,   4],\n",
      "       [  5, 292,  27],\n",
      "       [  8,  48, 154]]), array([[ 96,  11,   6],\n",
      "       [  6, 284,  35],\n",
      "       [ 10,  42, 157]]), array([[ 92,  12,   9],\n",
      "       [  6, 287,  32],\n",
      "       [  5,  53, 151]])]}, 'RF': {'None': [array([[ 93,  18,   3],\n",
      "       [  5, 296,  24],\n",
      "       [  7,  60, 142]]), array([[100,  12,   2],\n",
      "       [  5, 292,  28],\n",
      "       [  8,  54, 147]]), array([[ 87,  24,   3],\n",
      "       [  5, 291,  29],\n",
      "       [ 10,  52, 147]]), array([[ 95,  16,   3],\n",
      "       [  8, 288,  29],\n",
      "       [  7,  69, 133]]), array([[103,   7,   4],\n",
      "       [  7, 292,  26],\n",
      "       [  7,  64, 138]]), array([[ 95,  11,   8],\n",
      "       [  3, 289,  32],\n",
      "       [  9,  62, 138]]), array([[ 95,  14,   5],\n",
      "       [  6, 290,  28],\n",
      "       [  9,  59, 141]]), array([[ 94,  14,   5],\n",
      "       [  5, 292,  27],\n",
      "       [  9,  55, 146]]), array([[ 96,  11,   6],\n",
      "       [  3, 294,  28],\n",
      "       [ 14,  51, 144]]), array([[ 91,  14,   8],\n",
      "       [  3, 297,  25],\n",
      "       [  7,  66, 136]])], 'Gaussian_Blur': [array([[ 89,  23,   2],\n",
      "       [  4, 288,  33],\n",
      "       [  8,  68, 133]]), array([[100,  12,   2],\n",
      "       [  7, 286,  32],\n",
      "       [  9,  56, 144]]), array([[ 88,  24,   2],\n",
      "       [  5, 288,  32],\n",
      "       [  9,  52, 148]]), array([[ 92,  18,   4],\n",
      "       [  7, 283,  35],\n",
      "       [  5,  70, 134]]), array([[ 98,  13,   3],\n",
      "       [  8, 291,  26],\n",
      "       [  8,  59, 142]]), array([[ 99,   9,   6],\n",
      "       [  4, 293,  27],\n",
      "       [  6,  63, 140]]), array([[ 97,  12,   5],\n",
      "       [  8, 277,  39],\n",
      "       [  5,  62, 142]]), array([[ 92,  16,   5],\n",
      "       [  4, 303,  17],\n",
      "       [ 10,  57, 143]]), array([[ 93,  16,   4],\n",
      "       [  6, 287,  32],\n",
      "       [ 11,  56, 142]]), array([[ 91,  15,   7],\n",
      "       [  5, 295,  25],\n",
      "       [  8,  69, 132]])], 'Median_Blur': [array([[ 89,  21,   4],\n",
      "       [  3, 291,  31],\n",
      "       [  8,  56, 145]]), array([[101,  10,   3],\n",
      "       [ 10, 282,  33],\n",
      "       [  9,  64, 136]]), array([[ 86,  22,   6],\n",
      "       [  5, 287,  33],\n",
      "       [  9,  53, 147]]), array([[ 92,  16,   6],\n",
      "       [  8, 281,  36],\n",
      "       [  8,  80, 121]]), array([[100,  11,   3],\n",
      "       [  7, 291,  27],\n",
      "       [ 11,  70, 128]]), array([[ 95,  13,   6],\n",
      "       [  4, 290,  30],\n",
      "       [  6,  63, 140]]), array([[100,   9,   5],\n",
      "       [  5, 284,  35],\n",
      "       [  8,  75, 126]]), array([[ 94,  16,   3],\n",
      "       [  6, 291,  27],\n",
      "       [ 12,  62, 136]]), array([[ 95,  12,   6],\n",
      "       [  4, 290,  31],\n",
      "       [ 13,  57, 139]]), array([[ 92,  12,   9],\n",
      "       [  4, 296,  25],\n",
      "       [  6,  64, 139]])]}, 'KNN': {'None': [array([[ 95,  14,   5],\n",
      "       [  9, 291,  25],\n",
      "       [  3,  36, 170]]), array([[ 97,   9,   8],\n",
      "       [  7, 286,  32],\n",
      "       [  3,  32, 174]]), array([[ 93,  14,   7],\n",
      "       [ 11, 278,  36],\n",
      "       [  6,  45, 158]]), array([[ 98,  10,   6],\n",
      "       [  8, 287,  30],\n",
      "       [  2,  50, 157]]), array([[104,   4,   6],\n",
      "       [ 16, 276,  33],\n",
      "       [  5,  35, 169]]), array([[ 95,  12,   7],\n",
      "       [ 10, 288,  26],\n",
      "       [ 10,  44, 155]]), array([[ 93,  15,   6],\n",
      "       [ 10, 284,  30],\n",
      "       [  5,  36, 168]]), array([[ 99,   4,  10],\n",
      "       [  7, 280,  37],\n",
      "       [  6,  39, 165]]), array([[ 98,  11,   4],\n",
      "       [  6, 277,  42],\n",
      "       [  6,  36, 167]]), array([[ 95,  12,   6],\n",
      "       [  5, 289,  31],\n",
      "       [  2,  36, 171]]), array([[ 95,  14,   5],\n",
      "       [ 10, 292,  23],\n",
      "       [  5,  41, 163]]), array([[100,   9,   5],\n",
      "       [  8, 289,  28],\n",
      "       [ 10,  37, 162]]), array([[ 96,  12,   6],\n",
      "       [ 14, 278,  33],\n",
      "       [ 12,  40, 157]]), array([[ 98,  14,   2],\n",
      "       [  7, 293,  25],\n",
      "       [  8,  52, 149]]), array([[104,   6,   4],\n",
      "       [ 13, 282,  30],\n",
      "       [  8,  46, 155]]), array([[ 99,  11,   4],\n",
      "       [  6, 295,  23],\n",
      "       [ 12,  44, 153]]), array([[ 98,  12,   4],\n",
      "       [  8, 287,  29],\n",
      "       [  8,  42, 159]]), array([[101,   6,   6],\n",
      "       [  6, 290,  28],\n",
      "       [  8,  42, 160]]), array([[ 98,  12,   3],\n",
      "       [  2, 286,  37],\n",
      "       [  9,  39, 161]]), array([[ 95,  12,   6],\n",
      "       [  5, 295,  25],\n",
      "       [  7,  38, 164]]), array([[ 94,  14,   6],\n",
      "       [  4, 298,  23],\n",
      "       [  5,  44, 160]]), array([[102,   9,   3],\n",
      "       [  9, 290,  26],\n",
      "       [  9,  45, 155]]), array([[ 91,  19,   4],\n",
      "       [ 12, 283,  30],\n",
      "       [  8,  45, 156]]), array([[ 93,  19,   2],\n",
      "       [  7, 296,  22],\n",
      "       [  5,  53, 151]]), array([[105,   8,   1],\n",
      "       [  9, 292,  24],\n",
      "       [  9,  55, 145]]), array([[102,  10,   2],\n",
      "       [  6, 295,  23],\n",
      "       [  8,  51, 150]]), array([[ 97,  12,   5],\n",
      "       [ 10, 292,  22],\n",
      "       [  5,  49, 155]]), array([[101,   6,   6],\n",
      "       [  3, 292,  29],\n",
      "       [  7,  48, 155]]), array([[100,  12,   1],\n",
      "       [  3, 288,  34],\n",
      "       [ 10,  41, 158]]), array([[ 94,  13,   6],\n",
      "       [  3, 297,  25],\n",
      "       [  8,  46, 155]]), array([[ 91,  19,   4],\n",
      "       [  6, 292,  27],\n",
      "       [  4,  50, 155]]), array([[103,   9,   2],\n",
      "       [  8, 289,  28],\n",
      "       [  7,  48, 154]]), array([[ 92,  19,   3],\n",
      "       [  8, 296,  21],\n",
      "       [  8,  52, 149]]), array([[ 93,  16,   5],\n",
      "       [  8, 292,  25],\n",
      "       [  7,  58, 144]]), array([[103,   8,   3],\n",
      "       [  9, 291,  25],\n",
      "       [  8,  55, 146]]), array([[ 99,  10,   5],\n",
      "       [  6, 296,  22],\n",
      "       [  7,  57, 145]]), array([[ 98,  12,   4],\n",
      "       [  7, 295,  22],\n",
      "       [ 11,  60, 138]]), array([[ 99,   9,   5],\n",
      "       [  3, 299,  22],\n",
      "       [  7,  53, 150]]), array([[ 97,  13,   3],\n",
      "       [  1, 295,  29],\n",
      "       [ 11,  48, 150]]), array([[ 92,  15,   6],\n",
      "       [  3, 293,  29],\n",
      "       [  8,  51, 150]]), array([[ 89,  22,   3],\n",
      "       [  6, 297,  22],\n",
      "       [  5,  50, 154]]), array([[104,   9,   1],\n",
      "       [ 11, 290,  24],\n",
      "       [  7,  44, 158]]), array([[ 91,  19,   4],\n",
      "       [  7, 298,  20],\n",
      "       [  9,  49, 151]]), array([[ 94,  16,   4],\n",
      "       [  8, 291,  26],\n",
      "       [  5,  63, 141]]), array([[106,   5,   3],\n",
      "       [  8, 296,  21],\n",
      "       [  9,  57, 143]]), array([[101,   8,   5],\n",
      "       [  5, 300,  19],\n",
      "       [ 14,  52, 143]]), array([[ 95,  14,   5],\n",
      "       [  7, 294,  23],\n",
      "       [  8,  58, 143]]), array([[ 97,  12,   4],\n",
      "       [  2, 300,  22],\n",
      "       [  7,  48, 155]]), array([[ 95,  15,   3],\n",
      "       [  1, 294,  30],\n",
      "       [ 11,  46, 152]]), array([[ 92,  16,   5],\n",
      "       [  3, 297,  25],\n",
      "       [  9,  52, 148]]), array([[ 88,  23,   3],\n",
      "       [  7, 297,  21],\n",
      "       [  6,  52, 151]]), array([[104,   8,   2],\n",
      "       [ 10, 290,  25],\n",
      "       [  7,  50, 152]]), array([[ 87,  21,   6],\n",
      "       [  4, 299,  22],\n",
      "       [  7,  51, 151]]), array([[ 91,  20,   3],\n",
      "       [  7, 294,  24],\n",
      "       [  7,  61, 141]]), array([[107,   5,   2],\n",
      "       [  8, 299,  18],\n",
      "       [ 11,  54, 144]]), array([[ 99,  11,   4],\n",
      "       [  5, 296,  23],\n",
      "       [ 12,  57, 140]]), array([[ 94,  17,   3],\n",
      "       [  6, 292,  26],\n",
      "       [ 11,  61, 137]]), array([[ 95,  13,   5],\n",
      "       [  2, 299,  23],\n",
      "       [  8,  54, 148]]), array([[ 95,  15,   3],\n",
      "       [  2, 297,  26],\n",
      "       [ 13,  40, 156]]), array([[ 92,  16,   5],\n",
      "       [  1, 297,  27],\n",
      "       [  7,  57, 145]]), array([[ 90,  21,   3],\n",
      "       [  6, 299,  20],\n",
      "       [  7,  56, 146]]), array([[103,   8,   3],\n",
      "       [  6, 294,  25],\n",
      "       [  6,  54, 149]]), array([[ 87,  22,   5],\n",
      "       [  4, 302,  19],\n",
      "       [  7,  53, 149]]), array([[ 91,  19,   4],\n",
      "       [  7, 296,  22],\n",
      "       [  7,  61, 141]]), array([[105,   4,   5],\n",
      "       [  7, 299,  19],\n",
      "       [ 10,  58, 141]]), array([[ 97,  12,   5],\n",
      "       [  5, 297,  22],\n",
      "       [ 14,  60, 135]]), array([[ 93,  18,   3],\n",
      "       [  6, 293,  25],\n",
      "       [ 11,  64, 134]]), array([[ 96,  12,   5],\n",
      "       [  4, 296,  24],\n",
      "       [  8,  61, 141]]), array([[ 95,  14,   4],\n",
      "       [  3, 295,  27],\n",
      "       [ 15,  50, 144]]), array([[ 91,  16,   6],\n",
      "       [  1, 297,  27],\n",
      "       [  6,  54, 149]]), array([[ 89,  22,   3],\n",
      "       [  5, 301,  19],\n",
      "       [  7,  62, 140]]), array([[103,   8,   3],\n",
      "       [  7, 300,  18],\n",
      "       [  6,  50, 153]]), array([[ 88,  22,   4],\n",
      "       [  2, 305,  18],\n",
      "       [  9,  58, 142]]), array([[ 90,  20,   4],\n",
      "       [  7, 299,  19],\n",
      "       [  7,  63, 139]]), array([[104,   7,   3],\n",
      "       [  7, 300,  18],\n",
      "       [  9,  55, 145]]), array([[ 97,  13,   4],\n",
      "       [  4, 300,  20],\n",
      "       [ 11,  60, 138]]), array([[ 96,  16,   2],\n",
      "       [  6, 296,  22],\n",
      "       [ 11,  68, 130]]), array([[ 97,  10,   6],\n",
      "       [  3, 299,  22],\n",
      "       [  9,  60, 141]]), array([[ 95,  14,   4],\n",
      "       [  2, 298,  25],\n",
      "       [ 13,  50, 146]]), array([[ 90,  16,   7],\n",
      "       [  1, 298,  26],\n",
      "       [  6,  56, 147]]), array([[ 92,  19,   3],\n",
      "       [  5, 299,  21],\n",
      "       [  6,  62, 141]]), array([[103,   8,   3],\n",
      "       [  8, 298,  19],\n",
      "       [  6,  53, 150]]), array([[ 88,  22,   4],\n",
      "       [  2, 304,  19],\n",
      "       [  9,  64, 136]]), array([[ 93,  19,   2],\n",
      "       [  6, 303,  16],\n",
      "       [  6,  67, 136]]), array([[103,   9,   2],\n",
      "       [  5, 301,  19],\n",
      "       [ 10,  61, 138]]), array([[ 95,  15,   4],\n",
      "       [  4, 299,  21],\n",
      "       [ 12,  63, 134]]), array([[ 94,  17,   3],\n",
      "       [  5, 296,  23],\n",
      "       [ 11,  68, 130]]), array([[ 97,  11,   5],\n",
      "       [  5, 299,  20],\n",
      "       [  9,  60, 141]]), array([[ 95,  13,   5],\n",
      "       [  1, 295,  29],\n",
      "       [ 13,  54, 142]]), array([[ 92,  15,   6],\n",
      "       [  1, 299,  25],\n",
      "       [  6,  59, 144]]), array([[ 93,  18,   3],\n",
      "       [  5, 298,  22],\n",
      "       [  7,  64, 138]]), array([[104,   8,   2],\n",
      "       [  9, 297,  19],\n",
      "       [  6,  55, 148]]), array([[ 87,  22,   5],\n",
      "       [  2, 304,  19],\n",
      "       [  8,  65, 136]]), array([[ 94,  18,   2],\n",
      "       [  6, 302,  17],\n",
      "       [  6,  67, 136]]), array([[104,   8,   2],\n",
      "       [  5, 304,  16],\n",
      "       [  9,  61, 139]]), array([[ 94,  16,   4],\n",
      "       [  4, 301,  19],\n",
      "       [ 11,  64, 134]]), array([[ 93,  18,   3],\n",
      "       [  5, 295,  24],\n",
      "       [ 11,  69, 129]]), array([[ 94,  15,   4],\n",
      "       [  6, 295,  23],\n",
      "       [  9,  64, 137]]), array([[ 93,  14,   6],\n",
      "       [  1, 295,  29],\n",
      "       [ 13,  52, 144]]), array([[ 92,  14,   7],\n",
      "       [  1, 300,  24],\n",
      "       [  6,  61, 142]]), array([[ 93,  19,   2],\n",
      "       [  6, 295,  24],\n",
      "       [  8,  67, 134]]), array([[104,   8,   2],\n",
      "       [  8, 295,  22],\n",
      "       [  6,  58, 145]]), array([[ 87,  22,   5],\n",
      "       [  2, 304,  19],\n",
      "       [  9,  61, 139]]), array([[ 94,  17,   3],\n",
      "       [  5, 304,  16],\n",
      "       [  7,  74, 128]]), array([[104,   7,   3],\n",
      "       [  4, 303,  18],\n",
      "       [ 10,  65, 134]]), array([[ 94,  15,   5],\n",
      "       [  3, 300,  21],\n",
      "       [ 11,  60, 138]]), array([[ 93,  18,   3],\n",
      "       [  5, 298,  21],\n",
      "       [ 11,  69, 129]]), array([[ 94,  15,   4],\n",
      "       [  5, 298,  21],\n",
      "       [  9,  60, 141]]), array([[ 93,  15,   5],\n",
      "       [  1, 298,  26],\n",
      "       [ 13,  54, 142]]), array([[ 90,  15,   8],\n",
      "       [  1, 302,  22],\n",
      "       [  6,  63, 140]]), array([[ 92,  19,   3],\n",
      "       [  5, 299,  21],\n",
      "       [  7,  69, 133]]), array([[103,  10,   1],\n",
      "       [  9, 297,  19],\n",
      "       [  7,  53, 149]]), array([[ 87,  22,   5],\n",
      "       [  3, 304,  18],\n",
      "       [  8,  65, 136]]), array([[ 92,  20,   2],\n",
      "       [  5, 303,  17],\n",
      "       [  7,  74, 128]]), array([[102,  10,   2],\n",
      "       [  4, 304,  17],\n",
      "       [ 10,  64, 135]]), array([[ 95,  14,   5],\n",
      "       [  3, 299,  22],\n",
      "       [ 12,  64, 133]]), array([[ 94,  17,   3],\n",
      "       [  5, 295,  24],\n",
      "       [ 11,  76, 122]]), array([[ 94,  16,   3],\n",
      "       [  6, 297,  21],\n",
      "       [ 10,  63, 137]]), array([[ 94,  14,   5],\n",
      "       [  2, 295,  28],\n",
      "       [ 13,  57, 139]]), array([[ 90,  15,   8],\n",
      "       [  1, 303,  21],\n",
      "       [  7,  68, 134]]), array([[ 91,  20,   3],\n",
      "       [  6, 297,  22],\n",
      "       [  7,  66, 136]]), array([[104,  10,   0],\n",
      "       [  5, 300,  20],\n",
      "       [  6,  58, 145]]), array([[ 88,  22,   4],\n",
      "       [  4, 302,  19],\n",
      "       [  8,  65, 136]]), array([[ 91,  19,   4],\n",
      "       [  5, 302,  18],\n",
      "       [  7,  75, 127]]), array([[100,  11,   3],\n",
      "       [  4, 305,  16],\n",
      "       [ 10,  66, 133]]), array([[ 95,  15,   4],\n",
      "       [  3, 301,  20],\n",
      "       [ 11,  68, 130]]), array([[ 95,  16,   3],\n",
      "       [  5, 291,  28],\n",
      "       [ 11,  73, 125]]), array([[ 95,  16,   2],\n",
      "       [  6, 302,  16],\n",
      "       [ 10,  62, 138]]), array([[ 94,  15,   4],\n",
      "       [  2, 296,  27],\n",
      "       [ 13,  58, 138]]), array([[ 91,  16,   6],\n",
      "       [  1, 301,  23],\n",
      "       [  7,  68, 134]]), array([[ 88,  24,   2],\n",
      "       [  6, 298,  21],\n",
      "       [  7,  71, 131]]), array([[104,   9,   1],\n",
      "       [  5, 298,  22],\n",
      "       [  7,  63, 139]]), array([[ 87,  23,   4],\n",
      "       [  3, 304,  18],\n",
      "       [  8,  65, 136]]), array([[ 91,  19,   4],\n",
      "       [  6, 303,  16],\n",
      "       [  7,  78, 124]]), array([[100,  11,   3],\n",
      "       [  5, 304,  16],\n",
      "       [ 10,  66, 133]]), array([[ 94,  16,   4],\n",
      "       [  4, 298,  22],\n",
      "       [ 10,  69, 130]]), array([[ 93,  17,   4],\n",
      "       [  5, 293,  26],\n",
      "       [ 12,  75, 122]]), array([[ 95,  16,   2],\n",
      "       [  6, 301,  17],\n",
      "       [ 10,  64, 136]]), array([[ 94,  15,   4],\n",
      "       [  2, 299,  24],\n",
      "       [ 13,  56, 140]]), array([[ 91,  16,   6],\n",
      "       [  1, 303,  21],\n",
      "       [  7,  70, 132]]), array([[ 89,  23,   2],\n",
      "       [  6, 299,  20],\n",
      "       [  6,  69, 134]]), array([[103,   9,   2],\n",
      "       [  6, 298,  21],\n",
      "       [  6,  64, 139]]), array([[ 86,  24,   4],\n",
      "       [  3, 303,  19],\n",
      "       [  9,  63, 137]]), array([[ 92,  18,   4],\n",
      "       [  6, 302,  17],\n",
      "       [  7,  78, 124]]), array([[100,  11,   3],\n",
      "       [  5, 304,  16],\n",
      "       [ 10,  64, 135]]), array([[ 94,  16,   4],\n",
      "       [  3, 298,  23],\n",
      "       [ 11,  68, 130]]), array([[ 94,  16,   4],\n",
      "       [  5, 292,  27],\n",
      "       [ 11,  78, 120]]), array([[ 96,  15,   2],\n",
      "       [  6, 300,  18],\n",
      "       [ 10,  65, 135]]), array([[ 94,  13,   6],\n",
      "       [  3, 299,  23],\n",
      "       [ 13,  56, 140]]), array([[ 92,  16,   5],\n",
      "       [  1, 305,  19],\n",
      "       [  6,  72, 131]]), array([[ 90,  22,   2],\n",
      "       [  5, 299,  21],\n",
      "       [  6,  68, 135]]), array([[102,  10,   2],\n",
      "       [  7, 297,  21],\n",
      "       [  7,  64, 138]]), array([[ 86,  24,   4],\n",
      "       [  3, 304,  18],\n",
      "       [ 10,  64, 135]]), array([[ 91,  18,   5],\n",
      "       [  7, 300,  18],\n",
      "       [  7,  78, 124]]), array([[101,  10,   3],\n",
      "       [  5, 303,  17],\n",
      "       [ 10,  63, 136]]), array([[ 94,  16,   4],\n",
      "       [  3, 298,  23],\n",
      "       [ 11,  74, 124]]), array([[ 92,  17,   5],\n",
      "       [  5, 294,  25],\n",
      "       [ 10,  78, 121]]), array([[ 96,  15,   2],\n",
      "       [  6, 300,  18],\n",
      "       [ 10,  63, 137]]), array([[ 97,  12,   4],\n",
      "       [  2, 298,  25],\n",
      "       [ 13,  57, 139]]), array([[ 92,  16,   5],\n",
      "       [  2, 304,  19],\n",
      "       [  7,  73, 129]])], 'Gaussian_Blur': [array([[ 94,  17,   3],\n",
      "       [  6, 280,  39],\n",
      "       [  3,  44, 162]]), array([[ 97,  10,   7],\n",
      "       [  5, 292,  28],\n",
      "       [  6,  44, 159]]), array([[ 92,  15,   7],\n",
      "       [  8, 276,  41],\n",
      "       [  9,  53, 147]]), array([[ 96,  14,   4],\n",
      "       [ 10, 289,  26],\n",
      "       [  3,  40, 166]]), array([[101,   8,   5],\n",
      "       [ 10, 274,  41],\n",
      "       [  6,  42, 161]]), array([[ 96,  10,   8],\n",
      "       [  9, 284,  31],\n",
      "       [  8,  43, 158]]), array([[ 92,  14,   8],\n",
      "       [  9, 278,  37],\n",
      "       [  7,  38, 164]]), array([[ 98,   6,   9],\n",
      "       [  8, 283,  33],\n",
      "       [  6,  39, 165]]), array([[ 92,  15,   6],\n",
      "       [  5, 282,  38],\n",
      "       [  7,  31, 171]]), array([[ 93,  11,   9],\n",
      "       [  3, 288,  34],\n",
      "       [  4,  41, 164]]), array([[ 91,  21,   2],\n",
      "       [  7, 290,  28],\n",
      "       [  8,  42, 159]]), array([[ 99,  10,   5],\n",
      "       [  6, 287,  32],\n",
      "       [ 13,  39, 157]]), array([[ 95,  15,   4],\n",
      "       [ 11, 279,  35],\n",
      "       [  9,  47, 153]]), array([[ 99,  12,   3],\n",
      "       [  9, 288,  28],\n",
      "       [  6,  48, 155]]), array([[106,   6,   2],\n",
      "       [ 10, 281,  34],\n",
      "       [  8,  50, 151]]), array([[101,  11,   2],\n",
      "       [  7, 285,  32],\n",
      "       [ 11,  53, 145]]), array([[101,   9,   4],\n",
      "       [  7, 285,  32],\n",
      "       [ 11,  55, 143]]), array([[ 98,   9,   6],\n",
      "       [  9, 285,  30],\n",
      "       [  9,  41, 160]]), array([[ 96,  15,   2],\n",
      "       [  4, 286,  35],\n",
      "       [  9,  39, 161]]), array([[ 93,  13,   7],\n",
      "       [  6, 291,  28],\n",
      "       [  8,  41, 160]]), array([[ 94,  17,   3],\n",
      "       [  4, 300,  21],\n",
      "       [  6,  54, 149]]), array([[ 97,  14,   3],\n",
      "       [  6, 289,  30],\n",
      "       [ 10,  56, 143]]), array([[ 98,  13,   3],\n",
      "       [  9, 293,  23],\n",
      "       [  9,  51, 149]]), array([[ 96,  14,   4],\n",
      "       [  8, 288,  29],\n",
      "       [  4,  53, 152]]), array([[103,   9,   2],\n",
      "       [  7, 290,  28],\n",
      "       [  8,  57, 144]]), array([[101,  12,   1],\n",
      "       [  6, 291,  27],\n",
      "       [ 11,  56, 142]]), array([[ 98,  10,   6],\n",
      "       [  7, 293,  24],\n",
      "       [  5,  62, 142]]), array([[ 95,  11,   7],\n",
      "       [  6, 293,  25],\n",
      "       [ 10,  47, 153]]), array([[ 97,  15,   1],\n",
      "       [  3, 287,  35],\n",
      "       [ 10,  46, 153]]), array([[ 95,  12,   6],\n",
      "       [  4, 293,  28],\n",
      "       [ 11,  52, 146]]), array([[ 90,  21,   3],\n",
      "       [  4, 298,  23],\n",
      "       [  5,  56, 148]]), array([[ 98,  13,   3],\n",
      "       [  9, 291,  25],\n",
      "       [  9,  51, 149]]), array([[ 95,  15,   4],\n",
      "       [  8, 290,  27],\n",
      "       [  7,  51, 151]]), array([[ 93,  15,   6],\n",
      "       [  8, 287,  30],\n",
      "       [  4,  58, 147]]), array([[101,  11,   2],\n",
      "       [  6, 295,  24],\n",
      "       [  5,  55, 149]]), array([[ 98,  12,   4],\n",
      "       [  5, 295,  24],\n",
      "       [ 10,  62, 137]]), array([[ 97,  11,   6],\n",
      "       [  4, 298,  22],\n",
      "       [  9,  71, 129]]), array([[ 96,  11,   6],\n",
      "       [  8, 291,  25],\n",
      "       [  9,  45, 156]]), array([[ 95,  15,   3],\n",
      "       [  3, 294,  28],\n",
      "       [ 11,  48, 150]]), array([[ 93,  13,   7],\n",
      "       [  3, 293,  29],\n",
      "       [  9,  55, 145]]), array([[ 93,  19,   2],\n",
      "       [  4, 296,  25],\n",
      "       [  5,  57, 147]]), array([[ 98,  12,   4],\n",
      "       [  8, 290,  27],\n",
      "       [  8,  54, 147]]), array([[ 92,  19,   3],\n",
      "       [ 10, 289,  26],\n",
      "       [  8,  50, 151]]), array([[ 93,  15,   6],\n",
      "       [  9, 287,  29],\n",
      "       [  3,  59, 147]]), array([[100,  11,   3],\n",
      "       [  6, 296,  23],\n",
      "       [  6,  57, 146]]), array([[ 99,   9,   6],\n",
      "       [  4, 297,  23],\n",
      "       [ 10,  62, 137]]), array([[ 96,  11,   7],\n",
      "       [  4, 297,  23],\n",
      "       [  8,  71, 130]]), array([[ 97,  11,   5],\n",
      "       [  7, 291,  26],\n",
      "       [ 10,  54, 146]]), array([[ 94,  16,   3],\n",
      "       [  1, 298,  26],\n",
      "       [ 13,  51, 145]]), array([[ 93,  13,   7],\n",
      "       [  2, 298,  25],\n",
      "       [ 11,  55, 143]]), array([[ 97,  15,   2],\n",
      "       [  5, 297,  23],\n",
      "       [  4,  61, 144]]), array([[ 99,  14,   1],\n",
      "       [  8, 291,  26],\n",
      "       [  7,  55, 147]]), array([[ 88,  19,   7],\n",
      "       [  7, 295,  23],\n",
      "       [  7,  56, 146]]), array([[ 92,  16,   6],\n",
      "       [  7, 292,  26],\n",
      "       [  3,  65, 141]]), array([[100,  11,   3],\n",
      "       [  6, 296,  23],\n",
      "       [  8,  56, 145]]), array([[ 97,  11,   6],\n",
      "       [  6, 296,  22],\n",
      "       [  9,  66, 134]]), array([[100,  10,   4],\n",
      "       [  5, 295,  24],\n",
      "       [ 10,  71, 128]]), array([[ 97,  12,   4],\n",
      "       [  5, 295,  24],\n",
      "       [  8,  59, 143]]), array([[ 93,  16,   4],\n",
      "       [  3, 293,  29],\n",
      "       [ 14,  51, 144]]), array([[ 93,  15,   5],\n",
      "       [  3, 293,  29],\n",
      "       [  8,  66, 135]]), array([[ 94,  18,   2],\n",
      "       [  3, 299,  23],\n",
      "       [  5,  59, 145]]), array([[ 98,  14,   2],\n",
      "       [  6, 294,  25],\n",
      "       [  7,  61, 141]]), array([[ 87,  20,   7],\n",
      "       [  7, 300,  18],\n",
      "       [  9,  50, 150]]), array([[ 89,  19,   6],\n",
      "       [  6, 297,  22],\n",
      "       [  5,  68, 136]]), array([[100,  10,   4],\n",
      "       [  6, 298,  21],\n",
      "       [  8,  54, 147]]), array([[ 97,  12,   5],\n",
      "       [  5, 297,  22],\n",
      "       [ 10,  61, 138]]), array([[ 97,  11,   6],\n",
      "       [  5, 294,  25],\n",
      "       [ 11,  71, 127]]), array([[ 95,  13,   5],\n",
      "       [  5, 292,  27],\n",
      "       [  9,  64, 137]]), array([[ 94,  14,   5],\n",
      "       [  3, 294,  28],\n",
      "       [ 14,  53, 142]]), array([[ 93,  14,   6],\n",
      "       [  2, 299,  24],\n",
      "       [  8,  60, 141]]), array([[ 93,  18,   3],\n",
      "       [  3, 298,  24],\n",
      "       [  5,  61, 143]]), array([[ 99,  12,   3],\n",
      "       [  6, 292,  27],\n",
      "       [  7,  61, 141]]), array([[ 87,  20,   7],\n",
      "       [  6, 297,  22],\n",
      "       [  8,  56, 145]]), array([[ 89,  20,   5],\n",
      "       [  6, 295,  24],\n",
      "       [  5,  66, 138]]), array([[102,  10,   2],\n",
      "       [  7, 295,  23],\n",
      "       [ 10,  55, 144]]), array([[ 96,  13,   5],\n",
      "       [  4, 297,  23],\n",
      "       [ 11,  66, 132]]), array([[ 97,  13,   4],\n",
      "       [  5, 293,  26],\n",
      "       [ 11,  65, 133]]), array([[ 93,  15,   5],\n",
      "       [  5, 293,  26],\n",
      "       [  9,  67, 134]]), array([[ 96,  14,   3],\n",
      "       [  2, 295,  28],\n",
      "       [ 12,  61, 136]]), array([[ 92,  15,   6],\n",
      "       [  5, 297,  23],\n",
      "       [  8,  63, 138]]), array([[ 93,  18,   3],\n",
      "       [  3, 302,  20],\n",
      "       [  5,  69, 135]]), array([[100,  12,   2],\n",
      "       [  9, 291,  25],\n",
      "       [  8,  67, 134]]), array([[ 85,  22,   7],\n",
      "       [  6, 296,  23],\n",
      "       [  7,  55, 147]]), array([[ 88,  22,   4],\n",
      "       [  6, 296,  23],\n",
      "       [  6,  72, 131]]), array([[100,  11,   3],\n",
      "       [  6, 297,  22],\n",
      "       [ 10,  52, 147]]), array([[ 96,  13,   5],\n",
      "       [  5, 295,  24],\n",
      "       [ 11,  64, 134]]), array([[ 97,  13,   4],\n",
      "       [  6, 295,  23],\n",
      "       [ 11,  67, 131]]), array([[ 93,  14,   6],\n",
      "       [  5, 296,  23],\n",
      "       [  8,  69, 133]]), array([[ 93,  16,   4],\n",
      "       [  4, 292,  29],\n",
      "       [ 12,  66, 131]]), array([[ 91,  15,   7],\n",
      "       [  6, 297,  22],\n",
      "       [  7,  66, 136]]), array([[ 92,  19,   3],\n",
      "       [  3, 299,  23],\n",
      "       [  5,  69, 135]]), array([[ 99,  12,   3],\n",
      "       [  8, 292,  25],\n",
      "       [  8,  67, 134]]), array([[ 85,  22,   7],\n",
      "       [  4, 298,  23],\n",
      "       [  8,  60, 141]]), array([[ 91,  20,   3],\n",
      "       [  4, 294,  27],\n",
      "       [  5,  71, 133]]), array([[ 99,  12,   3],\n",
      "       [  6, 300,  19],\n",
      "       [  9,  56, 144]]), array([[ 96,  13,   5],\n",
      "       [  5, 296,  23],\n",
      "       [ 11,  66, 132]]), array([[ 92,  17,   5],\n",
      "       [  5, 295,  24],\n",
      "       [ 11,  72, 126]]), array([[ 95,  14,   4],\n",
      "       [  4, 302,  18],\n",
      "       [  8,  67, 135]]), array([[ 91,  17,   5],\n",
      "       [  4, 293,  28],\n",
      "       [ 13,  62, 134]]), array([[ 91,  15,   7],\n",
      "       [  2, 298,  25],\n",
      "       [  8,  64, 137]]), array([[ 92,  19,   3],\n",
      "       [  3, 298,  24],\n",
      "       [  5,  71, 133]]), array([[ 99,  12,   3],\n",
      "       [  9, 294,  22],\n",
      "       [  8,  69, 132]]), array([[ 86,  20,   8],\n",
      "       [  4, 300,  21],\n",
      "       [  8,  56, 145]]), array([[ 88,  21,   5],\n",
      "       [  4, 294,  27],\n",
      "       [  6,  72, 131]]), array([[100,  12,   2],\n",
      "       [  7, 301,  17],\n",
      "       [  9,  58, 142]]), array([[ 94,  14,   6],\n",
      "       [  5, 295,  24],\n",
      "       [ 10,  68, 131]]), array([[ 95,  14,   5],\n",
      "       [  5, 294,  25],\n",
      "       [ 11,  73, 125]]), array([[ 93,  15,   5],\n",
      "       [  4, 301,  19],\n",
      "       [  9,  70, 131]]), array([[ 91,  17,   5],\n",
      "       [  3, 296,  26],\n",
      "       [ 13,  64, 132]]), array([[ 91,  15,   7],\n",
      "       [  2, 298,  25],\n",
      "       [  8,  68, 133]]), array([[ 92,  19,   3],\n",
      "       [  5, 296,  24],\n",
      "       [  5,  70, 134]]), array([[ 98,  13,   3],\n",
      "       [ 10, 297,  18],\n",
      "       [  9,  69, 131]]), array([[ 86,  22,   6],\n",
      "       [  4, 301,  20],\n",
      "       [  8,  60, 141]]), array([[ 88,  21,   5],\n",
      "       [  4, 293,  28],\n",
      "       [  6,  74, 129]]), array([[ 99,  13,   2],\n",
      "       [  7, 297,  21],\n",
      "       [ 10,  59, 140]]), array([[ 95,  14,   5],\n",
      "       [  4, 296,  24],\n",
      "       [ 12,  70, 127]]), array([[ 96,  12,   6],\n",
      "       [  5, 294,  25],\n",
      "       [ 11,  73, 125]]), array([[ 92,  16,   5],\n",
      "       [  4, 300,  20],\n",
      "       [  9,  70, 131]]), array([[ 91,  17,   5],\n",
      "       [  3, 295,  27],\n",
      "       [ 12,  67, 130]]), array([[ 91,  18,   4],\n",
      "       [  2, 298,  25],\n",
      "       [  9,  72, 128]]), array([[ 93,  19,   2],\n",
      "       [  2, 301,  22],\n",
      "       [  6,  71, 132]]), array([[ 99,  12,   3],\n",
      "       [  8, 300,  17],\n",
      "       [  7,  72, 130]]), array([[ 80,  26,   8],\n",
      "       [  4, 300,  21],\n",
      "       [  7,  65, 137]]), array([[ 89,  22,   3],\n",
      "       [  4, 296,  25],\n",
      "       [  5,  72, 132]]), array([[ 97,  14,   3],\n",
      "       [  6, 298,  21],\n",
      "       [ 10,  60, 139]]), array([[ 95,  14,   5],\n",
      "       [  5, 294,  25],\n",
      "       [ 12,  71, 126]]), array([[ 92,  16,   6],\n",
      "       [  5, 292,  27],\n",
      "       [ 10,  76, 123]]), array([[ 93,  16,   4],\n",
      "       [  4, 302,  18],\n",
      "       [  8,  78, 124]]), array([[ 91,  17,   5],\n",
      "       [  3, 295,  27],\n",
      "       [ 12,  69, 128]]), array([[ 93,  15,   5],\n",
      "       [  3, 296,  26],\n",
      "       [  8,  76, 125]]), array([[ 92,  20,   2],\n",
      "       [  3, 300,  22],\n",
      "       [  6,  72, 131]]), array([[100,  12,   2],\n",
      "       [  8, 300,  17],\n",
      "       [  8,  68, 133]]), array([[ 81,  26,   7],\n",
      "       [  4, 301,  20],\n",
      "       [  8,  64, 137]]), array([[ 88,  21,   5],\n",
      "       [  4, 293,  28],\n",
      "       [  4,  77, 128]]), array([[ 96,  14,   4],\n",
      "       [  6, 297,  22],\n",
      "       [  9,  61, 139]]), array([[ 95,  14,   5],\n",
      "       [  6, 295,  23],\n",
      "       [ 13,  70, 126]]), array([[ 93,  15,   6],\n",
      "       [  5, 294,  25],\n",
      "       [ 10,  77, 122]]), array([[ 92,  16,   5],\n",
      "       [  4, 302,  18],\n",
      "       [  9,  80, 121]]), array([[ 91,  18,   4],\n",
      "       [  3, 295,  27],\n",
      "       [ 12,  71, 126]]), array([[ 91,  16,   6],\n",
      "       [  2, 299,  24],\n",
      "       [  9,  76, 124]]), array([[ 92,  20,   2],\n",
      "       [  2, 301,  22],\n",
      "       [  6,  71, 132]]), array([[ 98,  12,   4],\n",
      "       [  7, 299,  19],\n",
      "       [  8,  71, 130]]), array([[ 81,  25,   8],\n",
      "       [  4, 302,  19],\n",
      "       [  9,  66, 134]]), array([[ 87,  21,   6],\n",
      "       [  4, 293,  28],\n",
      "       [  5,  78, 126]]), array([[ 97,  13,   4],\n",
      "       [  7, 297,  21],\n",
      "       [  8,  61, 140]]), array([[ 95,  14,   5],\n",
      "       [  6, 295,  23],\n",
      "       [ 12,  71, 126]]), array([[ 91,  15,   8],\n",
      "       [  5, 294,  25],\n",
      "       [ 11,  76, 122]]), array([[ 93,  16,   4],\n",
      "       [  4, 304,  16],\n",
      "       [  8,  79, 123]]), array([[ 91,  19,   3],\n",
      "       [  2, 294,  29],\n",
      "       [ 15,  71, 123]]), array([[ 88,  19,   6],\n",
      "       [  3, 299,  23],\n",
      "       [  9,  76, 124]]), array([[ 93,  19,   2],\n",
      "       [  2, 302,  21],\n",
      "       [  5,  71, 133]]), array([[ 99,  13,   2],\n",
      "       [  7, 300,  18],\n",
      "       [  8,  69, 132]]), array([[ 80,  26,   8],\n",
      "       [  4, 302,  19],\n",
      "       [  8,  65, 136]]), array([[ 88,  22,   4],\n",
      "       [  4, 295,  26],\n",
      "       [  5,  77, 127]]), array([[ 97,  13,   4],\n",
      "       [  8, 298,  19],\n",
      "       [  8,  62, 139]]), array([[ 94,  15,   5],\n",
      "       [  6, 295,  23],\n",
      "       [ 12,  75, 122]]), array([[ 91,  15,   8],\n",
      "       [  5, 295,  24],\n",
      "       [ 10,  80, 119]]), array([[ 94,  15,   4],\n",
      "       [  5, 304,  15],\n",
      "       [  8,  75, 127]]), array([[ 89,  20,   4],\n",
      "       [  2, 294,  29],\n",
      "       [ 14,  71, 124]]), array([[ 90,  18,   5],\n",
      "       [  3, 302,  20],\n",
      "       [ 10,  75, 124]])], 'Median_Blur': [array([[ 95,  15,   4],\n",
      "       [  6, 278,  41],\n",
      "       [  6,  44, 159]]), array([[ 94,  11,   9],\n",
      "       [  8, 274,  43],\n",
      "       [  3,  44, 162]]), array([[ 87,  14,  13],\n",
      "       [  6, 286,  33],\n",
      "       [  5,  48, 156]]), array([[ 92,  14,   8],\n",
      "       [  9, 279,  37],\n",
      "       [  4,  49, 156]]), array([[104,   7,   3],\n",
      "       [ 11, 285,  29],\n",
      "       [  7,  45, 157]]), array([[ 95,  10,   9],\n",
      "       [  9, 277,  38],\n",
      "       [  8,  42, 159]]), array([[ 95,  14,   5],\n",
      "       [ 12, 267,  45],\n",
      "       [  7,  44, 158]]), array([[ 96,   7,  10],\n",
      "       [ 10, 273,  41],\n",
      "       [  8,  40, 162]]), array([[ 96,  12,   5],\n",
      "       [  8, 284,  33],\n",
      "       [  4,  45, 160]]), array([[ 93,  14,   6],\n",
      "       [  6, 289,  30],\n",
      "       [  6,  39, 164]]), array([[ 96,  14,   4],\n",
      "       [  3, 300,  22],\n",
      "       [ 10,  43, 156]]), array([[100,  10,   4],\n",
      "       [  8, 282,  35],\n",
      "       [ 11,  46, 152]]), array([[ 89,  16,   9],\n",
      "       [ 13, 283,  29],\n",
      "       [ 11,  47, 151]]), array([[ 95,  13,   6],\n",
      "       [ 11, 282,  32],\n",
      "       [  8,  57, 144]]), array([[104,   8,   2],\n",
      "       [ 14, 294,  17],\n",
      "       [ 11,  54, 144]]), array([[ 98,  12,   4],\n",
      "       [  5, 292,  27],\n",
      "       [ 14,  42, 153]]), array([[ 99,   9,   6],\n",
      "       [  9, 285,  30],\n",
      "       [ 11,  47, 151]]), array([[ 98,  11,   4],\n",
      "       [ 11, 280,  33],\n",
      "       [ 10,  40, 160]]), array([[ 95,  16,   2],\n",
      "       [  7, 289,  29],\n",
      "       [ 15,  46, 148]]), array([[ 88,  17,   8],\n",
      "       [  6, 291,  28],\n",
      "       [  6,  45, 158]]), array([[ 92,  17,   5],\n",
      "       [  4, 300,  21],\n",
      "       [  9,  44, 156]]), array([[ 98,  12,   4],\n",
      "       [  5, 288,  32],\n",
      "       [  9,  57, 143]]), array([[ 91,  18,   5],\n",
      "       [ 11, 290,  24],\n",
      "       [  9,  48, 152]]), array([[ 96,  14,   4],\n",
      "       [  9, 290,  26],\n",
      "       [  6,  55, 148]]), array([[104,   8,   2],\n",
      "       [  9, 297,  19],\n",
      "       [ 10,  61, 138]]), array([[ 99,  10,   5],\n",
      "       [  3, 296,  25],\n",
      "       [ 11,  54, 144]]), array([[ 98,  11,   5],\n",
      "       [  5, 296,  23],\n",
      "       [ 10,  59, 140]]), array([[ 94,  14,   5],\n",
      "       [  7, 296,  21],\n",
      "       [  7,  47, 156]]), array([[ 95,  16,   2],\n",
      "       [  5, 290,  30],\n",
      "       [ 12,  52, 145]]), array([[ 94,  14,   5],\n",
      "       [  2, 293,  30],\n",
      "       [  6,  49, 154]]), array([[ 91,  19,   4],\n",
      "       [  3, 304,  18],\n",
      "       [  7,  53, 149]]), array([[ 98,  12,   4],\n",
      "       [  5, 292,  28],\n",
      "       [  9,  58, 142]]), array([[ 90,  18,   6],\n",
      "       [  7, 292,  26],\n",
      "       [  9,  52, 148]]), array([[ 98,  12,   4],\n",
      "       [  7, 297,  21],\n",
      "       [  4,  61, 144]]), array([[101,  10,   3],\n",
      "       [  8, 302,  15],\n",
      "       [ 10,  67, 132]]), array([[100,  12,   2],\n",
      "       [  4, 294,  26],\n",
      "       [  8,  59, 142]]), array([[100,  10,   4],\n",
      "       [  7, 293,  24],\n",
      "       [ 10,  70, 129]]), array([[ 95,  14,   4],\n",
      "       [  6, 297,  21],\n",
      "       [  9,  47, 154]]), array([[ 94,  18,   1],\n",
      "       [  4, 289,  32],\n",
      "       [ 13,  59, 137]]), array([[ 93,  14,   6],\n",
      "       [  0, 299,  26],\n",
      "       [  8,  58, 143]]), array([[ 92,  20,   2],\n",
      "       [  3, 302,  20],\n",
      "       [  6,  53, 150]]), array([[ 99,  13,   2],\n",
      "       [  6, 290,  29],\n",
      "       [  8,  58, 143]]), array([[ 90,  17,   7],\n",
      "       [  5, 298,  22],\n",
      "       [  9,  56, 144]]), array([[ 96,  14,   4],\n",
      "       [  7, 294,  24],\n",
      "       [  5,  70, 134]]), array([[102,   9,   3],\n",
      "       [  6, 300,  19],\n",
      "       [  9,  62, 138]]), array([[ 99,  13,   2],\n",
      "       [  3, 299,  22],\n",
      "       [ 12,  63, 134]]), array([[ 99,  11,   4],\n",
      "       [  8, 290,  26],\n",
      "       [ 11,  70, 128]]), array([[ 95,  13,   5],\n",
      "       [  5, 296,  23],\n",
      "       [  9,  56, 145]]), array([[ 92,  17,   4],\n",
      "       [  5, 285,  35],\n",
      "       [ 14,  61, 134]]), array([[ 93,  15,   5],\n",
      "       [  1, 298,  26],\n",
      "       [  9,  59, 141]]), array([[ 92,  21,   1],\n",
      "       [  2, 303,  20],\n",
      "       [  6,  58, 145]]), array([[ 98,  13,   3],\n",
      "       [  8, 291,  26],\n",
      "       [ 10,  58, 141]]), array([[ 86,  20,   8],\n",
      "       [  5, 296,  24],\n",
      "       [  9,  63, 137]]), array([[ 94,  17,   3],\n",
      "       [  7, 296,  22],\n",
      "       [  6,  69, 134]]), array([[100,  10,   4],\n",
      "       [  6, 302,  17],\n",
      "       [ 12,  61, 136]]), array([[ 98,  11,   5],\n",
      "       [  4, 296,  24],\n",
      "       [ 14,  73, 122]]), array([[100,  11,   3],\n",
      "       [  9, 287,  28],\n",
      "       [ 11,  72, 126]]), array([[ 95,  15,   3],\n",
      "       [  6, 298,  20],\n",
      "       [  9,  64, 137]]), array([[ 93,  18,   2],\n",
      "       [  3, 288,  34],\n",
      "       [ 12,  65, 132]]), array([[ 91,  16,   6],\n",
      "       [  2, 301,  22],\n",
      "       [  9,  62, 138]]), array([[ 91,  21,   2],\n",
      "       [  2, 304,  19],\n",
      "       [  7,  61, 141]]), array([[ 99,  12,   3],\n",
      "       [  7, 292,  26],\n",
      "       [ 10,  60, 139]]), array([[ 87,  20,   7],\n",
      "       [  5, 298,  22],\n",
      "       [  9,  62, 138]]), array([[ 95,  16,   3],\n",
      "       [  6, 297,  22],\n",
      "       [  8,  70, 131]]), array([[ 99,  11,   4],\n",
      "       [  4, 301,  20],\n",
      "       [ 11,  63, 135]]), array([[ 95,  13,   6],\n",
      "       [  3, 293,  28],\n",
      "       [ 11,  66, 132]]), array([[ 97,  13,   4],\n",
      "       [  9, 288,  27],\n",
      "       [ 11,  74, 124]]), array([[ 95,  14,   4],\n",
      "       [  6, 300,  18],\n",
      "       [  9,  65, 136]]), array([[ 93,  18,   2],\n",
      "       [  5, 287,  33],\n",
      "       [ 13,  67, 129]]), array([[ 91,  18,   4],\n",
      "       [  2, 300,  23],\n",
      "       [  6,  64, 139]]), array([[ 90,  22,   2],\n",
      "       [  2, 303,  20],\n",
      "       [  7,  62, 140]]), array([[100,  11,   3],\n",
      "       [  6, 294,  25],\n",
      "       [  9,  59, 141]]), array([[ 88,  19,   7],\n",
      "       [  5, 298,  22],\n",
      "       [  9,  62, 138]]), array([[ 97,  15,   2],\n",
      "       [  6, 296,  23],\n",
      "       [  7,  74, 128]]), array([[ 98,  13,   3],\n",
      "       [  5, 301,  19],\n",
      "       [ 11,  63, 135]]), array([[ 98,  11,   5],\n",
      "       [  4, 294,  26],\n",
      "       [ 12,  70, 127]]), array([[ 97,  13,   4],\n",
      "       [  8, 289,  27],\n",
      "       [ 13,  72, 124]]), array([[ 95,  14,   4],\n",
      "       [  4, 301,  19],\n",
      "       [ 10,  67, 133]]), array([[ 92,  18,   3],\n",
      "       [  4, 289,  32],\n",
      "       [ 14,  70, 125]]), array([[ 92,  15,   6],\n",
      "       [  3, 299,  23],\n",
      "       [  6,  67, 136]]), array([[ 90,  22,   2],\n",
      "       [  2, 305,  18],\n",
      "       [  6,  63, 140]]), array([[ 99,  11,   4],\n",
      "       [  7, 294,  24],\n",
      "       [  7,  62, 140]]), array([[ 90,  18,   6],\n",
      "       [  4, 303,  18],\n",
      "       [  8,  64, 137]]), array([[ 97,  15,   2],\n",
      "       [  4, 302,  19],\n",
      "       [  7,  76, 126]]), array([[100,  11,   3],\n",
      "       [  5, 298,  22],\n",
      "       [ 11,  66, 132]]), array([[ 96,  13,   5],\n",
      "       [  5, 293,  26],\n",
      "       [ 12,  70, 127]]), array([[ 96,  14,   4],\n",
      "       [  6, 292,  26],\n",
      "       [ 12,  76, 121]]), array([[ 94,  15,   4],\n",
      "       [  6, 301,  17],\n",
      "       [ 10,  68, 132]]), array([[ 92,  18,   3],\n",
      "       [  4, 291,  30],\n",
      "       [ 14,  66, 129]]), array([[ 92,  15,   6],\n",
      "       [  2, 300,  23],\n",
      "       [  8,  71, 130]]), array([[ 90,  22,   2],\n",
      "       [  2, 302,  21],\n",
      "       [  7,  67, 135]]), array([[ 98,  12,   4],\n",
      "       [  8, 294,  23],\n",
      "       [  7,  69, 133]]), array([[ 89,  19,   6],\n",
      "       [  4, 302,  19],\n",
      "       [  8,  62, 139]]), array([[ 93,  16,   5],\n",
      "       [  4, 303,  18],\n",
      "       [  9,  70, 130]]), array([[ 99,  11,   4],\n",
      "       [  6, 299,  20],\n",
      "       [ 10,  66, 133]]), array([[ 98,  12,   4],\n",
      "       [  5, 296,  23],\n",
      "       [ 12,  76, 121]]), array([[ 97,  13,   4],\n",
      "       [  6, 292,  26],\n",
      "       [ 13,  80, 116]]), array([[ 91,  17,   5],\n",
      "       [  5, 301,  18],\n",
      "       [  9,  71, 130]]), array([[ 93,  17,   3],\n",
      "       [  3, 293,  29],\n",
      "       [ 15,  70, 124]]), array([[ 92,  15,   6],\n",
      "       [  2, 300,  23],\n",
      "       [  6,  72, 131]]), array([[ 90,  22,   2],\n",
      "       [  2, 306,  17],\n",
      "       [  7,  69, 133]]), array([[ 98,  12,   4],\n",
      "       [  7, 295,  23],\n",
      "       [  9,  67, 133]]), array([[ 90,  19,   5],\n",
      "       [  3, 301,  21],\n",
      "       [  8,  64, 137]]), array([[ 94,  17,   3],\n",
      "       [  4, 301,  20],\n",
      "       [  9,  71, 129]]), array([[100,  11,   3],\n",
      "       [  5, 300,  20],\n",
      "       [  9,  70, 130]]), array([[ 96,  13,   5],\n",
      "       [  4, 297,  23],\n",
      "       [ 11,  77, 121]]), array([[ 99,  12,   3],\n",
      "       [  5, 294,  25],\n",
      "       [ 14,  80, 115]]), array([[ 93,  16,   4],\n",
      "       [  5, 299,  20],\n",
      "       [  9,  73, 128]]), array([[ 96,  15,   2],\n",
      "       [  3, 293,  29],\n",
      "       [ 15,  70, 124]]), array([[ 92,  15,   6],\n",
      "       [  2, 303,  20],\n",
      "       [  7,  77, 125]]), array([[ 91,  22,   1],\n",
      "       [  3, 305,  17],\n",
      "       [  7,  69, 133]]), array([[ 98,  12,   4],\n",
      "       [  8, 294,  23],\n",
      "       [  9,  68, 132]]), array([[ 90,  19,   5],\n",
      "       [  4, 304,  17],\n",
      "       [  8,  64, 137]]), array([[ 94,  17,   3],\n",
      "       [  5, 299,  21],\n",
      "       [  9,  74, 126]]), array([[100,  11,   3],\n",
      "       [  6, 297,  22],\n",
      "       [  9,  72, 128]]), array([[ 95,  14,   5],\n",
      "       [  4, 298,  22],\n",
      "       [ 11,  82, 116]]), array([[ 99,  12,   3],\n",
      "       [  5, 292,  27],\n",
      "       [ 14,  81, 114]]), array([[ 93,  16,   4],\n",
      "       [  5, 300,  19],\n",
      "       [ 10,  72, 128]]), array([[ 94,  17,   2],\n",
      "       [  2, 295,  28],\n",
      "       [ 12,  76, 121]]), array([[ 91,  16,   6],\n",
      "       [  2, 299,  24],\n",
      "       [  8,  79, 122]]), array([[ 91,  21,   2],\n",
      "       [  2, 306,  17],\n",
      "       [  8,  71, 130]]), array([[100,  12,   2],\n",
      "       [  8, 295,  22],\n",
      "       [  7,  69, 133]]), array([[ 88,  21,   5],\n",
      "       [  4, 303,  18],\n",
      "       [ 10,  65, 134]]), array([[ 94,  17,   3],\n",
      "       [  5, 301,  19],\n",
      "       [  9,  72, 128]]), array([[100,  11,   3],\n",
      "       [  6, 298,  21],\n",
      "       [  9,  72, 128]]), array([[ 93,  15,   6],\n",
      "       [  4, 298,  22],\n",
      "       [ 10,  84, 115]]), array([[ 95,  14,   5],\n",
      "       [  5, 291,  28],\n",
      "       [ 12,  82, 115]]), array([[ 92,  16,   5],\n",
      "       [  4, 301,  19],\n",
      "       [ 12,  77, 121]]), array([[ 95,  16,   2],\n",
      "       [  2, 293,  30],\n",
      "       [ 11,  79, 119]]), array([[ 92,  15,   6],\n",
      "       [  1, 302,  22],\n",
      "       [  8,  76, 125]]), array([[ 90,  22,   2],\n",
      "       [  3, 307,  15],\n",
      "       [  7,  71, 131]]), array([[ 99,  12,   3],\n",
      "       [  9, 296,  20],\n",
      "       [  6,  69, 134]]), array([[ 89,  20,   5],\n",
      "       [  2, 305,  18],\n",
      "       [  9,  68, 132]]), array([[ 96,  15,   3],\n",
      "       [  6, 303,  16],\n",
      "       [  9,  72, 128]]), array([[100,  11,   3],\n",
      "       [  6, 298,  21],\n",
      "       [ 10,  72, 127]]), array([[ 95,  14,   5],\n",
      "       [  4, 297,  23],\n",
      "       [ 11,  86, 112]]), array([[ 95,  14,   5],\n",
      "       [  4, 293,  27],\n",
      "       [ 12,  86, 111]]), array([[ 91,  17,   5],\n",
      "       [  4, 300,  20],\n",
      "       [ 12,  71, 127]]), array([[ 93,  18,   2],\n",
      "       [  3, 295,  27],\n",
      "       [ 11,  76, 122]]), array([[ 93,  16,   4],\n",
      "       [  2, 297,  26],\n",
      "       [  9,  76, 124]]), array([[ 91,  21,   2],\n",
      "       [  2, 308,  15],\n",
      "       [  7,  69, 133]]), array([[100,  11,   3],\n",
      "       [  8, 295,  22],\n",
      "       [  7,  72, 130]]), array([[ 87,  21,   6],\n",
      "       [  2, 307,  16],\n",
      "       [  9,  66, 134]]), array([[ 93,  17,   4],\n",
      "       [  6, 301,  18],\n",
      "       [ 10,  78, 121]]), array([[101,  11,   2],\n",
      "       [  6, 300,  19],\n",
      "       [  8,  73, 128]]), array([[ 95,  14,   5],\n",
      "       [  4, 296,  24],\n",
      "       [ 11,  85, 113]]), array([[ 96,  13,   5],\n",
      "       [  5, 295,  24],\n",
      "       [ 13,  85, 111]]), array([[ 92,  17,   4],\n",
      "       [  4, 303,  17],\n",
      "       [ 12,  75, 123]]), array([[ 92,  19,   2],\n",
      "       [  3, 293,  29],\n",
      "       [ 13,  78, 118]]), array([[ 91,  16,   6],\n",
      "       [  1, 299,  25],\n",
      "       [  8,  79, 122]]), array([[ 90,  22,   2],\n",
      "       [  2, 308,  15],\n",
      "       [  7,  70, 132]]), array([[ 99,  11,   4],\n",
      "       [  7, 298,  20],\n",
      "       [  8,  70, 131]]), array([[ 86,  22,   6],\n",
      "       [  2, 308,  15],\n",
      "       [  9,  67, 133]]), array([[ 92,  19,   3],\n",
      "       [  6, 301,  18],\n",
      "       [  7,  81, 121]]), array([[101,  11,   2],\n",
      "       [  6, 300,  19],\n",
      "       [  8,  73, 128]]), array([[ 93,  16,   5],\n",
      "       [  4, 295,  25],\n",
      "       [ 11,  84, 114]]), array([[ 96,  15,   3],\n",
      "       [  4, 294,  26],\n",
      "       [ 11,  85, 113]]), array([[ 92,  16,   5],\n",
      "       [  5, 303,  16],\n",
      "       [ 11,  75, 124]]), array([[ 94,  17,   2],\n",
      "       [  4, 291,  30],\n",
      "       [ 12,  78, 119]]), array([[ 91,  17,   5],\n",
      "       [  1, 300,  24],\n",
      "       [  9,  82, 118]])]}}\n",
      "{'SVM': {'None': [0.8644464015960693, 0.8489458560943604, 0.8303875923156738, 0.8441228866577148, 0.8485069274902344, 0.839282751083374, 0.8272037506103516, 0.6879076957702637, 0.828066349029541, 0.8336968421936035], 'Gaussian_Blur': [0.8561840057373047, 0.8428502082824707, 0.8319203853607178, 0.8382830619812012, 0.8503255844116211, 0.8376994132995605, 0.843454122543335, 0.8452556133270264, 0.8406069278717041, 0.8344025611877441], 'Median_Blur': [1.0398097038269043, 1.018547773361206, 1.0280635356903076, 1.0348823070526123, 1.0584323406219482, 1.088325023651123, 1.1364688873291016, 1.1041386127471924, 1.0705926418304443, 1.1047117710113525]}, 'RF': {'None': [8.02333950996399, 8.234793663024902, 7.9224653244018555, 7.868056774139404, 7.946845293045044, 7.8946311473846436, 8.139342308044434, 7.938580751419067, 7.8842244148254395, 7.963507890701294], 'Gaussian_Blur': [7.865200757980347, 8.013236045837402, 7.735918760299683, 7.921578645706177, 7.937133312225342, 7.952821254730225, 8.062299489974976, 7.963570833206177, 7.859736680984497, 7.938720703125], 'Median_Blur': [8.109986782073975, 7.972768068313599, 8.068957567214966, 7.8763158321380615, 7.9858715534210205, 7.951327800750732, 8.008968830108643, 8.324324369430542, 8.24072527885437, 8.468615293502808]}, 'KNN': {'None': [0.0006351470947265625, 0.0006775856018066406, 0.0006842613220214844, 0.0007047653198242188, 0.0007569789886474609, 0.000762939453125, 0.0007746219635009766, 0.0007765293121337891, 0.0007507801055908203, 0.0007653236389160156, 0.0007317066192626953, 0.0007832050323486328, 0.0007824897766113281, 0.0007753372192382812, 0.0007531642913818359, 0.0007417201995849609, 0.0006737709045410156, 0.0006773471832275391, 0.0006530284881591797, 0.0007100105285644531, 0.0006303787231445312, 0.0006494522094726562, 0.0007059574127197266, 0.0007085800170898438, 0.0006859302520751953, 0.0006780624389648438, 0.0006587505340576172, 0.0006692409515380859, 0.0006413459777832031, 0.0006837844848632812, 0.0006680488586425781, 0.0006651878356933594, 0.0006606578826904297, 0.0006513595581054688, 0.0007166862487792969, 0.0006475448608398438, 0.0006775856018066406, 0.0006589889526367188, 0.0006842613220214844, 0.0006937980651855469, 0.0006415843963623047, 0.0006561279296875, 0.0006127357482910156, 0.0006368160247802734, 0.0006113052368164062, 0.0008175373077392578, 0.0007610321044921875, 0.0007011890411376953, 0.0006375312805175781, 0.0006656646728515625, 0.000820159912109375, 0.0006685256958007812, 0.0006580352783203125, 0.0006387233734130859, 0.0006902217864990234, 0.0007033348083496094, 0.0006489753723144531, 0.0006344318389892578, 0.0006186962127685547, 0.0006585121154785156, 0.0006301403045654297, 0.0006766319274902344, 0.0006616115570068359, 0.0006592273712158203, 0.0006725788116455078, 0.0006382465362548828, 0.0007259845733642578, 0.0007050037384033203, 0.0006222724914550781, 0.0006351470947265625, 0.0006344318389892578, 0.0006489753723144531, 0.0006284713745117188, 0.0006256103515625, 0.0006318092346191406, 0.00067138671875, 0.0007293224334716797, 0.0007131099700927734, 0.0007345676422119141, 0.0007302761077880859, 0.0007143020629882812, 0.0007259845733642578, 0.0007336139678955078, 0.0006456375122070312, 0.000614166259765625, 0.0006225109100341797, 0.0006520748138427734, 0.0006959438323974609, 0.0006825923919677734, 0.0007593631744384766, 0.0006673336029052734, 0.0006513595581054688, 0.0006272792816162109, 0.0006740093231201172, 0.0006208419799804688, 0.0006377696990966797, 0.0006577968597412109, 0.0006372928619384766, 0.0006601810455322266, 0.0006556510925292969, 0.0006365776062011719, 0.0008797645568847656, 0.0006511211395263672, 0.0006730556488037109, 0.0006515979766845703, 0.0008492469787597656, 0.0006277561187744141, 0.0006492137908935547, 0.0006196498870849609, 0.0006630420684814453, 0.0006456375122070312, 0.0006382465362548828, 0.0006225109100341797, 0.0007312297821044922, 0.0006556510925292969, 0.0006673336029052734, 0.0006978511810302734, 0.0006616115570068359, 0.000652313232421875, 0.0006949901580810547, 0.0006003379821777344, 0.0006210803985595703, 0.0006046295166015625, 0.0006539821624755859, 0.0006277561187744141, 0.0006380081176757812, 0.0006356239318847656, 0.0007634162902832031, 0.0007238388061523438, 0.0006465911865234375, 0.0006365776062011719, 0.0006906986236572266, 0.0006055831909179688, 0.0006117820739746094, 0.0006053447723388672, 0.0007073879241943359, 0.0006506443023681641, 0.0006096363067626953, 0.0006155967712402344, 0.0006535053253173828, 0.0006241798400878906, 0.0006475448608398438, 0.0006310939788818359, 0.0006070137023925781, 0.0006077289581298828, 0.0006201267242431641, 0.0007271766662597656, 0.0006093978881835938, 0.0006310939788818359, 0.0006682872772216797, 0.0006246566772460938, 0.0006248950958251953, 0.0006263256072998047, 0.0006392002105712891, 0.0007107257843017578, 0.0006504058837890625, 0.0006282329559326172, 0.0006079673767089844, 0.0006213188171386719, 0.0006213188171386719], 'Gaussian_Blur': [0.0005846023559570312, 0.0006473064422607422, 0.0006721019744873047, 0.0006544589996337891, 0.0007674694061279297, 0.0007410049438476562, 0.0006947517395019531, 0.0008375644683837891, 0.0006954669952392578, 0.0007185935974121094, 0.0007479190826416016, 0.0007445812225341797, 0.0007228851318359375, 0.0006701946258544922, 0.0006971359252929688, 0.0008003711700439453, 0.0007567405700683594, 0.000820159912109375, 0.0007996559143066406, 0.0008945465087890625, 0.0007476806640625, 0.0007965564727783203, 0.0006556510925292969, 0.0009229183197021484, 0.0006868839263916016, 0.0007746219635009766, 0.0007603168487548828, 0.0007174015045166016, 0.0007557868957519531, 0.0007293224334716797, 0.0007786750793457031, 0.0007319450378417969, 0.0006890296936035156, 0.0006968975067138672, 0.0007977485656738281, 0.0007009506225585938, 0.0007145404815673828, 0.0007643699645996094, 0.0008065700531005859, 0.0006494522094726562, 0.0008320808410644531, 0.0007302761077880859, 0.0008475780487060547, 0.0006859302520751953, 0.0007762908935546875, 0.0007171630859375, 0.0008633136749267578, 0.0007491111755371094, 0.00069427490234375, 0.0007138252258300781, 0.0007052421569824219, 0.0007395744323730469, 0.0007445812225341797, 0.0006840229034423828, 0.0007207393646240234, 0.0007781982421875, 0.0006635189056396484, 0.0007064342498779297, 0.0007581710815429688, 0.0006654262542724609, 0.0006902217864990234, 0.0007100105285644531, 0.0007987022399902344, 0.0007512569427490234, 0.0007216930389404297, 0.0008842945098876953, 0.0007443428039550781, 0.0006797313690185547, 0.0007154941558837891, 0.0006589889526367188, 0.0008232593536376953, 0.0007042884826660156, 0.0008137226104736328, 0.0006592273712158203, 0.0008878707885742188, 0.0006327629089355469, 0.0007433891296386719, 0.0007469654083251953, 0.0007073879241943359, 0.0006732940673828125, 0.0008287429809570312, 0.0006995201110839844, 0.0006945133209228516, 0.0006704330444335938, 0.0008151531219482422, 0.0007359981536865234, 0.0007121562957763672, 0.0006945133209228516, 0.000782012939453125, 0.0006794929504394531, 0.0006437301635742188, 0.000820159912109375, 0.0007627010345458984, 0.0006918907165527344, 0.0007998943328857422, 0.0007731914520263672, 0.0006923675537109375, 0.0007033348083496094, 0.0008151531219482422, 0.0007617473602294922, 0.0006926059722900391, 0.0006704330444335938, 0.0007796287536621094, 0.0006284713745117188, 0.0007121562957763672, 0.0007770061492919922, 0.0007302761077880859, 0.0009093284606933594, 0.0007266998291015625, 0.0006847381591796875, 0.0009162425994873047, 0.0006670951843261719, 0.0006797313690185547, 0.0007665157318115234, 0.0006878376007080078, 0.0006659030914306641, 0.0007593631744384766, 0.0007011890411376953, 0.0008230209350585938, 0.0006606578826904297, 0.0007579326629638672, 0.0007460117340087891, 0.0007636547088623047, 0.0006480216979980469, 0.0009224414825439453, 0.0006601810455322266, 0.0009436607360839844, 0.0006723403930664062, 0.0009012222290039062, 0.0006861686706542969, 0.0007297992706298828, 0.0007073879241943359, 0.0006761550903320312, 0.0007176399230957031, 0.0007221698760986328, 0.0007750988006591797, 0.0006802082061767578, 0.0007531642913818359, 0.0007777214050292969, 0.0006434917449951172, 0.0007200241088867188, 0.0006663799285888672, 0.0008122920989990234, 0.0007083415985107422, 0.0007791519165039062, 0.0006842613220214844, 0.0007929801940917969, 0.0006673336029052734, 0.0011017322540283203, 0.0007598400115966797, 0.0007319450378417969, 0.0007205009460449219, 0.0007066726684570312, 0.0007588863372802734, 0.0006859302520751953, 0.0007078647613525391, 0.0007698535919189453, 0.0007264614105224609, 0.0007584095001220703, 0.0006821155548095703], 'Median_Blur': [0.0006077289581298828, 0.0006835460662841797, 0.0007708072662353516, 0.0008394718170166016, 0.0006158351898193359, 0.0006134510040283203, 0.0009589195251464844, 0.0006666183471679688, 0.0008704662322998047, 0.0006322860717773438, 0.0006015300750732422, 0.0006334781646728516, 0.0007483959197998047, 0.0007760524749755859, 0.0006988048553466797, 0.0006771087646484375, 0.000667572021484375, 0.0006997585296630859, 0.0008459091186523438, 0.0009553432464599609, 0.0006492137908935547, 0.0007352828979492188, 0.0006954669952392578, 0.0007314682006835938, 0.000705718994140625, 0.0006945133209228516, 0.000728607177734375, 0.0006811618804931641, 0.0006346702575683594, 0.0006196498870849609, 0.0007224082946777344, 0.0008594989776611328, 0.0007240772247314453, 0.0007750988006591797, 0.0006933212280273438, 0.0008268356323242188, 0.0006692409515380859, 0.0007884502410888672, 0.0008747577667236328, 0.0007510185241699219, 0.0006444454193115234, 0.0006616115570068359, 0.0007417201995849609, 0.0008206367492675781, 0.0006537437438964844, 0.0006725788116455078, 0.0007789134979248047, 0.0007762908935546875, 0.0006642341613769531, 0.0006816387176513672, 0.0007190704345703125, 0.0007994174957275391, 0.0006861686706542969, 0.0006678104400634766, 0.0006818771362304688, 0.000865936279296875, 0.0007071495056152344, 0.0006880760192871094, 0.0008075237274169922, 0.0007510185241699219, 0.0006151199340820312, 0.0008013248443603516, 0.0007703304290771484, 0.0006866455078125, 0.0007164478302001953, 0.0007112026214599609, 0.0006592273712158203, 0.0007193088531494141, 0.0007326602935791016, 0.0007991790771484375, 0.0006592273712158203, 0.0007922649383544922, 0.0006082057952880859, 0.0007786750793457031, 0.0006945133209228516, 0.0007030963897705078, 0.0007865428924560547, 0.0006616115570068359, 0.0006706714630126953, 0.0009932518005371094, 0.0006382465362548828, 0.0008044242858886719, 0.0007987022399902344, 0.0006706714630126953, 0.0007746219635009766, 0.0007526874542236328, 0.0006797313690185547, 0.0008363723754882812, 0.0006964206695556641, 0.0006887912750244141, 0.000823974609375, 0.0006923675537109375, 0.0007276535034179688, 0.0008087158203125, 0.0006916522979736328, 0.0007140636444091797, 0.00077056884765625, 0.0006866455078125, 0.0007085800170898438, 0.0007953643798828125, 0.0006694793701171875, 0.0007474422454833984, 0.0006811618804931641, 0.0006892681121826172, 0.0007202625274658203, 0.0008213520050048828, 0.0007011890411376953, 0.0007827281951904297, 0.0008289813995361328, 0.0006525516510009766, 0.0007581710815429688, 0.0006780624389648438, 0.0007114410400390625, 0.0009205341339111328, 0.0007979869842529297, 0.000667572021484375, 0.0006406307220458984, 0.0006561279296875, 0.0006244182586669922, 0.0007491111755371094, 0.0006430149078369141, 0.0006520748138427734, 0.0007426738739013672, 0.0006811618804931641, 0.0007174015045166016, 0.0006542205810546875, 0.0007486343383789062, 0.0007364749908447266, 0.0006449222564697266, 0.0007097721099853516, 0.0006322860717773438, 0.000797271728515625, 0.0006425380706787109, 0.0006582736968994141, 0.0007636547088623047, 0.0007746219635009766, 0.0006761550903320312, 0.0006651878356933594, 0.0008161067962646484, 0.0006968975067138672, 0.0007665157318115234, 0.000644683837890625, 0.0008232593536376953, 0.0006768703460693359, 0.0006923675537109375, 0.0007956027984619141, 0.0006961822509765625, 0.0006542205810546875, 0.0008192062377929688, 0.0006241798400878906, 0.0008075237274169922, 0.0007343292236328125, 0.0007641315460205078, 0.0006809234619140625, 0.0007023811340332031, 0.0007221698760986328, 0.0006957054138183594, 0.0007951259613037109, 0.0006690025329589844, 0.0008077621459960938]}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models_denoising_accuracies = {\"SVM\": {}, \"RF\": {}, \"KNN\": {}}\n",
    "models_denoising_f1_scores = {\"SVM\": {}, \"RF\": {}, \"KNN\": {}}\n",
    "models_denoising_confusion_metrics = {\"SVM\": {}, \"RF\": {}, \"KNN\": {}}\n",
    "models_denoising_classification_times = {\"SVM\": {}, \"RF\": {}, \"KNN\": {}}\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "for denoise_method, dataset in denoising_datasets.items():\n",
    "    # Split dataset\n",
    "    X_train, y_train, X_test, y_test = datasets_feature_extractor(model, dataset)\n",
    "    # combine train & test set\n",
    "    X = np.concatenate([X_train, X_test], axis=0)\n",
    "    y = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "    # SVM\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        svm = SVC()\n",
    "        start = time.time()\n",
    "        svm.fit(X_tr, y_tr)\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        y_pred = svm.predict(X_val)\n",
    "\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "        if not denoise_method in models_denoising_accuracies[\"SVM\"].keys():\n",
    "            models_denoising_accuracies[\"SVM\"][denoise_method] = [accuracy]\n",
    "            models_denoising_f1_scores[\"SVM\"][denoise_method] = [f1]\n",
    "            models_denoising_confusion_metrics[\"SVM\"][denoise_method] = [cm]\n",
    "            models_denoising_classification_times[\"SVM\"][denoise_method] = [elapsed_time]\n",
    "        else:\n",
    "            models_denoising_accuracies[\"SVM\"][denoise_method].append(accuracy)\n",
    "            models_denoising_f1_scores[\"SVM\"][denoise_method].append(f1)\n",
    "            models_denoising_confusion_metrics[\"SVM\"][denoise_method].append(cm)\n",
    "            models_denoising_classification_times[\"SVM\"][denoise_method].append(elapsed_time)\n",
    "\n",
    "        print(f\"SVM {denoise_method} accuracy: {accuracy}\")\n",
    "        print(f\"SVM {denoise_method} f1 score: {f1}\")\n",
    "        print(f\"SVM {denoise_method} classification time: {elapsed_time}\")\n",
    "\n",
    "    #RF\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        rf = RandomForestClassifier()\n",
    "        start = time.time()\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        end = time.time()\n",
    "        elapsed_time = end - start\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "        if not denoise_method in models_denoising_accuracies[\"RF\"].keys():\n",
    "            models_denoising_accuracies[\"RF\"][denoise_method] = [accuracy]\n",
    "            models_denoising_f1_scores[\"RF\"][denoise_method] = [f1]\n",
    "            models_denoising_confusion_metrics[\"RF\"][denoise_method] = [cm]\n",
    "            models_denoising_classification_times[\"RF\"][denoise_method] = [elapsed_time]\n",
    "        else:\n",
    "            models_denoising_accuracies[\"RF\"][denoise_method].append(accuracy)\n",
    "            models_denoising_f1_scores[\"RF\"][denoise_method].append(f1)\n",
    "            models_denoising_confusion_metrics[\"RF\"][denoise_method].append(cm)\n",
    "            models_denoising_classification_times[\"RF\"][denoise_method].append(elapsed_time)\n",
    "\n",
    "        print(f\"Random Forest {denoise_method} accuracy: {accuracy}\")\n",
    "        print(f\"Random Forest {denoise_method} f1 score: {f1}\")\n",
    "        print(f\"Random Forest {denoise_method} classification time: {elapsed_time}\")\n",
    "\n",
    "    #Find best k for KNN\n",
    "    knn_models = []\n",
    "    knn_accuracies = []\n",
    "    knn_f1_scores = []\n",
    "    knn_confusion_metrics = []\n",
    "    knn_classification_times = []\n",
    "\n",
    "\n",
    "    for k in range(1, 32, 2): #k = 1 to 31\n",
    "        for train_idx, val_idx in kf.split(X, y):\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            y_tr, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            start = time.time()\n",
    "            knn.fit(X_tr, y_tr)\n",
    "            end = time.time()\n",
    "            elapsed_time = end - start\n",
    "            \n",
    "            y_pred = knn.predict(X_val)\n",
    "            accuracy = accuracy_score(y_val, y_pred)\n",
    "            f1 = f1_score(y_val, y_pred, average=\"macro\")\n",
    "            \n",
    "            cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "            if not denoise_method in models_denoising_accuracies[\"KNN\"].keys():\n",
    "                models_denoising_accuracies[\"KNN\"][denoise_method] = [accuracy]\n",
    "                models_denoising_f1_scores[\"KNN\"][denoise_method] = [f1]\n",
    "                models_denoising_confusion_metrics[\"KNN\"][denoise_method] = [cm]\n",
    "                models_denoising_classification_times[\"KNN\"][denoise_method] = [elapsed_time]\n",
    "            else:\n",
    "                models_denoising_accuracies[\"KNN\"][denoise_method].append(accuracy)\n",
    "                models_denoising_f1_scores[\"KNN\"][denoise_method].append(f1)\n",
    "                models_denoising_confusion_metrics[\"KNN\"][denoise_method].append(cm)\n",
    "                models_denoising_classification_times[\"KNN\"][denoise_method].append(elapsed_time)\n",
    "\n",
    "            knn_models.append(knn)\n",
    "            knn_accuracies.append(accuracy)\n",
    "            knn_f1_scores.append(f1)\n",
    "            knn_confusion_metrics.append(cm)\n",
    "            knn_classification_times.append(elapsed_time)\n",
    "            \n",
    "            print(f\"Best {k}NN {denoise_method} accuracy: {accuracy}\")\n",
    "            print(f\"Best {k}NN {denoise_method} f1 score: {f1}\")\n",
    "            print(f\"Best {k}NN {denoise_method} classification time: {elapsed_time}\")\n",
    "\n",
    "    knn_accuracies = np.array(knn_accuracies)\n",
    "    max_idx = np.argmax(knn_accuracies)\n",
    "    best_k = 2*max_idx+1\n",
    "\n",
    "    print(f\"Best {best_k}NN {denoise_method} accuracy: {knn_accuracies[max_idx]}\")\n",
    "    print(f\"Best {best_k}NN {denoise_method} f1 score: {knn_f1_scores[max_idx]}\")\n",
    "    print(f\"Best {best_k}NN {denoise_method} classification time: {knn_classification_times[max_idx]}\")\n",
    "\n",
    "    accuracy = float(knn_accuracies[max_idx])\n",
    "    f1 = knn_f1_scores[max_idx]\n",
    "    cm = knn_confusion_metrics[max_idx]\n",
    "    elapsed_time = knn_classification_times[max_idx]\n",
    "\n",
    "    print(models_denoising_accuracies)\n",
    "    print(models_denoising_f1_scores)\n",
    "    print(models_denoising_confusion_metrics)\n",
    "    print(models_denoising_classification_times)\n",
    "\n",
    "    # with open(f\"denoised_models/SVM_{denoise_method}_CV.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(svm, f)\n",
    "    # with open(f\"denoised_models/RF_{denoise_method}_CV.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(rf, f)\n",
    "    # with open(f\"denoised_models/{best_k}NN_{denoise_method}_CV.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(knn_models[max_idx], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc272ea",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818083cd",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a4ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(models_denoising_accuracies).T.reset_index().melt(id_vars='index', var_name='Denoising', value_name='Accuracy')\n",
    "df.columns = ['Model', 'Denoising', 'Accuracy']\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=df, x='Model', y='Accuracy', hue='Denoising')\n",
    "\n",
    "plt.title('Accuracy by Model and Denoising Methods')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Model')\n",
    "plt.legend(title='Denoising Methods')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(models_denoising_f1_scores).T.reset_index().melt(id_vars='index', var_name='Denoising', value_name='Accuracy')\n",
    "df.columns = ['Model', 'Denoising', 'F1 Score']\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=df, x='Model', y='F1 Score', hue='Denoising')\n",
    "\n",
    "plt.title('F1 Score by Model and Denoising Methods')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xlabel('Model')\n",
    "plt.legend(title='Denoising Methods')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f0a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(models_denoising_classification_times).T.reset_index().melt(id_vars='index', var_name='Denoising', value_name='Accuracy')\n",
    "df.columns = ['Model', 'Denoising', 'Classification Time']\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=df, x='Model', y='Classification Time', hue='Denoising')\n",
    "\n",
    "plt.title('Classification Time by Model and Denoising Methods')\n",
    "plt.ylabel('Classification Time')\n",
    "plt.xlabel('Model')\n",
    "plt.legend(title='Denoising Methods')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c70ab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24373/257706186.py:14: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(data, labels=d_methods)\n",
      "/tmp/ipykernel_24373/257706186.py:14: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(data, labels=d_methods)\n",
      "/tmp/ipykernel_24373/257706186.py:14: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(data, labels=d_methods)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdEAAAHqCAYAAADrpwd3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAaFNJREFUeJzt3XlYVHX///HXAAoD4pKopJHjzmSmgkJJinbrraW4tLggrqktWiZ3i5pLaklWGmWm1c+lO9zSzLyzLLPUTEvDtPwKginpXeKSGSq4BOf3RxdzN8HoDM44gM/HdXHRnPmcz3kPHec985oz55gMwzAEAAAAAAAAAACK8PF2AQAAAAAAAAAAlFaE6AAAAAAAAAAAOECIDgAAAAAAAACAA4ToAAAAAAAAAAA4QIgOAAAAAAAAAIADhOgAAAAAAAAAADhAiA4AAAAAAAAAgAOE6AAAAAAAAAAAOECIDgAAAAAAAACAA4ToAAAAAAAAAAA4QIgOlAM//PCD7r33XtWtW1cBAQGqU6eOOnXqpNmzZ0uSdu7cKZPJpAkTJjicIzMzUyaTSYmJiZKkZ555RiaTST4+Pjp8+HCR8Tk5OTKbzTKZTBo1apTTtebn56t27doymUz6+OOPXXykAACUTosWLZLJZLL9+Pn5qU6dOho8eLB+/vnnIuPbt29vN/6vP+np6U5t89SpUwoICJDJZFJaWpq7HxIAAF5R2FO//fZbu+W///67oqKiFBAQoHXr1kn63/vWWrVqKTc3t8hcFotF3bp1s1tW2G9nzpzp9LYv5aOPPpLJZFLt2rVVUFDg9HoAyhZCdKCM27p1q1q1aqXdu3dr+PDheu211zRs2DD5+PjolVdekSRFREQoPDxcS5cudTjPkiVLJEkJCQl2y/39/Ytdb9WqVSWq9/PPP9eRI0dksVi0ePHiEs0BAEBpNXXqVL3zzjuaN2+e7rzzTqWkpCg2Nlbnzp0rMvaGG27QO++8U+Sndu3aTm1rxYoVMplMCg0NpacCAMq1nJwc/fOf/9T333+v999/X126dLG7/9ixY5o7d65Lc7744ovFBu+uWrx4sSwWi44cOaLPP//8iucDUDr5ebsAAFfmueeeU5UqVbRjxw5VrVrV7r5jx47Z/rt///6aOHGivv76a916661F5lm6dKnCw8MVERFht/yuu+7S0qVL9eSTT9otX7Jkibp27ar33nvPpXpTUlIUERGhQYMGafz48Tp79qyCgoJcmuNq+OOPP1RQUKCKFSt6uxQAQBly5513qlWrVpKkYcOGKSQkRDNmzNCaNWvUu3dvu7FVqlQp8uG1K1JSUnTXXXepbt26WrJkiZ599tkrqt1Tzp07p4oVK8rHh+N3AACuO336tDp37qxdu3Zp1apVuvPOO4uMadGihV588UU9/PDDMpvNl52zRYsW2rVrl+bNm2f7NnZJnD17Vh988IGSkpK0cOFCLV68WB07dizxfJ5UWt97A2UFr2SBMu7HH39U06ZNiwToklSzZk3bf/fv31/S/444/6vU1FTt27fPNuav4uPjtWvXLruvlmdnZ+vzzz9XfHy8S7Xm5eXp/fffV9++fdW7d2/l5eXpgw8+KHbsxx9/rNjYWAUHB6ty5cpq3bp1kdq/+eYb3XXXXapWrZqCgoJ0yy232I6+l/78qnz79u2LzD148GBZLBbb7aysLJlMJr300ktKTk5WgwYN5O/vr7179+rChQuaNGmSIiMjVaVKFQUFBalt27b64osvisxbUFCgV155Rc2aNVNAQIBq1KihLl262L4KGBsbq+bNmxf7eJs0aaLOnTtf7k8IAChj2rZtK+nPfu1Ohw4d0pdffqm+ffuqb9++OnjwoLZu3Vrs2JSUFEVFRSkwMFDVqlVTu3bt9Omnn9qNuVzftVgsGjx4cJG5/95rN27cKJPJpGXLlmnChAmqU6eOAgMDlZOTo5MnT+rxxx9Xs2bNVKlSJVWuXFl33nmndu/eXWTec+fO6ZlnnlHjxo0VEBCg66+/Xnfffbd+/PFHGYYhi8WiHj16FLtelSpV9MADDzj5lwQAlGZnzpxRly5dtHPnTr333nvq2rVrseMmTZqko0ePOn00ekxMjO644w698MILysvLK3F977//vvLy8nTfffepb9++WrVqVbHfPrtUXyt0ufeThe9bFy1aVGR+k8mkZ555xna78DQ3e/fuVXx8vKpVq6bbb79dkvT9999r8ODBql+/vgICAhQaGqqhQ4fq119/LTLvzz//rPvvv1+1a9eWv7+/6tWrp4ceekgXLlzQgQMHZDKZ9PLLLxdZb+vWrTKZTJf8NjxQ1hCiA2Vc3bp1lZqaqj179lxyXL169dSmTRu9++67ys/Pt7uv8E1ycaF4u3btdMMNN9i9kV6+fLkqVark8AWMI2vWrNGZM2fUt29fhYaGqn379sV+/XzRokXq2rWrTp48qXHjxun5559XixYtbOe9k6T169erXbt22rt3r0aPHq2ZM2eqQ4cO+vDDD12q6a8WLlyo2bNna8SIEZo5c6auu+465eTk6P/9v/+n9u3ba8aMGXrmmWd0/Phx25EQf3X//ffrscceU1hYmGbMmKGxY8cqICBAX3/9tSRpwIAB+v7774v8v9qxY4cyMjKu6GhEAEDplJWVJUmqVq1akfvy8/N14sQJu58zZ844Ne/SpUsVFBSkbt26KSoqSg0aNCi2p06ZMkUDBgxQhQoVNHXqVE2ZMkVhYWF2Xzd3pu+6atq0aVq7dq0ef/xxTZ8+XRUrVtSBAwe0evVqdevWTbNmzdITTzyhH374QbGxsfrll1/s/i7dunXTlClTFBkZqZkzZ2r06NH6/ffftWfPHplMJiUkJOjjjz/WyZMn7bb7n//8Rzk5OfRUACgHzp49qzvvvFM7duzQihUripzb/K/atm3rcij+zDPPuBS8F2fx4sXq0KGDQkND1bdvX50+fVr/+c9/7MZcrq8Vutz7yZK47777lJubq+nTp2v48OGS/nwvfeDAAQ0ZMkSzZ89W3759tWzZMt11110yDMO27i+//KKoqCgtW7ZMffr00auvvqoBAwZo06ZNys3NVf369RUTE1Ps64/FixcrODi42A+8gTLLAFCmffrpp4avr6/h6+tr3HbbbcaTTz5pfPLJJ8aFCxeKjJ0zZ44hyfjkk09sy/Lz8406deoYt912m93YyZMnG5KM48ePG48//rjRsGFD232tW7c2hgwZYhiGYUgyRo4c6VSt3bp1M2JiYmy333zzTcPPz884duyYbdmpU6eM4OBgIzo62sjLy7Nbv6CgwDAMw/jjjz+MevXqGXXr1jV+++23YscYhmHExsYasbGxReoYNGiQUbduXdvtgwcPGpKMypUr29VSuK3z58/bLfvtt9+MWrVqGUOHDrUt+/zzzw1JxqOPPlpke4U1nTp1yggICDCeeuopu/sfffRRIygoyDhz5kyRdQEAZcPChQsNScZnn31mHD9+3Dh8+LCxcuVKo0aNGoa/v79x+PBhu/GxsbGGpCI/gwYNcmp7zZo1M/r372+7PX78eCMkJMS4ePGibVlmZqbh4+Nj9OrVy8jPz7db/6+96XJ91zAMo27dusXW9vde+8UXXxiSjPr16xu5ubl2Y8+dO1ekjoMHDxr+/v7G1KlTbcsWLFhgSDJmzZpVZHuFNe3bt8+QZMydO9fu/u7duxsWi8WudgBA2VLYU+vWrWtUqFDBWL16tcOxf33fumnTpiL9o27dukbXrl3t1vnre9gOHToYoaGhtp5VuO0dO3Zcts6jR48afn5+xltvvWVb1qZNG6NHjx5245zpa868nyx837pw4cIiYyQZkydPtt0u/Lv069evyNi/92fDMIylS5cakozNmzfblg0cONDw8fEp9m9RWNMbb7xhSDLS0tJs9124cMEICQlx+jUNUFZwJDpQxnXq1Enbtm1T9+7dtXv3br3wwgvq3Lmz6tSpozVr1tiN7dOnjypUqGB3VPmmTZv0888/F3sql0Lx8fHav3+/duzYYfvt6qlcfv31V33yySfq16+fbdk999wjk8mkd99917Zs/fr1On36tO1T978ymUySpO+++04HDx7UY489VuQ0NoVjSuKee+5RjRo17Jb5+vrazoteUFCgkydP6o8//lCrVq20c+dO27j33ntPJpNJkydPLjJvYU1VqlRRjx49tHTpUtsn/Pn5+Vq+fLl69uzJ+ekAoBzo2LGjatSoobCwMN17770KCgrSmjVrdMMNNxQZa7FYtH79erufv1+DpDjff/+9fvjhB7ue2q9fP504cUKffPKJbdnq1atVUFCgSZMmFTkfeWFvcqbvlsSgQYOKnJPW39/fVkd+fr5+/fVXVapUSU2aNCnSU0NCQvTII48UmbewpsaNGys6Otru6LeTJ0/q448/Vv/+/a+odgBA6XD06FEFBAQoLCzMqfHt2rVThw4dXD4aPTs7W/PmzXO5vmXLlsnHx0f33HOPbVm/fv308ccf67fffrMtc6avOfN+siQefPDBIsv+2p/PnTunEydO2K6bVtiPCwoKtHr1asXFxdmu9VJcTb1791ZAQIBdP/7kk0904sQJvhWGcocQHSgHWrdurVWrVum3337T9u3bNW7cOJ0+fVr33nuv9u7daxtXvXp1de7cWe+//77tPG1LliyRn59fkYud/VXLli0VHh6uJUuWaPHixQoNDdUdd9zhUo3Lly/XxYsX1bJlS+3fv1/79+/XyZMni7wBLjwn3M033+xwLmfGlES9evWKXf7222/rlltuUUBAgKpXr64aNWpo7dq1+v333+1qql27tq677rpLbmPgwIG289hK0meffaajR49qwIAB7nsgAACvmTNnjtavX6+VK1fqrrvu0okTJ+Tv71/s2KCgIHXs2NHu56abbrrsNlJSUhQUFKT69evbempAQIAsFkuRnurj43PJOa9mTy0oKNDLL7+sRo0ayd/fXyEhIapRo4a+//77Ij21SZMm8vPzu+Q2Bg4cqK+++ko//fSTJGnFihW6ePEiPRUAyok33nhDFStWVJcuXbRv3z6n1nE1FC9J8F6o8Jojv/76q60ft2zZUhcuXNCKFSts45zpa86+n3RVcf345MmTGj16tGrVqiWz2awaNWrYxhX24+PHjysnJ+eyrw+qVq2quLg4uwP1Fi9erDp16ricGQClHSE6UI5UrFhRrVu31vTp0zV37lxdvHjRrnlLUkJCgnJycvThhx/qwoULeu+99/TPf/6zyBHYfxcfH6/ly5dryZIl6tOnT5Ej2i6n8E19TEyMGjVqZPvZsmWLtm3bpgMHDrj2YJ3g6BP7v58TvlBxV3FPSUnR4MGD1aBBA82fP1/r1q3T+vXrdccdd6igoMDlmjp37qxatWopJSXFNn9oaGipvYI7AMA1UVFR6tixo+655x6tWbNGN998s+Lj450+1/nlGIahpUuX6uzZs7rpppvsempWVpY++OADt23rr9zRU6dPn67ExES1a9dOKSkp+uSTT7R+/Xo1bdq0RD21b9++qlChgu01RkpKilq1aqUmTZq4PBcAoPS56aab9NFHHykvL0+dOnXS4cOHL7tOu3bt1L59e5dC8cmTJys7O1tvvPGG07VlZmZqx44d2rJli10vLrx4Z3HnCb9SrvZiqfh+3Lt3b7311lt68MEHtWrVKn366ae266CUpB8PHDhQBw4c0NatW3X69GmtWbNG/fr1czkzAEq7Sx/eAaDMKvzK1ZEjR+yWd+/eXcHBwVqyZIkqVKig33777ZKncikUHx+vSZMm6ciRI3rnnXdcquXgwYPaunWrRo0apdjYWLv7CgoKNGDAAC1ZskQTJkxQgwYNJEl79uxRw4YNi53vr2MuFT5Xq1at2HC+8Ig1Z6xcuVL169fXqlWr7F60/P1rdg0aNNAnn3yikydPXvLoAV9fX8XHx2vRokWaMWOGVq9ereHDh8vX19fpmgAAZYOvr6+SkpLUoUMHvfbaaxo7duwVz7lp0yb997//1dSpU2W1Wu3u++233zRixAitXr1aCQkJatCggQoKCrR37161aNGi2Pmc6bvSnz311KlTRZb/9NNPql+/vlO1r1y5Uh06dND8+fPtlp86dUohISF2NX3zzTe6ePGiKlSo4HC+6667Tl27dtXixYvVv39/ffXVV0pOTnaqFgBA2RAVFaXVq1era9eu6tSpk7788svLHgD2zDPPqH379k6H4rGxsWrfvr1mzJihSZMmObXO4sWLVaFCBb3zzjtF3stt2bJFr776qg4dOqQbb7zRqb7mzPvJwouU/70fu/L+9rffftOGDRs0ZcoUu8eamZlpN65GjRqqXLmy3YVPHenSpYtq1KihxYsXKzo6Wrm5uXwrDOUSHwsBZdwXX3xhdwXtQh999JEkFTkay2w2q1evXvroo480d+5cBQUFOXXF7AYNGig5OVlJSUmKiopyqcbCT+GffPJJ3XvvvXY/vXv3VmxsrG3MP//5TwUHByspKcl2yplChY8zIiJC9erVU3JycpEXEH/9WzRo0EDp6ek6fvy4bdnu3bv11VdfOV174Quiv877zTffaNu2bXbj7rnnHhmGoSlTphSZ4+//fwYMGKDffvtNDzzwgM6cOcO54gCgHGvfvr2ioqKUnJxcpK+VROGpXJ544okiPXX48OFq1KiRraf27NlTPj4+mjp1apEjywp7kzN9V/qzp3799de6cOGCbdmHH37o1FGBhXx9fYv0xBUrVujnn3+2W3bPPffoxIkTeu2114rMUVxP3bt3r5544gn5+vqqb9++TtcDACgb/vGPf2jp0qXav3+/unTpopycnEuO/2so7mzvLTwNzJtvvunU+MWLF6tt27bq06dPkX78xBNPSJKWLl0qybm+5sz7ycqVKyskJESbN2+2u//11193qmap+Pe3kop8CO3j46OePXvqP//5j7799luHNUmSn5+f+vXrp3fffVeLFi1Ss2bNdMsttzhdE1BWcCQ6UMY98sgjys3NVa9evRQeHq4LFy5o69atWr58uSwWi4YMGVJknYSEBP373//WJ598ov79+zt9QcvRo0eXqMbFixerRYsWDi8I0717dz3yyCPauXOnIiIi9PLLL2vYsGFq3bq14uPjVa1aNe3evVu5ubl6++235ePjo7lz5youLk4tWrTQkCFDdP311ys9PV3/93//Z7uo2tChQzVr1ix17txZ999/v44dO6Z58+apadOml33hVahbt25atWqVevXqpa5du+rgwYOaN2+ebrrpJruvy3fo0EEDBgzQq6++qszMTHXp0kUFBQX68ssv1aFDB40aNco2tmXLlrr55pu1YsUKWa1WRURElOjvCgAoG5544gndd999WrRoUbEX+HLW+fPn9d5776lTp05FLgJaqHv37nrllVd07NgxNWzYUE8//bSmTZumtm3b6u6775a/v7927Nih2rVrKykpSZUrV75s35WkYcOGaeXKlerSpYt69+6tH3/8USkpKbYj2Z3RrVs3TZ06VUOGDFGbNm30ww8/aPHixUWOZB84cKD+/e9/KzExUdu3b1fbtm119uxZffbZZ3r44YftPvzv2rWrqlevrhUrVujOO+9UzZo1S/CXBQCUdr169dJbb72loUOHqnv37lq3bp3DXij9+c3hDh06OD1/bGysYmNjtWnTpsuO/eabb7R//36793h/VadOHUVERGjx4sV66qmnnOprzr6fHDZsmJ5//nkNGzZMrVq10ubNm5WRkeH046xcubLatWunF154QRcvXlSdOnX06aef6uDBg0XGTp8+XZ9++qliY2M1YsQIWa1WHTlyRCtWrNCWLVtUtWpV29iBAwfq1Vdf1RdffKEZM2Y4XQ9QphgAyrSPP/7YGDp0qBEeHm5UqlTJqFixotGwYUPjkUceMY4ePVrsOn/88Ydx/fXXG5KMjz76qNgxkydPNiQZx48fv+T2JRkjR450eH9qaqohyZg4caLDMVlZWYYkY8yYMbZla9asMdq0aWOYzWajcuXKRlRUlLF06VK79bZs2WJ06tTJCA4ONoKCgoxbbrnFmD17tt2YlJQUo379+kbFihWNFi1aGJ988okxaNAgo27durYxBw8eNCQZL774YpHaCgoKjOnTpxt169Y1/P39jZYtWxoffvhhkTkM48+/64svvmiEh4cbFStWNGrUqGHceeedRmpqapF5X3jhBUOSMX36dId/FwBA2bFw4UJDkrFjx44i9+Xn5xsNGjQwGjRoYPzxxx+GYRhGbGys0bRpU5e28d577xmSjPnz5zscs3HjRkOS8corr9iWLViwwGjZsqXh7+9vVKtWzYiNjTXWr19vt54zfXfmzJlGnTp1DH9/fyMmJsb49ttvjdjYWCM2NtY25osvvjAkGStWrChS27lz54x//etfxvXXX2+YzWYjJibG2LZtW5E5DMMwcnNzjaefftqoV6+eUaFCBSM0NNS49957jR9//LHIvA8//LAhyViyZMml/nwAgDLiUj31pZdeMiQZ3bp1My5evHjJ962xsbGGJKNr1652yx29hy3sYY62XeiRRx4xJBXbkwo988wzhiRj9+7dhmE419eceT+Zm5tr3H///UaVKlWM4OBgo3fv3saxY8cMScbkyZNt4y71d/nvf/9r9OrVy6hatapRpUoV47777jN++eWXInMYhmH89NNPxsCBA40aNWoY/v7+Rv369Y2RI0ca58+fLzJv06ZNDR8fH+O///2vw78LUJaZDKOY80AAADzqlVde0ZgxY5SVlaUbb7zR2+UAAFBmjRkzRvPnz1d2drYCAwO9XQ4AANekli1b6rrrrtOGDRu8XQrgEZwTHQCuMsMwNH/+fMXGxhKgAwBwBc6dO6eUlBTdc889BOgAAHjJt99+q127dmngwIHeLgXwGM6JDgBXydmzZ7VmzRp98cUX+uGHH/TBBx94uyQAAMqkY8eO6bPPPtPKlSv166+/lvi6LQAAoOT27Nmj1NRUzZw5U9dff7369Onj7ZIAjyFEB4Cr5Pjx44qPj1fVqlU1fvx4de/e3dslAQBQJu3du1f9+/dXzZo19eqrr6pFixbeLgkAgGvOypUrNXXqVDVp0kRLly695MVegbKOc6IDAAAAAAAAAOCA18+JPmfOHFksFgUEBCg6Olrbt2+/5Pjk5GQ1adJEZrNZYWFhGjNmjM6dO2e7Pz8/XxMnTlS9evVkNpvVoEEDTZs2TXxWAAAAAAAAAABwlVdP57J8+XIlJiZq3rx5io6OVnJysjp37qx9+/apZs2aRcYvWbJEY8eO1YIFC9SmTRtlZGRo8ODBMplMmjVrliRpxowZmjt3rt5++201bdpU3377rYYMGaIqVaro0UcfvdoPEQAAAAAAAABQhnn1dC7R0dFq3bq1XnvtNUlSQUGBwsLC9Mgjj2js2LFFxo8aNUppaWnasGGDbdm//vUvffPNN9qyZYskqVu3bqpVq5bmz59vG3PPPffIbDYrJSXFqboKCgr0yy+/KDg4WCaT6UoeIgAA5YphGDp9+rRq164tH58r/0IbPRcAgKLotwAAXB3O9lyvHYl+4cIFpaamaty4cbZlPj4+6tixo7Zt21bsOm3atFFKSoq2b9+uqKgoHThwQB999JEGDBhgN+bNN99URkaGGjdurN27d2vLli22I9Wd8csvvygsLKzkDw4AgHLu8OHDuuGGG654HnouAACO0W8BALg6LtdzvRainzhxQvn5+apVq5bd8lq1aik9Pb3YdeLj43XixAndfvvtMgxDf/zxhx588EGNHz/eNmbs2LHKyclReHi4fH19lZ+fr+eee079+/d3WMv58+d1/vx52+3Cg/MPHz6sypUrX8nDBACgXMnJyVFYWJiCg4NLtD49FwCAy6PfAgBwdTjbc716TnRXbdy4UdOnT9frr7+u6Oho7d+/X6NHj9a0adM0ceJESdK7776rxYsXa8mSJWratKl27dqlxx57TLVr19agQYOKnTcpKUlTpkwpsrxy5cq8wAAAoBgl/So4PRcAAOfRbwEAuDou13O9dk70CxcuKDAwUCtXrlTPnj1tywcNGqRTp07pgw8+KLJO27Ztdeutt+rFF1+0LUtJSdGIESN05swZ+fj4KCwsTGPHjtXIkSNtY5599lmlpKQ4PML975/SF34C8fvvv/MCAwCAv8jJyVGVKlVK3CPpuQAAXB79FgCAq8PZnuu1I9ErVqyoyMhIbdiwwRaiFxQUaMOGDRo1alSx6+Tm5hY5wbuvr6+k/309zdGYgoICh7X4+/vL39+/pA8FAAA4iZ4LAIDn0W8BAHAvr57OJTExUYMGDVKrVq0UFRWl5ORknT17VkOGDJEkDRw4UHXq1FFSUpIkKS4uTrNmzVLLli1tp3OZOHGi4uLibGF6XFycnnvuOd14441q2rSpvvvuO82aNUtDhw712uMEAAAAAAAAAJRNXg3R+/Tpo+PHj2vSpEnKzs5WixYttG7dOtvFRg8dOmR3VPmECRNkMpk0YcIE/fzzz6pRo4YtNC80e/ZsTZw4UQ8//LCOHTum2rVr64EHHtCkSZOu+uMDAAAAAAAAAJRtXjsneml2peefAwCgvHJ3j6TnAgBQFP0WAICrw9ke6ePwHgAAAAAAAAAArnGE6AAAAAAAAAAAOECIDgAAAAAAAACAA4ToAAAAAAAAAAA4QIgOAAAAAAAAAIADhOgAAAAAAAAAADhAiA4AAAAAAAAAgAOE6AAAAAAAAAAAOECIDgAAAAAAAACAA4ToAAAAAAAAAAA4QIgOAAAAAAAAAIADft4uAKVTbm6u0tPTnRqbl5enrKwsWSwWmc1mp7cRHh6uwMDAkpYIAAAAAAAAAB5HiI5ipaenKzIy0qPbSE1NVUREhEe3AQAAAAAAAABXghAdxQoPD1dqaqpTY9PS0pSQkKCUlBRZrVaXtgEAAAAAAAAApRkhOooVGBjo8lHiVquVI8sBAAAAAAAAlCtcWBQAAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAe8HqLPmTNHFotFAQEBio6O1vbt2y85Pjk5WU2aNJHZbFZYWJjGjBmjc+fO2Y35+eeflZCQoOrVq8tsNqtZs2b69ttvPfkwAAAAAAAAAADlkJ83N758+XIlJiZq3rx5io6OVnJysjp37qx9+/apZs2aRcYvWbJEY8eO1YIFC9SmTRtlZGRo8ODBMplMmjVrliTpt99+U0xMjDp06KCPP/5YNWrUUGZmpqpVq3a1Hx4AAAAAAAAAoIzzaog+a9YsDR8+XEOGDJEkzZs3T2vXrtWCBQs0duzYIuO3bt2qmJgYxcfHS5IsFov69eunb775xjZmxowZCgsL08KFC23L6tWr5+FHAgAAAAAAAAAoj7x2OpcLFy4oNTVVHTt2/F8xPj7q2LGjtm3bVuw6bdq0UWpqqu2ULwcOHNBHH32ku+66yzZmzZo1atWqle677z7VrFlTLVu21FtvvXXJWs6fP6+cnBy7HwAA4H70XAAAPI9+CwCAe3ktRD9x4oTy8/NVq1Ytu+W1atVSdnZ2sevEx8dr6tSpuv3221WhQgU1aNBA7du31/jx421jDhw4oLlz56pRo0b65JNP9NBDD+nRRx/V22+/7bCWpKQkValSxfYTFhbmngcJAADs0HMBAPA8+i0AAO7l1dO5uGrjxo2aPn26Xn/9dUVHR2v//v0aPXq0pk2bpokTJ0qSCgoK1KpVK02fPl2S1LJlS+3Zs0fz5s3ToEGDip133LhxSkxMtN3OycnhRQYAAB5AzwUAwPPot0Xl5uYqPT3dqbF5eXnKysqSxWKR2Wx2ehvh4eEKDAwsaYkAgFLMayF6SEiIfH19dfToUbvlR48eVWhoaLHrTJw4UQMGDNCwYcMkSc2aNdPZs2c1YsQIPf300/Lx8dH111+vm266yW49q9Wq9957z2Et/v7+8vf3v8JHBMBZrryAlUr2IpYXsEDpRM8FAMDz6LdFpaenKzIy0qPbSE1NVUREhEe3AQDwDq+F6BUrVlRkZKQ2bNignj17SvrzKPINGzZo1KhRxa6Tm5srHx/7M9D4+vpKkgzDkCTFxMRo3759dmMyMjJUt25dNz8CACXFC1gAAAAAV1N4eLhSU1OdGpuWlqaEhASlpKTIarW6tA0AQPnk1dO5JCYmatCgQWrVqpWioqKUnJyss2fPasiQIZKkgQMHqk6dOkpKSpIkxcXFadasWWrZsqXtdC4TJ05UXFycLUwfM2aM2rRpo+nTp6t3797avn273nzzTb355ptee5wA7LnyAlYq2YtYXsACAAAAKBQYGOjyQTZWq5UDcwAAkrwcovfp00fHjx/XpEmTlJ2drRYtWmjdunW2i40eOnTI7sjzCRMmyGQyacKECfr5559Vo0YNxcXF6bnnnrONad26td5//32NGzdOU6dOVb169ZScnKz+/ftf9ccHoHgleQEr8SIWAAAAAAAAV5/XLyw6atQoh6dv2bhxo91tPz8/TZ48WZMnT77knN26dVO3bt3cVSIAAAAAAAAA4Brlc/khAAAAAAAAAABcmwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHvH5OdFw9mZmZOn36tNvnTUtLs/vtbsHBwWrUqJFH5gYAAAAAAACASyFEv0ZkZmaqcePGHt1GQkKCx+bOyMggSAcAAAAAAABw1RGiXyMKj0BPSUmR1Wp169x5eXnKysqSxWKR2Wx269xpaWlKSEjwyBH0AAAAAAAAAHA5hOjXGKvVqoiICLfPGxMT4/Y5AQAAAAAAAMDbuLAoAAAAAAAAAAAOEKIDAAAAAAAAAOAAIToAAAAAAAAAAA4QogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADgACE6AAAAAAAAAAAOEKIDAAAAAAAAAOAAIToAAAAAAAAAAA4QogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADgACE6AAAAAAAAAAAO+Hm7AAAAAAAAAABAUbm5uUpPT3d6fF5enrKysmSxWGQ2m51aJzw8XIGBgSUt8ZpAiA4AAAAAAAAApVB6eroiIyM9uo3U1FRFRER4dBtlHSE6AAAAAAAAAJRC4eHhSk1NdXp8WlqaEhISlJKSIqvV6vQ2cGmE6AAAAAAAAABQCgUGBpboKHGr1crR5W7EhUUBAAAAAAAAAHCAEB0AAAAAAAAAAAc4nQsAoFzhyuUAAAAAribegwDlHyE6ALfJzMzU6dOn3T5vWlqa3W93Cg4OVqNGjdw+L7yHK5cDAAAAuJp4DwKUf4ToANwiMzNTjRs39ug2EhISPDJvRkYGQXo5wpXLAQAAAFxNvAcByj9CdABuUXgEuisvApxVkq+6OaPwhYsnjp6H93DlcgAAAABXE+9BgPKPEB2AW3nqRUBMTIzb5wQAAAAAAAAux8fbBQAAAAAAAAAAUFoRogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADgACE6AAAAAAAAAAAO+Hm7AFw9oZVMMp/KkH4pO5+dmE9lKLSSydtlAAAAAAAAALhGEaJfQx6IrCjr5gekzd6uxHlW/Vk3AAAAAAAAAHgDIfo15I3UC+ozaZGs4eHeLsVpaenpemNmvLp7uxAAAAAAAAAA1yRC9GtI9hlDeVUbS7VbeLsUp+VlFyj7jOHtMgAAAAAAAABco8rOybEBAAAAAAAAALjKCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAH/LxdAK6O3NxcSdLOnTvdPndeXp6ysrJksVhkNpvdOndaWppb5wMAAAAAAAAAVxCiXyPS09MlScOHD/dyJSUTHBzs7RIAAAAAAAAAXIMI0a8RPXv2lCSFh4crMDDQrXOnpaUpISFBKSkpslqtbp1b+jNAb9SokdvnBQAAAAAAAIDLIUS/RoSEhGjYsGEe3YbValVERIRHt4HSLbSSSeZTGdIvZeNyC+ZTGQqtZPJ2GQAAAAAAACjFCNEBuM0DkRVl3fyAtNnblTjHqj9rBgAAAAAAABwhRAfgNm+kXlCfSYtkDQ/3dilOSUtP1xsz49Xd24UAAAAAAACg1CJEB+A22WcM5VVtLNVu4e1SnJKXXaDsM4a3ywAAAAAAAEApVjZOXAwAAAAAAAAAgBcQogMAAAAAAAAA4AAhOgAAAAAAAAAADpSKEH3OnDmyWCwKCAhQdHS0tm/ffsnxycnJatKkicxms8LCwjRmzBidO3eu2LHPP/+8TCaTHnvsMQ9UDgAAAAAAAAAoz7weoi9fvlyJiYmaPHmydu7cqebNm6tz5846duxYseOXLFmisWPHavLkyUpLS9P8+fO1fPlyjR8/vsjYHTt26I033tAtt9zi6YcBAAAAAAAAACiH/LxdwKxZszR8+HANGTJEkjRv3jytXbtWCxYs0NixY4uM37p1q2JiYhQfHy9Jslgs6tevn7755hu7cWfOnFH//v311ltv6dlnn/X8AwEAeFRmZqZOnz7t9nnT0tLsfrtbcHCwGjVq5JG5AQAAAACA53k1RL9w4YJSU1M1btw42zIfHx917NhR27ZtK3adNm3aKCUlRdu3b1dUVJQOHDigjz76SAMGDLAbN3LkSHXt2lUdO3YkRAeAMi4zM1ONGzf26DYSEhI8NndGRgZBOgAAAAAAZZRXQ/QTJ04oPz9ftWrVslteq1YtpaenF7tOfHy8Tpw4odtvv12GYeiPP/7Qgw8+aHc6l2XLlmnnzp3asWOHU3WcP39e58+ft93OyckpwaMBAHhK4RHoKSkpslqtbp07Ly9PWVlZslgsMpvNbp07LS1NCQkJHjmCvqyi5wIA4Hn0WwAA3Mvrp3Nx1caNGzV9+nS9/vrrio6O1v79+zV69GhNmzZNEydO1OHDhzV69GitX79eAQEBTs2ZlJSkKVOmeLhyAMCVslqtioiIcPu8MTExbp8TxaPnAgDgefRbAADcy6sXFg0JCZGvr6+OHj1qt/zo0aMKDQ0tdp2JEydqwIABGjZsmJo1a6ZevXpp+vTpSkpKUkFBgVJTU3Xs2DFFRETIz89Pfn5+2rRpk1599VX5+fkpPz+/yJzjxo3T77//bvs5fPiwRx4vAADXOnouAACeR78FAMC9vHokesWKFRUZGakNGzaoZ8+ekqSCggJt2LBBo0aNKnad3Nxc+fjYZ/++vr6SJMMw9I9//EM//PCD3f1DhgxReHi4nnrqKdvYv/L395e/v78bHhEAALgUei4AAJ5HvwUAwL28fjqXxMREDRo0SK1atVJUVJSSk5N19uxZDRkyRJI0cOBA1alTR0lJSZKkuLg4zZo1Sy1btrSdzmXixImKi4uTr6+vgoODdfPNN9ttIygoSNWrVy+yHAAAAAAAAACAS/F6iN6nTx8dP35ckyZNUnZ2tlq0aKF169bZLjZ66NAhuyPPJ0yYIJPJpAkTJujnn39WjRo1FBcXp+eee85bDwEAAAAAAAAAUE55PUSXpFGjRjk8fcvGjRvtbvv5+Wny5MmaPHmy0/P/fQ4AAAAAAAAAAJzh1QuLAgAAAAAAAABQmpWKI9EBAAAAAABclZmZqdOnT7t1zrS0NLvf7hYcHKxGjRp5ZG4AgGcQogMAAAAAgDInMzNTjRs39tj8CQkJHps7IyODIB0AyhBCdAAAAAAAUOYUHoGekpIiq9Xqtnnz8vKUlZUli8Uis9nstnmlP49uT0hIcPvR8wAAzyJEBwAAAAAAZZbValVERIRb54yJiXHrfACAso0LiwIAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADgACE6AAAAAAAAAAAOEKIDAAAAAAAAAOAAIToAAAAAAAAAAA4QogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADggJ+3CwBQPuTm5kqSdu7c6fa58/LylJWVJYvFIrPZ7LZ509LS3DYXAAAAAAAAyidCdABukZ6eLkkaPny4lytxXXBwsLdLAAAAAAAAQClFiA7ALXr27ClJCg8PV2BgoFvnTktLU0JCglJSUmS1Wt06d3BwsBo1auTWOeEZoZVMMp/KkH4pO2ciM5/KUGglk7fLAAAAAAAAV4AQHYBbhISEaNiwYR7dhtVqVUREhEe3gdLrgciKsm5+QNrs7UqcZ9WfdQMAAAAAgLKLEB0AUCa8kXpBfSYtkjU83NulOC0tPV1vzIxXd28XAgAAAAAASowQHQBQJmSfMZRXtbFUu4W3S3FaXnaBss8Y3i4DAAAAAABcgbJzYlkAAAAAAAAAAK4yjkQHAADwktzcXKWnpzs9Pi8vT1lZWbJYLDKbzU6t44kLPgMAAADAtYQQHQAAwEvS09MVGRnp0W2kpqZyUeZrmCsf1PAhDQAAAFA8QnQAAAAvCQ8PV2pqqtPj09LSlJCQoJSUFFmtVqe3gWuXpz+o4UMaAAAAXAsI0QEAALwkMDCwRAGk1WoluIRTXPmghg9pAAAAgOIRogMAAADlVEk+qOFDGgAAAMAeITqK5cr5M9PS0ux+O4tzaAIAAAAAAAAo7QjRUaySnD8zISHBpfGcQxMAAAAAAABAaUeIjmK5cv7MvLw8ZWVlyWKxyGw2u7QNAAAAAAAAACjNCNFRLFfPnxkTE+PBagAAAAAAAADAO3y8XQAAAAAAAAAAAKUVIToAAAAAAAAAAA4QogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADgACE6AAAAAAAAAAAOEKIDAAAAAAAAAOCAn7cLAAAAAAAAAEqbzMxMnT592u3zpqWl2f12p+DgYDVq1Mjt8wLXOpdDdIvFoqFDh2rw4MG68cYbPVETgHIuNzdX6enpTo8vyQuM8PBwBQYGulwbAAAAAACZmZlq3LixR7eRkJDgkXkzMjII0gE3czlEf+yxx7Ro0SJNnTpVHTp00P33369evXrJ39/fE/UBKIfS09MVGRnp8nquvMBITU1VRESEy9sAAAAAAKDwCPSUlBRZrVa3zp2Xl6esrCxZLBaZzWa3zZuWlqaEhASPHD0PXOtKFKI/9thj2rlzpxYtWqRHHnlEDz/8sOLj4zV06FBCKwCXFR4ertTUVKfHl+QFRnh4eEnLAwAAAABAkmS1Wj2SdcXExLh9TgCeU+JzokdERCgiIkIzZ87U66+/rqeeekpz585Vs2bN9Oijj2rIkCEymUzurBVAOREYGOjyixBeYAAAAAAAAMAbShyiX7x4Ue+//74WLlyo9evX69Zbb9X999+v//73vxo/frw+++wzLVmyxJ21AgAAAAAA2IRWMsl8KkP6xcfbpTjFfCpDoZU44BAAyhqXQ/SdO3dq4cKFWrp0qXx8fDRw4EC9/PLLdqdO6NWrl1q3bu3WQgEAAAAAAP7qgciKsm5+QNrs7UqcY9WfNQMAyhaXQ/TWrVurU6dOmjt3rnr27KkKFSoUGVOvXj317dvXLQUCAAAAAAAU543UC+ozaZGsZeSaSGnp6XpjZry6e7sQAIBLXA7RDxw4oLp1615yTFBQkBYuXFjiogAAAAAAAC4n+4yhvKqNpdotvF2KU/KyC5R9xvB2GQAAF7l80rBjx47pm2++KbL8m2++0bfffuuWogAAAAAAAAAAKA1cDtFHjhypw4cPF1n+888/a+TIkW4pCgAAAAAAAACA0sDlEH3v3r2KiIgosrxly5bau3evW4oCAAAAAAAAAKA0cDlE9/f319GjR4ssP3LkiPz8XD7FOgAAAAAAAAAApZbLIfo///lPjRs3Tr///rtt2alTpzR+/Hh16tTJrcUBAAAAAAAAAOBNLh86/tJLL6ldu3aqW7euWrZsKUnatWuXatWqpXfeecftBQIAAAAAAAAA4C0uh+h16tTR999/r8WLF2v37t0ym80aMmSI+vXrpwoVKniiRgAAAAAAAAAAvKJEJzEPCgrSiBEj3F0LAABAuZCZmanTp0+7fd60tDS73+4WHBysRo0aeWRuAAAAACirSnwl0L179+rQoUO6cOGC3fLu3btfcVEAAABlVWZmpho3buzRbSQkJHhs7oyMDIJ0AAAAAPgLl0P0AwcOqFevXvrhhx9kMplkGIYkyWQySZLy8/PdWyEAAEAZUngEekpKiqxWq1vnzsvLU1ZWliwWi8xms1vnTktLU0JCgkeOoAcAAACAsszlEH306NGqV6+eNmzYoHr16mn79u369ddf9a9//UsvvfSSJ2oEAAAoc6xWqyIiItw+b0xMjNvnBAAAAAA45nKIvm3bNn3++ecKCQmRj4+PfHx8dPvttyspKUmPPvqovvvuO0/UCQAAAAAAAADAVedyiJ6fn6/g4GBJUkhIiH755Rc1adJEdevW1b59+0pUxJw5c/Tiiy8qOztbzZs31+zZsxUVFeVwfHJysubOnatDhw4pJCRE9957r5KSkhQQECBJSkpK0qpVq5Seni6z2aw2bdpoxowZatKkSYnqAwAAAAAAAAB3yczM9MipFNPS0ux+u1twcPA1eQ0ll0P0m2++Wbt371a9evUUHR2tF154QRUrVtSbb76p+vXru1zA8uXLlZiYqHnz5ik6OlrJycnq3Lmz9u3bp5o1axYZv2TJEo0dO1YLFixQmzZtlJGRocGDB8tkMmnWrFmSpE2bNmnkyJFq3bq1/vjjD40fP17//Oc/tXfvXgUFBblcIwAAAAAAAAC4Q2Zmpho3buzRbSQkJHhs7oyMjGsuSHc5RJ8wYYLOnj0rSZo6daq6deumtm3bqnr16lq+fLnLBcyaNUvDhw/XkCFDJEnz5s3T2rVrtWDBAo0dO7bI+K1btyomJkbx8fGSJIvFon79+umbb76xjVm3bp3dOosWLVLNmjWVmpqqdu3auVwjAMC7cnNzJUk7d+50+9yevlAjAAAAAAB/VXgEekpKiqxWq1vn9vR73ISEBI8cQV/auRyid+7c2fbfDRs2VHp6uk6ePKlq1arJZDK5NNeFCxeUmpqqcePG2Zb5+PioY8eO2rZtW7HrtGnTRikpKdq+fbuioqJ04MABffTRRxowYIDD7fz++++SpOuuu67Y+8+fP6/z58/bbufk5Lj0OAAAnpWeni5JGj58uJcrKZnC06CBngsAwNVAvwWAssFqtSoiIsLt88bExLh9zmudSyH6xYsXZTabtWvXLt1888225Y7C6cs5ceKE8vPzVatWLbvltWrVsgUmfxcfH68TJ07o9ttvl2EY+uOPP/Tggw9q/PjxxY4vKCjQY489ppiYGLua/yopKUlTpkwp0WMAAHhez549JUnh4eEKDAx069yFn6R74ggA6do9X5wj9FwAADyPfgsAgHu5FKJXqFBBN954o/Lz8z1Vz2Vt3LhR06dP1+uvv67o6Gjt379fo0eP1rRp0zRx4sQi40eOHKk9e/Zoy5YtDuccN26cEhMTbbdzcnIUFhbmkfoBAK4LCQnRsGHDPLoNTx0BAHv0XAAAPI9+CwCAe7l8Openn35a48eP1zvvvFPiI9ALhYSEyNfXV0ePHrVbfvToUYWGhha7zsSJEzVgwABbmNKsWTOdPXtWI0aM0NNPPy0fHx/b2FGjRunDDz/U5s2bdcMNNzisw9/fX/7+/lf0WAAAwOXRcwEA8Dz6LQAA7uVyiP7aa69p//79ql27turWraugoCC7+1256FvFihUVGRmpDRs22L6qX1BQoA0bNmjUqFHFrpObm2sXlEuSr6+vJMkwDNvvRx55RO+//742btyoevXqOV0TAAAAUJplZmZ65GJOhRdD9tRFkTm9FQAAAMoql0P0wrDbXRITEzVo0CC1atVKUVFRSk5O1tmzZzVkyBBJ0sCBA1WnTh0lJSVJkuLi4jRr1iy1bNnSdjqXiRMnKi4uzhamjxw5UkuWLNEHH3yg4OBgZWdnS5KqVKni9qvSAgAAAFdLZmamGjdu7NFtJCQkeGzujIwMgnQAAACUOS6H6JMnT3ZrAX369NHx48c1adIkZWdnq0WLFlq3bp3tYqOHDh2yO/J8woQJMplMmjBhgn7++WfVqFFDcXFxeu6552xj5s6dK0lq37693bYWLlyowYMHu7V+AAAA4GopPALdExdDzsvLU1ZWliwWi9sPPCm8iLMnjqAHAAAAPM3lEN0TRo0a5fD0LRs3brS77efnp8mTJ18yzC88rQsAAABQHnnqYsgxMTFunxMAAAAo61wO0X18fGQymRzen5+ff0UFAQAAAAAAAABQWrgcor///vt2ty9evKjvvvtOb7/9tqZMmeK2wgAAAAAAAAAA8DaXQ/QePXoUWXbvvfeqadOmWr58ue6//363FAYAAAAAAAAAgLf5XH6Ic2699VZt2LDBXdMBAAAAAAAAAOB1bgnR8/Ly9Oqrr6pOnTrumA4AAAAAAAAAgFLB5dO5VKtWze7CooZh6PTp0woMDFRKSopbiwMAACiLQiuZZD6VIf3iti/9eZz5VIZCKzm+eDwAAAAAXKtcDtFffvlluxDdx8dHNWrUUHR0tKpVq+bW4gAAAMqiByIryrr5AWmztytxnlV/1g0AAAAAsOdyiD548GAPlAEAAFB+vJF6QX0mLZI1PNzbpTgtLT1db8yMV3dvFwIAAAAApYzLIfrChQtVqVIl3XfffXbLV6xYodzcXA0aNMhtxQEAAJRF2WcM5VVtLNVu4e1SnJaXXaDsM4a3ywAAAACAUsflE3UmJSUpJCSkyPKaNWtq+vTpbikKAAAAAAAAAIDSwOUQ/dChQ6pXr16R5XXr1tWhQ4fcUhQAAAAAAAAAAKWBy6dzqVmzpr7//ntZLBa75bt371b16tXdVRcAAAAAoBTLzc1Venq6U2Pz8vKUlZUli8Uis9ns9DbCw8MVGBhY0hJRzuXm5kqSdu7c6dZ5S7q/OiMtLc2t88GzQiuZZD6VIf3i8jGoXmE+laHQSiZvlwGUSy6H6P369dOjjz6q4OBgtWvXTpK0adMmjR49Wn379nV7gQAAAACA0ic9PV2RkZEe3UZqaqoiIiI8ug2UXYUf4gwfPtzLlbguODjY2yXACQ9EVpR18wPSZm9X4hyr/qwZgPu5HKJPmzZNWVlZ+sc//iE/vz9XLygo0MCBAzknOgAAAABcI8LDw5WamurU2LS0NCUkJCglJUVWq9WlbQCO9OzZU5L7v7FQ0v3VWcHBwWrUqJHb54X7vZF6QX0mLZK1jDwXpaWn642Z8eru7UKAcsjlEL1ixYpavny5nn32We3atUtms1nNmjVT3bp1PVEfAAAAAKAUCgwMdPkocavVypHlcJuQkBANGzbMY/OzvyL7jKG8qo2l2i28XYpT8rILlH3G8HYZQLnkcoheqFGjRnxyCgAAAAAAAAAuKmvn3Jeu7fPuuxyi33PPPYqKitJTTz1lt/yFF17Qjh07tGLFCrcVBwAAAAAAAADlTVk75750bZ933+UQffPmzXrmmWeKLL/zzjs1c+ZMd9QEAAAAAAAAAOVWWTvnvnRtn3ff5RD9zJkzqlix6CcOFSpUUE5OjluKAgAAAAAAAIDyqqydc1+6ts+77/JJd5o1a6bly5cXWb5s2TLddNNNbikKAAAAAAAAAIDSwOUj0SdOnKi7775bP/74o+644w5J0oYNG7RkyRKtXLnS7QUCAAAAAAAAAOAtLofocXFxWr16taZPn66VK1fKbDarefPm+vzzz3Xdddd5okYAAAAAAAAAALzC5RBdkrp27aquXbtKknJycrR06VI9/vjjSk1NVX5+vlsLBAAAAAAAAADAW1w+J3qhzZs3a9CgQapdu7ZmzpypO+64Q19//bU7awMAAAAAAAAAwKtcOhI9OztbixYt0vz585WTk6PevXvr/PnzWr16NRcVBQAAAAAAAACUO04fiR4XF6cmTZro+++/V3Jysn755RfNnj3bk7UBAAAAAAAAAOBVTh+J/vHHH+vRRx/VQw89pEaNGnmyJgAAAACAF2RmZur06dNunzctLc3ut7sFBwfzPhUAAHiM0yH6li1bNH/+fEVGRspqtWrAgAHq27evJ2sDAAAoc3JzcyVJO3fudPvceXl5ysrKksVikdlsduvcngq2AJQdmZmZaty4sUe3kZCQ4LG5MzIyCNIBAIBHOB2i33rrrbr11luVnJys5cuXa8GCBUpMTFRBQYHWr1+vsLAwBQcHe7JWAACAUi89PV2SNHz4cC9XUjK8ngOuXYVHoKekpMhqtbp1bk9/CJiQkOCRI+gBAAAkFy8sKklBQUEaOnSohg4dqn379mn+/Pl6/vnnNXbsWHXq1Elr1qzxRJ0AAABlQs+ePSVJ4eHhCgwMdOvchUGRJwIuidMhAPiT1WpVRESE2+eNiYlx+5wAAABXg8sh+l81adJEL7zwgpKSkvSf//xHCxYscFddAAAAZVJISIiGDRvm0W14KuACAAAAABR1RSF6IV9fX/Xs2dN25BUAAN6Sm5trO52GM0pyoTNPHGEMAAAAAABKJ7eE6AAAlBbp6emKjIx0eT1XLnSWmprKUcAAAAAAAFwjCNEBAOVKeHi4UlNTnR5fkgudhYeHl7Q8AAAAAABQxhCiAwDKlcDAQJePEudCZwDKktBKJplPZUi/+Hi7FKeZT2UotJLJ22UAAAAAJUKIDgAAAJQhD0RWlHXzA9Jmb1fiPKv+rBsAAAAoiwjRAQAAgDLkjdQL6jNpkaxl6NRSaenpemNmvLp7uxAAAACgBAjRAQAAgDIk+4yhvKqNpdotvF2K0/KyC5R9xvB2GQAAAECJlJ0TKQIAAAAAAAAAcJURogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADgACE6AAAAAAAAAAAOEKIDAAAAAAAAAOAAIToAAAAAAAAAAA4QogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADggJ+3CwAAAAAAlA6hlUwyn8qQfik7x1uZT2UotJLJ22UAAIByjBAdAAAAACBJeiCyoqybH5A2e7sS51n1Z90AAACeQogOAAAAAJAkvZF6QX0mLZI1PNzbpTgtLT1db8yMV3dvFwIAAMotQnQAAAAAgCQp+4yhvKqNpdotvF2K0/KyC5R9xvB2GQAAoBwrOye6AwAAAAAAAADgKuNIdAAAAAAAAOAvcnNzJUk7d+50+9x5eXnKysqSxWKR2Wx227xpaWlumwuAPUJ0AAAAAAAA4C/S09MlScOHD/dyJa4LDg72dglAuUOIDgAAAAAAAPxFz549JUnh4eEKDAx069xpaWlKSEhQSkqKrFarW+cODg5Wo0aN3DonAEJ0AAAAAAAAwE5ISIiGDRvm0W1YrVZFRER4dBsA3KNUXFh0zpw5slgsCggIUHR0tLZv337J8cnJyWrSpInMZrPCwsI0ZswYnTt37ormBAAAAAAAAADg77x+JPry5cuVmJioefPmKTo6WsnJyercubP27dunmjVrFhm/ZMkSjR07VgsWLFCbNm2UkZGhwYMHy2QyadasWSWaEwAAAACudWXxInoSF9IDAACe5/UQfdasWRo+fLiGDBkiSZo3b57Wrl2rBQsWaOzYsUXGb926VTExMYqPj5ckWSwW9evXT998802J5wQAAACAa11ZvoiexIX0AACA53g1RL9w4YJSU1M1btw42zIfHx917NhR27ZtK3adNm3aKCUlRdu3b1dUVJQOHDigjz76SAMGDCjxnAAAAABwrSurF9GTuJAeAKBs4dtfZY9XQ/QTJ04oPz9ftWrVslteq1Yt21EQfxcfH68TJ07o9ttvl2EY+uOPP/Tggw9q/PjxJZ7z/PnzOn/+vO12Tk7OlTwsAADgAD0XAEovLqJXftBvAaB049tfZY/XT+fiqo0bN2r69Ol6/fXXFR0drf3792v06NGaNm2aJk6cWKI5k5KSNGXKFDdXCgAA/o6eCwCA59FvAaB049tfZY9XQ/SQkBD5+vrq6NGjdsuPHj2q0NDQYteZOHGiBgwYYDtColmzZjp79qxGjBihp59+ukRzjhs3TomJibbbOTk5CgsLu5KHBgAAikHPBQDA8+i3AFC68e2vssfHmxuvWLGiIiMjtWHDBtuygoICbdiwQbfddlux6+Tm5srHx75sX19fSZJhGCWa09/fX5UrV7b7AQAA7kfPBQDA8+i3AAC4l9dP55KYmKhBgwapVatWioqKUnJyss6ePashQ4ZIkgYOHKg6deooKSlJkhQXF6dZs2apZcuWttO5TJw4UXFxcbYw/XJzAgAAAAAAAADgDK+H6H369NHx48c1adIkZWdnq0WLFlq3bp3twqCHDh2yO/J8woQJMplMmjBhgn7++WfVqFFDcXFxeu6555yeEwAAAAAAAAAAZ3g9RJekUaNGadSoUcXet3HjRrvbfn5+mjx5siZPnlziOQEAAICyKDc3V5K0c+dOt8+dl5enrKwsWSwWmc1mt86dlpbm1vkAAACAq6lUhOgAAAAALi89PV2SNHz4cC9XUjLBwcHeLgEAAABwGSE6AAAAUEb07NlTkhQeHq7AwEC3zp2WlqaEhASlpKTIarW6dW7pzwC9UaNGbp8XAAAA8DRCdAAAAKCMCAkJ0bBhwzy6DavVqoiICI9uAwAAAChLfC4/BAAAAAAAAACAaxMhOgAAAAAAAAAADhCiAwAAAAAAAADgAOdEBwAA8JLc3Fylp6c7PT4tLc3utzM8cQFKAAAAALiWEKIDAAB4SXp6uiIjI11eLyEhwemxqampXCQSAAAAAK4AIToAAICXhIeHKzU11enxeXl5ysrKksVikdlsdnobAAAAAICSI0QHAADwksDAQJePEo+JifFQNQAAAACA4nBhUQAAAAAAAAAAHCBEBwAAAAAAAADAAUJ0AAAAAAAAAAAcIEQHAAAAAAAAAMABQnQAAAAAAAAAABwgRAcAAAAAAAAAwAFCdAAAAAAAAAAAHCBEBwAAAAAAAADAAUJ0AAAAAAAAAAAcIEQHAAAAAAAAAMABQnQAAAAAAAAAABwgRAcAAAAAAAAAwAFCdAAAAAAAAAAAHCBEBwAAAAAAAADAAUJ0AAAAAAAAAAAcIEQHAAAAAAAAAMABQnQAAAAAAAAAABwgRAcAAAAAAAAAwAFCdAAAAAAAAAAAHCBEBwAAAAAAAADAAUJ0AAAAAAAAAAAcIEQHAAAAAAAAAMABQnQAAAAAAAAAABzw83YBAAAAAICyJzc3V+np6U6NTUtLs/vtrPDwcAUGBrpcGwAAgDsRogMAAAAAXJaenq7IyEiX1klISHBpfGpqqiIiIlxaBwAAwN0I0QEAAAAALgsPD1dqaqpTY/Py8pSVlSWLxSKz2ezSNgAAALyNEB0AAAAA4LLAwECXjhKPiYnxYDUAAACew4VFAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAAB/y8XQAAAAAAz8jNzVV6erpTY9PS0ux+OyM8PFyBgYElqg0AAAAoKwjRAQAAgHIqPT1dkZGRLq2TkJDg9NjU1FRFRES4WhYAAABQphCiAwAAAOVUeHi4UlNTnRqbl5enrKwsWSwWmc1mp+cHAAAAyjtCdAAAAKCcCgwMdOlI8ZiYGA9WAwAAAJRNXFgUAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAH/LxdAAAAAAAAgCfl5uYqPT3dqbFpaWl2v50VHh6uwMBAl2sDAJR+hOgAAAAAAKBcS09PV2RkpEvrJCQkuDQ+NTVVERERLq0DACgbCNEBAAAAAEC5Fh4ertTUVKfG5uXlKSsrSxaLRWaz2aVtAADKJ0J0AAAAAABQrgUGBrp0lHhMTIwHqwEAlDWl4sKic+bMkcViUUBAgKKjo7V9+3aHY9u3by+TyVTkp2vXrrYxZ86c0ahRo3TDDTfIbDbrpptu0rx5867GQwEAAAAAAAAAlCNeD9GXL1+uxMRETZ48WTt37lTz5s3VuXNnHTt2rNjxq1at0pEjR2w/e/bska+vr+677z7bmMTERK1bt04pKSlKS0vTY489plGjRmnNmjVX62EBAAAAAAAAAMoBr5/OZdasWRo+fLiGDBkiSZo3b57Wrl2rBQsWaOzYsUXGX3fddXa3ly1bpsDAQLsQfevWrRo0aJDat28vSRoxYoTeeOMNbd++Xd27d/fcgwEAAAAAAMA1JTc3V+np6U6PT0tLs/vtjPDwcAUGBrpcGwD38GqIfuHCBaWmpmrcuHG2ZT4+PurYsaO2bdvm1Bzz589X3759FRQUZFvWpk0brVmzRkOHDlXt2rW1ceNGZWRk6OWXXy52jvPnz+v8+fO22zk5OSV8RAAA4FLouQAAeB79Fri60tPTFRkZ6fJ6CQkJTo9NTU116bz+ANzLqyH6iRMnlJ+fr1q1atktr1WrllOf4G3fvl179uzR/Pnz7ZbPnj1bI0aM0A033CA/Pz/5+PjorbfeUrt27YqdJykpSVOmTCn5AwEAAE6h5wIA4Hn0W+DqCg8PV2pqqtPj8/LylJWVJYvFIrPZ7PQ2AHiP10/nciXmz5+vZs2aKSoqym757Nmz9fXXX2vNmjWqW7euNm/erJEjR6p27drq2LFjkXnGjRunxMRE2+2cnByFhYV5vH4AAK419FwAADyPfgtcXYGBgS4fJR4TE+OhagB4gldD9JCQEPn6+uro0aN2y48eParQ0NBLrnv27FktW7ZMU6dOtVuel5en8ePH6/3331fXrl0lSbfccot27dqll156qdgQ3d/fX/7+/lf4aAAAwOXQcwEA8Dz6LQAA7uXjzY1XrFhRkZGR2rBhg21ZQUGBNmzYoNtuu+2S665YsULnz58vcv6oixcv6uLFi/LxsX9ovr6+KigocF/xAAAAAAAAAIByz+unc0lMTNSgQYPUqlUrRUVFKTk5WWfPntWQIUMkSQMHDlSdOnWUlJRkt978+fPVs2dPVa9e3W555cqVFRsbqyeeeEJms1l169bVpk2b9O9//1uzZs26ao8LAAAAAAAAAFD2eT1E79Onj44fP65JkyYpOztbLVq00Lp162wXGz106FCRo8r37dunLVu26NNPPy12zmXLlmncuHHq37+/Tp48qbp16+q5557Tgw8+6PHHAwAAAAAAAAAoP7weokvSqFGjNGrUqGLv27hxY5FlTZo0kWEYDucLDQ3VwoUL3VUeAAAAAAAAAOAa5dVzogMAAAAAAAAAUJoRogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADgACE6AAAAAAAAAAAOEKIDAAAAAAAAAOAAIToAAAAAAAAAAA4QogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADgACE6AAAAAAAAAAAOEKIDAAAAAAAAAOAAIToAAAAAAAAAAA4QogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADgACE6AAAAAAAAAAAOEKIDAAAAAAAAAOAAIToAAAAAAAAAAA4QogMAAAAAAAAA4AAhOgAAAAAAAAAADhCiAwAAAAAAAADggJ+3CwAAAAAAAAAAFJWbm6v09HSnx6elpdn9dkZ4eLgCAwNdru1aQogOAAAAAAAAAKVQenq6IiMjXV4vISHB6bGpqamKiIhweRvXEkJ0AAAAAAAAACiFwsPDlZqa6vT4vLw8ZWVlyWKxyGw2O70NXBohOgAAAAAAAACUQoGBgS4fJR4TE+Ohaq5dXFgUAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAAB/y8XQAAAAAAoPzKz8/Xl19+qSNHjuj6669X27Zt5evr6+2yAAAAnMaR6AAAAAAAj1i1apUaNmyoDh06KD4+Xh06dFDDhg21atUqb5cGAADgNEJ0AAAAAIDbrVq1Svfee6+aNWumbdu26fTp09q2bZuaNWume++9lyAdAACUGYToAAAAAAC3ys/P17/+9S9169ZNq1ev1q233qpKlSrp1ltv1erVq9WtWzc9/vjjys/P93apAAAAl0WIDgAAAABwqy+//FJZWVkaP368fHzs33b6+Pho3LhxOnjwoL788ksvVQgAAOA8QnQAAAAAgFsdOXJEknTzzTcXe3/h8sJxAAAApRkhOgAAAADAra6//npJ0p49e4q9v3B54TgAAIDSjBAdAAAAAOBWbdu2lcVi0fTp01VQUGB3X0FBgZKSklSvXj21bdvWSxUCAAA4r1SE6HPmzJHFYlFAQICio6O1fft2h2Pbt28vk8lU5Kdr165249LS0tS9e3dVqVJFQUFBat26tQ4dOuTphwIAAAAA1zxfX1/NnDlTH374oXr27Klt27bp9OnT2rZtm3r27KkPP/xQL730knx9fb1dKgAAwGV5PURfvny5EhMTNXnyZO3cuVPNmzdX586ddezYsWLHr1q1SkeOHLH97NmzR76+vrrvvvtsY3788UfdfvvtCg8P18aNG/X9999r4sSJCggIuFoPCwAAAACuaXfffbdWrlypH374QW3atFHlypXVpk0b7dmzRytXrtTdd9/t7RIBAACcYjIMw/BmAdHR0WrdurVee+01SX9+tS8sLEyPPPKIxo4de9n1k5OTNWnSJB05ckRBQUGSpL59+6pChQp65513SlRTTk6OqlSpot9//12VK1cu0RwAAJRH7u6R9FwAKP/y8/P15Zdf6siRI7r++uvVtm1bjkC/DPotAABXh7M90qtHol+4cEGpqanq2LGjbZmPj486duyobdu2OTXH/Pnz1bdvX1uAXlBQoLVr16px48bq3LmzatasqejoaK1evdoTDwEAAAAAcAm+vr5q3769+vXrp/bt2xOgAwCAMserIfqJEyeUn5+vWrVq2S2vVauWsrOzL7v+9u3btWfPHg0bNsy27NixYzpz5oyef/55denSRZ9++ql69eqlu+++W5s2bSp2nvPnzysnJ8fuBwAAuB89FwAAz6PfAgDgXl4/J/qVmD9/vpo1a6aoqCjbssIrv/fo0UNjxoxRixYtNHbsWHXr1k3z5s0rdp6kpCRVqVLF9hMWFnZV6gcA4FpDzwUAwPPotwAAuJdXQ/SQkBD5+vrq6NGjdsuPHj2q0NDQS6579uxZLVu2TPfff3+ROf38/HTTTTfZLbdarTp06FCxc40bN06///677efw4cMleDQAAOBy6LkAAHge/RYAAPfy8+bGK1asqMjISG3YsEE9e/aU9OeR5Bs2bNCoUaMuue6KFSt0/vx5JSQkFJmzdevW2rdvn93yjIwM1a1bt9i5/P395e/vX/IHAgAAnELPBQDA8+i3AAC4l1dDdElKTEzUoEGD1KpVK0VFRSk5OVlnz57VkCFDJEkDBw5UnTp1lJSUZLfe/Pnz1bNnT1WvXr3InE888YT69Omjdu3aqUOHDlq3bp3+85//aOPGjVfjIQEAAAAAAAAAygmvh+h9+vTR8ePHNWnSJGVnZ6tFixZat26d7WKjhw4dko+P/Vln9u3bpy1btujTTz8tds5evXpp3rx5SkpK0qOPPqomTZrovffe0+233+7xxwMAAAAAAAAAKD9MhmEY3i6itMnJyVGVKlX0+++/q3Llyt4uBwCAUsPdPZKeCwBAUfRbAACuDmd7pFcvLAoAAAAAAAAAQGlGiA4AAAAAAAAAgAOE6AAAAAAAAAAAOECIDgAAAAAAAACAA37eLqA0KrzWak5OjpcrAQCgdCnsje66Ljk9FwCAoui3AABcHc72XEL0Ypw+fVqSFBYW5uVKAAAonU6fPq0qVaq4ZR6JngsAQHHotwAAXB2X67kmw10fbZcjBQUF+uWXXxQcHCyTyeTtckq9nJwchYWF6fDhw6pcubK3y0E5xD4GT2L/co1hGDp9+rRq164tH58rPyscPdc17K/wJPYveBL7l2vot97F/gpPYx+DJ7F/ucbZnsuR6MXw8fHRDTfc4O0yypzKlSvzjxMexT4GT2L/cp47jogrRM8tGfZXeBL7FzyJ/ct59FvvY3+Fp7GPwZPYv5znTM/lwqIAAAAAAAAAADhAiA4AAAAAAAAAgAOE6Lhi/v7+mjx5svz9/b1dCsop9jF4EvsXyhL2V3gS+xc8if0LZQn7KzyNfQyexP7lGVxYFAAAAAAAAAAABzgSHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBlBqLFi1S1apVvV1GEc8884xatGjh7TJQjI0bN8pkMunUqVOS2IcAwFk8X8JV9FwAcB3PlXAV/bb0IkSHJGnw4MEymUx6/vnn7ZavXr1aJpPJS1XB3bKzszV69Gg1bNhQAQEBqlWrlmJiYjR37lzl5uZ6uzz16dNHGRkZV3WbJpPJ9uPn56cbb7xRiYmJOn/+/FWto7wqfG558MEHi9w3cuRImUwmDR482G3bYx9CaUe/vXbQc4vi+dKz6LmAPXrutYF+WxTPlZ5Fv712EaLDJiAgQDNmzNBvv/3m7VLgAQcOHFDLli316aefavr06fruu++0bds2Pfnkk/rwww/12WefebtEmc1m1axZ86pvd+HChTpy5IgOHjyo119/Xe+8846effZZt27jwoULbp2vLAkLC9OyZcuUl5dnW3bu3DktWbJEN954o1u3xT6EsoB+W/7Rcx3j+dKz6LnucS3vQ+UNPbd8o986xnOlZ9Fv3aOs7UOE6LDp2LGjQkNDlZSU5HDMe++9p6ZNm8rf318Wi0UzZ860u99isWj69OkaOnSogoODdeONN+rNN9+0G3P48GH17t1bVatW1XXXXacePXooKyvLEw8Jf/Hwww/Lz89P3377rXr37i2r1ar69eurR48eWrt2reLi4iRJs2bNUrNmzRQUFKSwsDA9/PDDOnPmjG2e4r6yk5ycLIvFYru9ceNGRUVFKSgoSFWrVlVMTIx++uknSdLu3bvVoUMHBQcHq3LlyoqMjNS3334rqejXlH788Uf16NFDtWrVUqVKldS6desiL4Sc2ecup2rVqgoNDVVYWJi6deumHj16aOfOnQ7Ht2/fXo899pjdsp49e9p92myxWDRt2jQNHDhQlStX1ogRI1yqqTyJiIhQWFiYVq1aZVu2atUq3XjjjWrZsqVtWUFBgZKSklSvXj2ZzWY1b95cK1eutJvro48+UuPGjWU2m9WhQ4cizx3sQygL6LflHz3XMZ4vPYueWxT70LWNnlu+0W8d47nSs+i3RV0L+xAhOmx8fX01ffp0zZ49W//973+L3J+amqrevXurb9+++uGHH/TMM89o4sSJWrRokd24mTNnqlWrVvruu+/08MMP66GHHtK+ffskSRcvXlTnzp0VHBysL7/8Ul999ZUqVaqkLl26lLlPoMqSX3/9VZ9++qlGjhypoKCgYscUfqXRx8dHr776qv7v//5Pb7/9tj7//HM9+eSTTm/rjz/+UM+ePRUbG6vvv/9e27Zt04gRI2zz9+/fXzfccIN27Nih1NRUjR07VhUqVCh2rjNnzuiuu+7Shg0b9N1336lLly6Ki4vToUOH7MZdap9zVUZGhj7//HNFR0eXaP2/eumll9S8eXN99913mjhx4hXPV5YNHTpUCxcutN1esGCBhgwZYjcmKSlJ//73vzVv3jz93//9n8aMGaOEhARt2rRJ0p9vTu6++27FxcVp165dGjZsmMaOHXvJ7bIPoTSi35Zv9Fzn8XzpGfTckmEfKp/oueUX/dZ5PFd6Bv22ZMr0PmQAhmEMGjTI6NGjh2EYhnHrrbcaQ4cONQzDMN5//32jcDeJj483OnXqZLfeE088Ydx0002223Xr1jUSEhJstwsKCoyaNWsac+fONQzDMN555x2jSZMmRkFBgW3M+fPnDbPZbHzyySceeWwwjK+//tqQZKxatcpuefXq1Y2goCAjKCjIePLJJ4tdd8WKFUb16tVttydPnmw0b97cbszLL79s1K1b1zAMw/j1118NScbGjRuLnS84ONhYtGhRsfctXLjQqFKlyiUfS9OmTY3Zs2fbbl9un7scSUZAQIARFBRk+Pv7G5KMbt26GRcuXLCN+ftjjo2NNUaPHm03T48ePYxBgwbZ1dWzZ0+naijPCp9bjh07Zvj7+xtZWVlGVlaWERAQYBw/ftz2dzt37pwRGBhobN261W79+++/3+jXr59hGIYxbtw4u+cbwzCMp556ypBk/Pbbb4ZhsA+h9KPfln/0XMd4vvQseu6f2IdQiJ5bvtFvHeO50rPot3+6FvchjkRHETNmzNDbb7+ttLQ0u+VpaWmKiYmxWxYTE6PMzEzl5+fblt1yyy22/zaZTAoNDdWxY8ck/fk1p/379ys4OFiVKlVSpUqVdN111+ncuXP68ccfPfioUJzt27dr165datq0qe0CEZ999pn+8Y9/qE6dOgoODtaAAQP066+/On1Rluuuu06DBw9W586dFRcXp1deeUVHjhyx3Z+YmKhhw4apY8eOev755y/5//3MmTN6/PHHZbVaVbVqVVWqVElpaWlFPmG91D7njJdfflm7du3S7t279eGHHyojI0MDBgxwen1HWrVqdcVzlBc1atRQ165dtWjRIi1cuFBdu3ZVSEiI7f79+/crNzdXnTp1sj03VKpUSf/+979t+0haWlqRT75vu+22S26XfQilGf322kLP/RPPl55Hzy0Z9qHyjZ577aDf/onnSs+j35ZMWd6H/LxdAEqfdu3aqXPnzho3blyJrij8968tmUwmFRQUSPrzH3tkZKQWL15cZL0aNWqUqF5cXsOGDWUymYp8dad+/fqS/rxQhSRlZWWpW7dueuihh/Tcc8/puuuu05YtW3T//ffrwoULCgwMlI+PjwzDsJvn4sWLdrcXLlyoRx99VOvWrdPy5cs1YcIErV+/XrfeequeeeYZxcfHa+3atfr44481efJkLVu2TL169SpS9+OPP67169frpZdeUsOGDWU2m3XvvfcW+VrkpfY5Z4SGhqphw4aSpCZNmuj06dPq16+fnn32Wdvyv3LmbyDJ4dcKr1VDhw7VqFGjJElz5syxu6/wnIRr165VnTp17O7z9/cv8TbZh1Ca0W/LJ3rupfF8eXXQc/+HfQgSPbc8ot9eGs+VVwf99n+uhX2IEB3Fev7559WiRQs1adLEtsxqteqrr76yG/fVV1+pcePG8vX1dWreiIgILV++XDVr1lTlypXdWjMcq169ujp16qTXXntNjzzyiMMnrdTUVBUUFGjmzJny8fnziyrvvvuu3ZgaNWooOztbhmHYzgG3a9euInO1bNlSLVu21Lhx43TbbbdpyZIluvXWWyVJjRs3VuPGjTVmzBj169dPCxcuLPYFxldffaXBgwfb7jtz5sxVuUBP4f781ytt/1WNGjXsjjzIz8/Xnj171KFDB4/XVpYVnhfSZDKpc+fOdvfddNNN8vf316FDhxQbG1vs+larVWvWrLFb9vXXX19ym+xDKO3ot+UPPdc1PF96Bj33f9iHUIieW77Qb13Dc6Vn0G//51rYhzidC4rVrFkz9e/fX6+++qpt2b/+9S9t2LBB06ZNU0ZGht5++2299tprevzxx52et3///goJCVGPHj305Zdf6uDBg9q4caMeffTRYi/0Avd5/fXX9ccff6hVq1Zavny50tLStG/fPqWkpCg9PV2+vr5q2LChLl68qNmzZ+vAgQN65513NG/ePLt52rdvr+PHj+uFF17Qjz/+qDlz5ujjjz+23X/w4EGNGzdO27Zt008//aRPP/1UmZmZslqtysvL06hRo7Rx40b99NNP+uqrr7Rjxw5ZrdZia27UqJFWrVpl+wpRfHy8S5+cOuvUqVPKzs7WL7/8ok2bNmnq1Klq3Lixw7ruuOMOrV27VmvXrlV6eroeeughnTp1yu11lTe+vr5KS0vT3r17i7wpCQ4O1uOPP64xY8bo7bff1o8//qidO3dq9uzZevvttyVJDz74oDIzM/XEE09o3759WrJkSZGLPv0d+xBKO/pt+UTPdYzny6uDnvs/7EMoRM8tf+i3jvFceXXQb//nWtiHCNHh0NSpU+3+IUZEROjdd9/VsmXLdPPNN2vSpEmaOnWqS1+HCwwM1ObNm3XjjTfq7rvvltVq1f33369z587xqb2HNWjQQN999506duyocePGqXnz5mrVqpVmz56txx9/XNOmTVPz5s01a9YszZgxQzfffLMWL16spKQku3msVqtef/11zZkzR82bN9f27dvtXmQGBgYqPT1d99xzjxo3bqwRI0Zo5MiReuCBB+Tr66tff/1VAwcOVOPGjdW7d2/deeedmjJlSrE1z5o1S9WqVVObNm0UFxenzp07KyIiwu1/myFDhuj666/XDTfcoH79+qlp06b6+OOP5edX/Jd1hg4dqkGDBmngwIGKjY1V/fr1y9Wnq55UuXJlh//Wp02bpokTJyopKUlWq1VdunTR2rVrVa9ePUnSjTfeqPfee0+rV69W8+bNNW/ePE2fPv2S22MfQllAvy1/6LmO8Xx59dBz/8Q+hL+i55Yv9FvHeK68eui3f7oW9iGT8fcT1gAAAAAAAAAAAEkciQ4AAAAAAAAAgEOE6ADKrenTp6tSpUrF/tx5553eLg9lAPsQADiH50tcKfYhALg8nitxpdiHSo7TuQAot06ePKmTJ08We5/ZbFadOnWuckUoa9iHAMA5PF/iSrEPAcDl8VyJK8U+VHKE6AAAAAAAAAAAOMDpXAAAAAAAAAAAcIAQHQAAAAAAAAAABwjRAQAAAAAAAABwgBAdAAAAAAAAAAAHCNEBAAAAAAAAAHCAEB0AAAAAAAAAAAcI0QEAAAAAAAAAcIAQHQAAAAAAAAAAB/4/Ot0BMTWu15cAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24373/257706186.py:26: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(data, labels=d_methods)\n",
      "/tmp/ipykernel_24373/257706186.py:26: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(data, labels=d_methods)\n",
      "/tmp/ipykernel_24373/257706186.py:26: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(data, labels=d_methods)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24373/257706186.py:38: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(data, labels=d_methods)\n",
      "/tmp/ipykernel_24373/257706186.py:38: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(data, labels=d_methods)\n",
      "/tmp/ipykernel_24373/257706186.py:38: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(data, labels=d_methods)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Side-by-side box plots for accuracy, f1, and classfication time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ls_model_names = [\"SVM\", \"RF\", \"KNN\"]\n",
    "n_models = len(ls_model_names)\n",
    "d_methods = list(models_denoising_accuracies[\"SVM\"].keys())\n",
    "\n",
    "d_method_names = denoise_methods.keys()\n",
    "\n",
    "# 1) Accuracy boxplots\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 5), sharey=True)\n",
    "for ax, name, d_method_name in zip(axes, ls_model_names, d_method_names):\n",
    "    data = [models_denoising_accuracies[name][m] for m in d_methods]\n",
    "    ax.boxplot(data, labels=d_methods)\n",
    "    ax.set_title(f\"{name} Accuracy\")\n",
    "    #ax.set_xlabel(d_method_name)\n",
    "    if ax is axes[0]:\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2) F1 score boxplots\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 5), sharey=True)\n",
    "for ax, name, d_method_name in zip(axes, ls_model_names, d_method_names):\n",
    "    data = [models_denoising_f1_scores[name][m] for m in d_methods]\n",
    "    ax.boxplot(data, labels=d_methods)\n",
    "    ax.set_title(f\"{name} F1 Score\")\n",
    "    #ax.set_xlabel(d_method_name)\n",
    "    if ax is axes[0]:\n",
    "        ax.set_ylabel(\"F1 Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3) Classification time boxplots\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 5), sharey=False)\n",
    "for ax, name, d_method_name in zip(axes, ls_model_names, d_method_names):\n",
    "    data = [models_denoising_classification_times[name][m] for m in d_methods]\n",
    "    ax.boxplot(data, labels=d_methods)\n",
    "    ax.set_title(f\"{name} Time (s)\")\n",
    "    #ax.set_xlabel(d_method_name)\n",
    "    if ax is axes[0]:\n",
    "        ax.set_ylabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cdd82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37786fa1",
   "metadata": {},
   "source": [
    "### Validation Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for denoise, val_losses in val_loss_curves:\n",
    "    plt.plot(val_losses, label=denoise)\n",
    "df = pd.DataFrame(val_loss_curves)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Curve For each Denoiser')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef88e9d",
   "metadata": {},
   "source": [
    "### Confusion Matrix(How?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76b696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_rocm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
