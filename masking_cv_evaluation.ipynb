{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419936bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from masking import centre_mask, non_centre_mask, random_mask\n",
    "import time\n",
    "\n",
    "#Turn all the randomisation off to ensure the results of every execution is the same \n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239c6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "FOLDER_PATH = \"Images/100\"\n",
    "\n",
    "masks = {\n",
    "    \"Centre\": centre_mask,\n",
    "    \"Non-centre\": non_centre_mask,\n",
    "    \"Random\": random_mask \n",
    "}\n",
    "\n",
    "classification_models = [\"CNN\", \"KNN\", \"SVM\", \"Random Forest\"]\n",
    "\n",
    "models_masks_accuracies = {model: {mask: [] for mask in masks.keys()}\n",
    "                               for model in classification_models}\n",
    "models_masks_f1_scores = {model: {mask: [] for mask in masks.keys()}\n",
    "                               for model in classification_models}\n",
    "models_masks_classification_times = {model: {mask: [] for mask in masks.keys()}\n",
    "                               for model in classification_models}\n",
    "\n",
    "def build_transform(mask: str) -> transforms.Compose:\n",
    "    mask = masks.get(mask)\n",
    "\n",
    "    return transforms.Compose([\n",
    "        transforms.Lambda(mask),\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8848ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataSet(Dataset):\n",
    "    def __init__(self, image_names, transform):\n",
    "        self.file_names = []\n",
    "        self.labels = []\n",
    "        for numeric_label, names in enumerate(image_names):\n",
    "            self.labels.extend([numeric_label]*len(names))\n",
    "            self.file_names.extend(names)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.file_names[index]\n",
    "        img = Image.open(img_name).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        label = self.labels[index]\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)\n",
    "    \n",
    "\n",
    "labels = [\"Immune_Cells\", \"Non_Invasive_Tumor\", \"Invasive_Tumor_Set\"]\n",
    "le = LabelEncoder()\n",
    "numeric_labels = le.fit_transform(labels)\n",
    "image_names = []\n",
    "for _ in numeric_labels:\n",
    "    image_names.append([])\n",
    "\n",
    "for (dir_path, dir_names, file_names) in os.walk(FOLDER_PATH):\n",
    "    parent_folder = os.path.basename(dir_path)\n",
    "    if parent_folder in labels: # Read the subset of dataset to reduce training time \n",
    "        for file in file_names:\n",
    "            image = cv2.imread(os.path.join(dir_path, file))\n",
    "            if image.shape[0] < 100 and image.shape[1] < 100: #skip the small image, it doesn't give much info\n",
    "                continue\n",
    "            numeric_label = le.transform([parent_folder])[0]\n",
    "            image_names[numeric_label].append(os.path.join(dir_path, file))\n",
    "\n",
    "\n",
    "\n",
    "masking_datasets = {key : ImageDataSet(image_names, build_transform(key)) for key in masks.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #check if the computer has GPU\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(image_names))\n",
    "model = model.to(device)\n",
    "\n",
    "masking_cnn_models = {key : deepcopy(model).to(device) for key in masks.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters setting\n",
    "num_epochs = 100\n",
    "patience = 10 #for early stopping\n",
    "batch_size = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5abf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(model, train_loader, test_loader):\n",
    "    model.eval()\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1]) # remove the last layer\n",
    "    feature_extractor.eval()\n",
    "    feature_extractor.to(device)\n",
    "\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            output = feature_extractor(images).squeeze()\n",
    "            train_features.append(output.cpu().numpy())\n",
    "            train_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    X_train = np.vstack(train_features)\n",
    "    y_train = np.hstack(train_labels)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            output = feature_extractor(images).squeeze()\n",
    "            test_features.append(output.cpu().numpy())\n",
    "            test_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    X_test = np.vstack(test_features)\n",
    "    y_test = np.hstack(test_labels)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "for mask, model in masking_cnn_models.items():\n",
    "    print(f\"Start training {mask} model.\")\n",
    "    dataset = masking_datasets[mask]\n",
    "    all_indices, _ = train_test_split(list(range(len(dataset))), test_size=0.5, random_state=0) #only use 50% of the dataset\n",
    "    all_labels = [dataset[i][1] for i in all_indices]\n",
    "\n",
    "    for train_idx, test_idx in kf.split(all_indices, all_labels):\n",
    "        train_idx, val_idx = train_test_split(train_idx, test_size=0.2, random_state=0)\n",
    "\n",
    "        train_isx_set = set(train_idx)\n",
    "        val_isx_set = set(val_idx)\n",
    "        test_isx_set = set(test_idx)\n",
    "        print(train_isx_set.intersection(val_isx_set))\n",
    "        print(train_isx_set.intersection(test_isx_set))\n",
    "        print(val_isx_set.intersection(test_isx_set))\n",
    "        print(\"-------------\")\n",
    "\n",
    "        train_subset = Subset(dataset, train_idx)\n",
    "        val_subset = Subset(dataset, val_idx)\n",
    "        test_subset = Subset(dataset, test_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "        test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        epoch_no_improvement = 0\n",
    "        best_model_parameters = None\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = 100 * correct / total\n",
    "            print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epoch_no_improvement = 0\n",
    "                best_model_parameters = deepcopy(model.state_dict())\n",
    "            else:\n",
    "                epoch_no_improvement += 1\n",
    "                if epoch_no_improvement >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        if best_model_parameters is not None:\n",
    "            model.load_state_dict(best_model_parameters)      \n",
    "        \n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "        end = time.time()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        elapsed_time = end - start\n",
    "\n",
    "        models_masks_accuracies[\"CNN\"][mask].append(accuracy)\n",
    "        models_masks_f1_scores[\"CNN\"][mask].append(f1)\n",
    "        models_masks_classification_times[\"CNN\"][mask].append(elapsed_time)\n",
    "\n",
    "        print(f\"CNN - Test Accuracy: {accuracy:.2f}%, Test F1 Score: {f1:.4f}, Classification Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        feature_extraction_start = time.time()\n",
    "        X_train, y_train, X_test, y_test = feature_extractor(model, train_loader, test_loader)\n",
    "        feature_extraction_end = time.time()\n",
    "        feature_extraction_time = feature_extraction_end - feature_extraction_start\n",
    "\n",
    "        svm = SVC()\n",
    "        svm.fit(X_train, y_train)\n",
    "        start = time.time()\n",
    "        y_pred = svm.predict(X_test)\n",
    "        end = time.time()\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        classification_time = end - start + feature_extraction_time\n",
    "\n",
    "        models_masks_accuracies[\"SVM\"][mask].append(accuracy)\n",
    "        models_masks_f1_scores[\"SVM\"][mask].append(f1)\n",
    "        models_masks_classification_times[\"SVM\"][mask].append(classification_time)\n",
    "        print(f\"SVM - Test Accuracy: {accuracy:.2f}%, Test F1 Score: {f1:.4f}, Classification Time: {classification_time:.2f} seconds\")\n",
    "\n",
    "        rf = RandomForestClassifier()\n",
    "        rf.fit(X_train, y_train)\n",
    "        start = time.time()\n",
    "        y_pred = rf.predict(X_test)\n",
    "        end = time.time()\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        classification_time = end - start + feature_extraction_time\n",
    "\n",
    "        models_masks_accuracies[\"Random Forest\"][mask].append(accuracy)\n",
    "        models_masks_f1_scores[\"Random Forest\"][mask].append(f1)\n",
    "        models_masks_classification_times[\"Random Forest\"][mask].append(classification_time)\n",
    "        print(f\"Random Forest - Test Accuracy: {accuracy:.2f}%, Test F1 Score: {f1:.4f}, Classification Time: {classification_time:.2f} seconds\")\n",
    "\n",
    "        knn_accuracies = []\n",
    "        knn_f1_scores = []\n",
    "        knn_classification_times = []\n",
    "        for k in range(1, 32, 2): #k = 1 to 31\n",
    "            knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            knn.fit(X_train, y_train)\n",
    "\n",
    "            start = time.time()\n",
    "            y_pred = knn.predict(X_test)\n",
    "            end = time.time()\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            classification_time = end - start + feature_extraction_time\n",
    "\n",
    "            knn_accuracies.append(accuracy)\n",
    "            knn_f1_scores.append(f1)\n",
    "            knn_classification_times.append(classification_time)\n",
    "\n",
    "        knn_accuracies = np.array(knn_accuracies)\n",
    "        max_idx = np.argmax(knn_accuracies)\n",
    "        best_k = 2*max_idx+1\n",
    "\n",
    "        accuracy = knn_accuracies[max_idx]\n",
    "        f1 = knn_f1_scores[max_idx]\n",
    "        classification_time = knn_classification_times[max_idx]\n",
    "\n",
    "        models_masks_accuracies[\"KNN\"][mask].append(accuracy)\n",
    "        models_masks_f1_scores[\"KNN\"][mask].append(f1)\n",
    "        models_masks_classification_times[\"KNN\"][mask].append(classification_time)\n",
    "        print(f\"KNN - Test Accuracy: {accuracy:.2f}%, Test F1 Score: {f1:.4f}, Classification Time: {classification_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc687d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"masking_cv_results/models_masks_accuracies.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models_masks_accuracies, f)\n",
    "with open(\"masking_cv_results/models_masks_f1_scores.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models_masks_f1_scores, f)\n",
    "with open(\"masking_cv_results/models_masks_classification_times.pkl\", \"wb\") as f:\n",
    "    pickle.dump(models_masks_classification_times, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5cddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nested = pd.DataFrame(models_masks_accuracies).T  # Models become rows\n",
    "df_nested.reset_index(inplace=True)\n",
    "df_nested = df_nested.melt(id_vars='index', var_name='Mask', value_name='Accuracy')\n",
    "df_nested.columns = ['Model', 'Mask', 'Accuracy(%)']\n",
    "\n",
    "df_long = df_nested.explode('Accuracy(%)', ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(data=df_long, x='Model', y='Accuracy(%)', hue='Mask')\n",
    "\n",
    "# Add vertical lines between model groups\n",
    "xticks = ax.get_xticks()\n",
    "for i in range(1, len(xticks)):\n",
    "    plt.axvline(x=(xticks[i-1] + xticks[i]) / 2, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.title(\"Accuracy by Model and Mask Method\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850de5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nested = pd.DataFrame(models_masks_f1_scores).T  # Models become rows\n",
    "df_nested.reset_index(inplace=True)\n",
    "df_nested = df_nested.melt(id_vars='index', var_name='Mask', value_name='F1_Score')\n",
    "df_nested.columns = ['Model', 'Mask', 'F1 Score']\n",
    "\n",
    "df_long = df_nested.explode('F1 Score', ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(data=df_long, x='Model', y='F1 Score', hue='Mask')\n",
    "\n",
    "# Add vertical lines between model groups\n",
    "xticks = ax.get_xticks()\n",
    "for i in range(1, len(xticks)):\n",
    "    plt.axvline(x=(xticks[i-1] + xticks[i]) / 2, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.title(\"F1 Score by Model and Mask Method\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c36e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nested = pd.DataFrame(models_masks_classification_times).T  # Models become rows\n",
    "df_nested.reset_index(inplace=True)\n",
    "df_nested = df_nested.melt(id_vars='index', var_name='Mask', value_name='Time(s)')\n",
    "df_nested.columns = ['Model', 'Mask', 'Time(s)']\n",
    "\n",
    "df_long = df_nested.explode('Time(s)', ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(data=df_long, x='Model', y='Time(s)', hue='Mask')\n",
    "\n",
    "# Add vertical lines between model groups\n",
    "xticks = ax.get_xticks()\n",
    "for i in range(1, len(xticks)):\n",
    "    plt.axvline(x=(xticks[i-1] + xticks[i]) / 2, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.title(\"Classification Time by Model and Mask Method\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
